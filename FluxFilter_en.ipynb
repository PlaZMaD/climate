{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665abd5c",
   "metadata": {
    "id": "pqQVYpfkwA8E"
   },
   "source": [
    "# **FluxFilter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c4ce2",
   "metadata": {
    "id": "oE87fcFbwlIu"
   },
   "source": [
    "## **Introduction**\n",
    "This open-sourced script is designed for the post-processing - visualization, filtering, filling, and partitioning - of 30-minute data from the eddy covariance stations. The script can be used for obtaining reliable cumulative sums of ecosystem heat, water vapour and NEE fluxes. A user-specified post-processing pipeline is available by means of a set of instruments for data filtering. The input parameters are: data on ecosystem fluxes with a time step of 30 minutes, calculated from the high-frequency data, with diagnostic flags, as well as meteorological parameters with a time resolution of 30 minutes. The main purpose of the script: postprocessing of the eddy covariance data of the 1-st processing level to obtain the data of the levels 2, 3 and 4.\n",
    "\n",
    "* Level 1 means fluxes calculated using the special software with the widely used filtering and correction procedures (example is Full Output file of EddyPro, LI-COR Inc., USA) and meteorological data averaged for each 30 minutes.\n",
    "* Level 2 means unfilled 30-minute data, the PI of the station excludes periods of the obvious malfunction of the instruments (i.e. data for these periods are filled with the missing values code -9999). Such data are equivalent of the Level 2 data of the European fluxes database cluster.\n",
    "* Level 3 refers to level 2 data, also unfilled, but carefully filtered based on physical and statistical criteria.\n",
    "* Level 4 refers to the gapfilled data.\n",
    "To run the demo-version of the script, just click in the Google Colab \"Runtime - Run All\"  \n",
    "The newest version of the script is in the repository https://github.com/PlaZMaD/climate/releases\n",
    "\n",
    "## **Input files**\n",
    "Input file formats: EddyPro full output file (see [EddyPro manual](https://licor.app.boxenterprise.net/s/1ium2zmwm6hl36yz9bu4)) for fluxes, and EddyPro biomet file ([again](https://licor.app.boxenterprise.net/s/1ium2zmwm6hl36yz9bu4)) for meteorology. Turbulent fluxes and u* are taken from the full output file, and all meteorological variables (air temperature and relative humidity, etc.) are taken from the biomet file. Basic requirements for input files:\n",
    "\n",
    "* Files must be in .csv (comma separated value) format.\n",
    "* Column headings should strictly correspond to the EddyPro manual. The variables in the script are identified by the column names (co2_flux for CO2 flux in full output, Ta_1_1_1 for air temperature in biomet, etc.).\n",
    "* The missing value indicator in the input files must be -9999.\n",
    "* Units for biomet file variables should be the same as the base units for biomet file as per EddyPro manual. Exception: Air/Soil Temperature should be in degrees Celsius.\n",
    "* An example full output file can be downloaded [here](https://drive.google.com/file/d/1TyuHYZ0uh5teRiRFAga0XIqfU4vYW4-N/view?usp=sharing).\n",
    "* An example biomet file can be downloaded [here](https://drive.google.com/file/d/1FjiBcSspDBlYlcg9Vzy71Sm49gOFZGBF/view?usp=sharing).\n",
    "* The full output file should have 3 header lines and the variable names should be written in the 2nd line.\n",
    "* The biomet file must have 2 header lines and the variable names must be written in the 1st line. By default, files with date and time written in the TIMESTAMP_1 column in the yyyy-mm-dd HHMM format are read without problems.\n",
    "\n",
    "## **Output files**\n",
    "Output file formats (written to the FluxFilter_output.zip archive and to the output directory in the Files section):\n",
    "1.   A file formatted for European fluxes database cluster, level 2 (ias-file);\n",
    "2.   Input file for the u* filtering, gap filling and partitioning tool REddyProcWeb (Max Planck Institute, Germany). This file is used as input for the \"Processing with the REddyProc tool\" section of this script. These are the data of level 3.\n",
    "3.   Input file for the Flux Analysis Tool (M. Ueyama, Japan) for the gap filling, level 3.\n",
    "4.   The output_all file contains all the raw variables (level 1) and all flags implemented for the data.\n",
    "5.   The output_summary file contains the time series for the main variables of the raw data, filtered data, the flag for each filter applyed, average diurnal courses in a 30- and 10-day window.\n",
    "6.   The log file records during the script's run, includes parameters that were accepted for the filtering at this run.\n",
    "7.   The reddyproc directory contains the results of gapfilling (level 4) in the same format as the original [REddyProcWeb tool](https://www.bgc-jena.mpg.de/5624929/Output-Format). Additionally, the output/reddyproc directory contains summary files (level 4) with the indexes _hourly (diurnal courses of the original and filled variables), _daily (daily averages), _monthly (monthly averages), and _yearly (yearly values, or for the entire processing period if there is less data).\n",
    "\n",
    "## **Loading the input files**\n",
    "* upload full output and biomet files to the Google drive,\n",
    "* open access to them (sharing to everyone with the link)\n",
    "* in the Data loading config replace the names of the input files with the names of the files under processing\n",
    "* in the Data loading config check the format of the input date and time\n",
    "* copy the part of the public link to the section Loading data into the command !gdown\n",
    "\n",
    "## **Before the filtering**\n",
    "*   You can upload multiple full output and biomet files, they will be automatically arranged in ascending date-time order and merged into the one table\n",
    "*   The timestamps of each input file are checked (regularization)\n",
    "*   VPD <-> RH, SWIN <-> RG <-> PAR are calculated in case of absence of any of them\n",
    "*   You can process CO2 flux or check the storage data, add it to the CO2 flux and work with NEE\n",
    "\n",
    "## **How does the filtering happen**\n",
    "The script allows you to identify and remove low-quality and outlier values using 1) physical, 2) statistical filtering, which occurs under your visual control - you can look at the graphs before and after filtering. Each filter can be switched off (just comment the corresponding line), the parameters should be adjusted to the data of a given site and eddy covariance system.\n",
    "1. Physical filtering includes removing of the following types of the suspicious values: based at the quality flag greater than the threshold, with the gas analyzer signal strength below the threshold, in case of rain events, at high humidity, by night and daytime plausible ranges, by the plausible range in winter.\n",
    "2. Statistical filtering includes the removal of outliers/spikes using filters for minimum and maximum plausible values, quantiles, deviations from the average diurnal course in a moving window, deviations from the average in a sliding window of several points MAD (Sachs, 2006) and HAMPEL (Pearson et al., 2016).\n",
    "3. It is possible to exclude data by a list of intervals (exclude from ... to ...), for example, calibrations according to the technical work log.\n",
    "\n",
    "## **Downloading the output files**\n",
    "All output files can be downloaded in the last section \"Downloading results\" by clicking the \"Download outputs\" button.\n",
    "\n",
    "(c)Evgeny Kurbatov, Vadim Mamkin, Olga Kuricheva  \n",
    "(c)REddyProc tool: Wutzler T, Lucas-Moffat A, Migliavacca M, Knauer J, Sickel K, Sigut, Menzer O & Reichstein M (2018) Basic and extensible post-processing of eddy covariance flux data with REddyProc. Biogeosciences, Copernicus, 15, doi: 10.5194/bg-15-5015-2018  \n",
    "(c)REddyProc adaptation and post-processing: Oleg Deshcherevskii\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39330183",
   "metadata": {
    "id": "sj6Z0gnhVM-R"
   },
   "source": [
    "# Technical block\n",
    "Importing libraries and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe0819",
   "metadata": {
    "id": "lZliIHxRJiqk"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "# key = userdata.get('registry_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb89b98",
   "metadata": {
    "id": "HT2KP0eYk1r3"
   },
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d148df",
   "metadata": {
    "id": "E-a6ANTGBsqg"
   },
   "outputs": [],
   "source": [
    "%pip install plotly-resampler dateparser >> /dev/null\n",
    "# %pip install --index-url https://public:{key}@gitlab.com/api/v4/projects/55331319/packages/pypi/simple --no-deps bglabutils==0.0.21 >> /dev/null\n",
    "%pip install --index-url https://gitlab.com/api/v4/projects/55331319/packages/pypi/simple --no-deps bglabutils==0.0.21 >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a4499",
   "metadata": {
    "id": "Ywv5kp0rzanK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "import dateutil\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\"\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly_resampler\n",
    "import dateparser\n",
    "\n",
    "import bglabutils.basic as bg\n",
    "import bglabutils.filters as bf\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# import bglabutils.boosting as bb\n",
    "# import textwrap\n",
    "\n",
    "from google.colab import output\n",
    "output.no_vertical_scroll()\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, filename=\"/content/output/log.log\", filemode=\"w\", force=True)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "logging.info(\"START\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e0809",
   "metadata": {
    "id": "c_5uwjkzfk45"
   },
   "source": [
    "## Functions for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e2378",
   "metadata": {
    "id": "5AXOLjh5VeMp"
   },
   "outputs": [],
   "source": [
    "def colapse_filters(data, filters_db_in):\n",
    "  out_filter = {}\n",
    "  for feature, filters in filters_db_in.items():\n",
    "    if len(filters)>0:\n",
    "      out_filter[feature] = data[filters[0]].astype(int) if len(filters)==1 else np.logical_and.reduce((data[filters].astype(int)), axis=1).astype(int)\n",
    "  return out_filter\n",
    "\n",
    "def get_column_filter(data, filters_db_in, column_name):\n",
    "  if column_name not in filters_db_in.keys():\n",
    "    return np.array([1]*len(data.index))\n",
    "  if len(filters_db_in[column_name]) > 0:\n",
    "    return colapse_filters(data, filters_db_in)[column_name]\n",
    "  else:\n",
    "    return np.array([1]*len(data.index))\n",
    "\n",
    "def basic_plot( data, col2plot, filters_db=None,  min_days=8, window_days = 10, steps_per_day=2*24, use_resample=False):\n",
    "\n",
    "  multiplot = isinstance(col2plot, list)\n",
    "\n",
    "  window_days = window_days   # days in a moving window\n",
    "  min_days = window_days//2 - 1\n",
    "  pl_data = data.copy()\n",
    "\n",
    "  layout = go.Layout(\n",
    "      paper_bgcolor='rgba(0,0,0,0)',\n",
    "      plot_bgcolor='rgba(0,0,0,0)'\n",
    "  )\n",
    "  color_data = 'darkorange'\n",
    "  color_line = 'darkslateblue'\n",
    "\n",
    "  add_color_data = copy(px.colors.qualitative.Pastel1)\n",
    "  add_color_line = copy(px.colors.qualitative.Prism)\n",
    "\n",
    "  add_color_data.insert(0, color_data)\n",
    "  add_color_line.insert(0, color_line)\n",
    "\n",
    "  fig = go.Figure(layout=layout)\n",
    "  if multiplot:\n",
    "    fig = make_subplots(rows=len(col2plot), cols=1, shared_xaxes=True, figure=fig, subplot_titles=[i.upper() for i in col2plot])\n",
    "  else:\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, figure=fig, row_heights=[.8, .2], subplot_titles=[col2plot.upper(), 'Residuals'])\n",
    "\n",
    "  fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey', minor_ticks='inside', minor_tickcolor='Grey')\n",
    "  fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey')\n",
    "  # fig.update_layout(\n",
    "  #     title = col2plot,\n",
    "  #     xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
    "  # )\n",
    "  if not multiplot:\n",
    "    cols = [col2plot]\n",
    "  else:\n",
    "    cols = col2plot\n",
    "\n",
    "  fig.update_layout(\n",
    "    # title = \" \".join(cols),\n",
    "    xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
    "  )\n",
    "  for row, col2plot in enumerate(cols):\n",
    "    if filters_db is not None:\n",
    "      filters =  get_column_filter(pl_data, filters_db, col2plot)\n",
    "      pl_data.loc[~filters.astype(bool), col2plot] = np.nan\n",
    "\n",
    "    if steps_per_day % 2 == 0:\n",
    "      closed='left'\n",
    "    else:\n",
    "      closed='both'\n",
    "    rolling_mean = bg.calc_rolling(pl_data[col2plot], step=steps_per_day, rolling_window=window_days, min_periods=min_days)\n",
    "\n",
    "    fig.add_trace(go.Scattergl(x=pl_data.index, y=pl_data[col2plot], mode='markers', name=col2plot, marker_color=add_color_data[row]), row=row+1, col=1)\n",
    "    fig.add_trace(go.Scattergl(x=rolling_mean.index, y=rolling_mean, mode='lines', name=f'{col2plot} mean {window_days} days', opacity=.7, line_color=add_color_line[row]), row=row+1, col=1)\n",
    "    if not multiplot:\n",
    "      fig.add_trace(go.Scattergl(x=rolling_mean.index, y=rolling_mean - pl_data[col2plot], mode='lines', name=f'residuals'), row=2, col=1)\n",
    "\n",
    "  if use_resample:\n",
    "    fig = plotly_resampler.FigureResampler(fig, default_n_shown_samples=5000)\n",
    "\n",
    "  fig_name = f\"_{int(np.median(pl_data.index.year))}\"\n",
    "  if \"ias_output_prefix \" in locals() or \"ias_output_prefix\" in globals():\n",
    "    fig_name = fig_name + \"_\" + ias_output_prefix\n",
    "  fig_config = {'toImageButtonOptions': {'filename': '_'.join(cols)+fig_name,}}\n",
    "  fig.show(config=fig_config)\n",
    "\n",
    "\n",
    "\n",
    "def plot_nice_year_hist_plotly(df, to_plot, time_col, filters_db):\n",
    "    pl_data = df.copy()#[to_plot]\n",
    "    # point\n",
    "    if filters_db is not None:\n",
    "      print()\n",
    "      filters =  get_column_filter(df, filters_db, to_plot)\n",
    "      pl_data['filter'] = filters\n",
    "      pl_data.loc[~filters.astype(bool), to_plot] = np.nan\n",
    "    # print(pl_data.loc[pd.to_datetime('26 June 2016 1:30'), ['nee', 'nee_nightFilter', 'swin_1_1_1', 'filter']].to_string())\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(title = f'{to_plot}')\n",
    "    fig.add_trace(go.Heatmap(x=pl_data[time_col].dt.date, y=pl_data[time_col].dt.hour + 0.5*(pl_data[time_col].dt.minute//30), z=pl_data[to_plot]))\n",
    "    fig_config = {'toImageButtonOptions': {'filename': f'{to_plot}_{int(np.median(pl_data.index.year))}',}}\n",
    "\n",
    "    fig.show(config=fig_config)\n",
    "\n",
    "\n",
    "def make_filtered_plot(data_pl, col, filters_db):\n",
    "  data = data_pl.copy()\n",
    "  layout = go.Layout(\n",
    "      paper_bgcolor='rgba(0,0,0,0)',\n",
    "      plot_bgcolor='rgba(0,0,0,0)'\n",
    "  )\n",
    "  add_color_dot = copy(px.colors.qualitative.Dark24)\n",
    "  fig = go.Figure(layout=layout)\n",
    "  fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey', minor_ticks='inside', minor_tickcolor='Grey')\n",
    "  fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey')\n",
    "\n",
    "  data['full_filter'] =  get_column_filter (data, filters_db, col)\n",
    "  data['full_filter'] = data['full_filter'].astype(int)\n",
    "  pl_data = data.query(f\"full_filter==0\")\n",
    "  color_ind = 0\n",
    "  fig.add_trace(go.Scattergl(x=data.query(\"full_filter==1\").index, y=data.query(\"full_filter==1\")[col], mode='markers', name=\"Good data\", marker_color=add_color_dot[color_ind] ))\n",
    "  color_ind += 1\n",
    "\n",
    "  if len(filters_db[col]) > 0:\n",
    "    for filter_name in filters_db[col]:\n",
    "      fig.add_trace(go.Scattergl(x=pl_data.query(f\"{filter_name}==0\").index, y=pl_data.query(f\"{filter_name}==0\")[col], mode='markers',   name=filter_name, marker_color=add_color_dot[color_ind]))\n",
    "      color_ind += 1\n",
    "      pl_data = pl_data.query(f\"{filter_name}==1\")\n",
    "\n",
    "  fig.update_layout(\n",
    "      title = f'{col2plot}',\n",
    "      xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
    "  )\n",
    "  fileName = \"basic\"\n",
    "  if \"ias_output_prefix \" in locals() or \"ias_output_prefix\" in globals():\n",
    "    fileName = ias_output_prefix\n",
    "  fileName += f'_{int(np.median(data.index.year))}_{col}'\n",
    "  fig_config = {'toImageButtonOptions': {'filename': fileName,}}\n",
    "  fig.show(config=fig_config)\n",
    "\n",
    "\n",
    "def plot_albedo (plot_data, filters_db):\n",
    "  pl_data = plot_data.copy()\n",
    "\n",
    "  layout = go.Layout(\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)'\n",
    "    )\n",
    "\n",
    "\n",
    "  if ('swin_1_1_1' not in pl_data.columns) or ('swout_1_1_1' not in pl_data.columns):\n",
    "    print(\"No swin_1_1_1/sout_1_1_1\")\n",
    "    return 0\n",
    "  pl_data['albedo'] = pl_data['swout_1_1_1'].div(pl_data['swin_1_1_1'])\n",
    "  pl_data.loc[pl_data['swin_1_1_1']<=20., 'albedo'] = np.nan\n",
    "  pl_data.loc[pl_data['swout_1_1_1']<=0, 'albedo'] = np.nan\n",
    "\n",
    "  pl_ind  = pl_data[pl_data['albedo']<pl_data['albedo'].quantile(0.95)].index\n",
    "  fig = go.Figure(layout=layout)\n",
    "  fig.add_trace(go.Scattergl(x=pl_data.loc[pl_ind].index, y=pl_data.loc[pl_ind, 'albedo'], name=\"Albedo\"))\n",
    "  fig.update_layout(title = 'Albedo')\n",
    "  fig_config = {'toImageButtonOptions': {'filename': 'albedo',}}\n",
    "  fig.show(config=fig_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede8989",
   "metadata": {
    "id": "PKznP_r1foao"
   },
   "source": [
    "## Functions for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6cb24",
   "metadata": {
    "id": "EuUwWEPRaVT5"
   },
   "outputs": [],
   "source": [
    "def min_max_filter(data_in, filters_db_in, config):\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "    for col, limits in config.items():\n",
    "      if col not in data.columns:\n",
    "        print(f\"No column with name {col}, skipping...\")\n",
    "        continue\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "\n",
    "      data[f\"{col}_minmaxfilter\"] = filter\n",
    "\n",
    "      if col not in ['rh_1_1_1', 'swin_1_1_1', 'ppfd_1_1_1', 'swin_1_1_1']:\n",
    "        data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
    "      else:\n",
    "        if col == 'rh_1_1_1':\n",
    "          data[col] = data[col].clip(upper=limits[1])\n",
    "          data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
    "        else:\n",
    "          data[col] = data[col].clip(lower=limits[0])\n",
    "          if col not in ['swin_1_1_1']:\n",
    "            data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
    "          else:\n",
    "            data.loc[data.query(f\"{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
    "\n",
    "      if f\"{col}_minmaxfilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_minmaxfilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "    logging.info(f\"min_max_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def qc_filter(data_in, filters_db_in, config):\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "    for col, limits in config.items():\n",
    "      if col not in data.columns:\n",
    "        print(f\"No column with name {col}, skipping...\")\n",
    "        continue\n",
    "\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      data[f\"{col}_qcfilter\"] = filter\n",
    "      if f\"qc_{col}\" not in data.columns and col != 'nee':\n",
    "        print(f\"No qc_{col} in data\")\n",
    "        continue\n",
    "      if col != 'nee':\n",
    "        data.loc[data[f\"qc_{col}\"] > config[col], f\"{col}_qcfilter\"] = 0\n",
    "      else:\n",
    "        data.loc[data[f\"qc_co2_flux\"] > config['co2_flux'], f\"nee_qcfilter\"] = 0\n",
    "\n",
    "      if f\"{col}_qcfilter\" not in filters_db[col]:\n",
    "          filters_db[col].append(f\"{col}_qcfilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "    logging.info(f\"qc_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def std_window_filter(data_in, filters_db_in, config):\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "    for col, lconfig in config.items():\n",
    "      sigmas = lconfig['sigmas']\n",
    "      window_size = lconfig['window']\n",
    "      min_periods = lconfig['min_periods']#(window_size//2-1)\n",
    "      points_per_day = int(pd.Timedelta('24h')/data_in.index.freq)#lconfig['points_per_day']\n",
    "      if col not in data.columns:\n",
    "        print(f\"No column with name {col}, skipping...\")\n",
    "        continue\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      data[f\"{col}_stdwindowfilter\"] = filter\n",
    "      data['tmp_col'] = data[col]\n",
    "      data.loc[~filter.astype(bool), 'tmp_col'] = np.nan\n",
    "      rolling_mean = bg.calc_rolling(data['tmp_col'], rolling_window=window_size, step=points_per_day, min_periods= min_periods)\n",
    "      residuals = data['tmp_col'] - rolling_mean\n",
    "      rolling_sigma = residuals.rolling(window=window_size * points_per_day, center=True, closed='both',  min_periods=window_size * points_per_day//2).std()\n",
    "      data = data.drop(columns='tmp_col')\n",
    "      # print(rolling_sigma, rolling_mean)\n",
    "      upper_bound = rolling_mean + rolling_sigma * sigmas\n",
    "      lower_bound = rolling_mean - rolling_sigma * sigmas\n",
    "      upper_inds = upper_bound[upper_bound < data[col]].index\n",
    "      lower_inds = lower_bound[lower_bound > data[col]].index\n",
    "      data.loc[upper_inds , f\"{col}_stdwindowfilter\"] = 0\n",
    "      data.loc[lower_inds , f\"{col}_stdwindowfilter\"] = 0\n",
    "      # # print(len(lower_inds), len(upper_inds))\n",
    "      # plt.plot(rolling_mean)\n",
    "      # plt.title(col)\n",
    "      # plt.show()\n",
    "\n",
    "      if f\"{col}_stdwindowfilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_stdwindowfilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "    logging.info(f\"std_window_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "\n",
    "def meteorological_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
    "    file_freq = data_in.index.freq\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "\n",
    "    for col in [\"co2_flux\", 'h', 'le', 'ch4_flux']:\n",
    "\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_physFilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_physFilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_physFilter\"] = filter\n",
    "\n",
    "    if 'co2_signal_strength' in data.columns and 'CO2SS_min' in config.keys():\n",
    "      data.loc[data['co2_signal_strength'] < config['CO2SS_min'], 'co2_flux_physFilter'] = 0\n",
    "    else:\n",
    "      print(\"No co2_signal_strength found\")\n",
    "\n",
    "    if 'ch4_signal_strength' in data.columns and 'CH4SS_min' in config.keys():\n",
    "      data.loc[data['ch4_signal_strength'] < config['CH4SS_min'], 'ch4_flux_physFilter'] = 0\n",
    "    else:\n",
    "      print(\"No ch4_signal_strength found\")\n",
    "\n",
    "    if 'p_rain_limit' in config.keys():\n",
    "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'co2_flux_physFilter'] = 0\n",
    "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'h_physFilter'] = 0\n",
    "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'le_physFilter'] = 0\n",
    "      if 'rain_forward_flag' in config:\n",
    "        rain_forward_flag = config['rain_forward_flag']\n",
    "        for i in range(rain_forward_flag):\n",
    "          ind = data.loc[data['p_rain_1_1_1'] >  config['p_rain_limit']].index.shift(i, freq=file_freq)\n",
    "          data.loc[ind, 'co2_flux_physFilter'] = 0\n",
    "          data.loc[ind, 'h_physFilter'] = 0\n",
    "          data.loc[ind, 'le_physFilter'] = 0\n",
    "\n",
    "    if 'RH_max' in config.keys():\n",
    "      RH_max = config['RH_max']\n",
    "      data.loc[data['rh_1_1_1']>RH_max, 'co2_flux_physFilter'] = 0\n",
    "      data.loc[data['rh_1_1_1']>RH_max, 'le_physFilter'] = 0\n",
    "    logging.info(f\"meteorological_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def meteorological_rh_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
    "    file_freq = data_in.index.freq\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "    for col in [\"co2_flux\", 'le', 'nee']:\n",
    "\n",
    "      if col not in data.columns:\n",
    "        print(f\"no {col}\")\n",
    "        continue\n",
    "\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_rhFilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_rhFilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_rhFilter\"] = filter\n",
    "\n",
    "    if 'RH_max' in config.keys() and 'rh_1_1_1' in data.columns:\n",
    "      RH_max = config['RH_max']\n",
    "      data.loc[data['rh_1_1_1']>RH_max, 'co2_flux_rhFilter'] = 0\n",
    "      if 'nee' in data.columns:\n",
    "        data.loc[data['rh_1_1_1']>RH_max, 'nee_rhFilter'] = 0\n",
    "      data.loc[data['rh_1_1_1']>RH_max, 'le_rhFilter'] = 0\n",
    "    logging.info(f\"meteorological_rh_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "\n",
    "def meteorological_night_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
    "    if \"swin_1_1_1\" not in data_in.columns:\n",
    "      logging.info(f\"meteorological_night_filter not applied, no SWIN found  \\n\")\n",
    "      return data_in, filters_db_in\n",
    "\n",
    "    if not config['use_night_filter']:\n",
    "      logging.info(f\"Night filter dissabled.\")\n",
    "      return data_in, filters_db_in\n",
    "\n",
    "    file_freq = data_in.index.freq\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "    col_of_interest = [\"h\", 'le', 'nee', 'co2_flux']\n",
    "\n",
    "    for col in col_of_interest:\n",
    "      if col not in data.columns:\n",
    "        print(f\"no {col} column\")\n",
    "        continue\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_nightFilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_nightFilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_nightFilter\"] = filter\n",
    "\n",
    "    if \"nee\" in data.columns:\n",
    "      data_night_index = data.query(f\"swin_1_1_1<10&nee<{config['night_nee_min']}\").index\n",
    "      data.loc[data_night_index, f\"nee_nightFilter\"] = 0\n",
    "\n",
    "    if \"co2_flux\" in data.columns:\n",
    "      data_night_index = data.query(\"swin_1_1_1<10&co2_flux<0\").index\n",
    "      data.loc[data_night_index, f\"co2_flux_nightFilter\"] = 0\n",
    "\n",
    "    data_night_index = data.query(f\"(h<{config['night_h_limits'][0]}|h>{config['night_h_limits'][1]})&swin_1_1_1<10\").index\n",
    "    data.loc[data_night_index, f\"h_nightFilter\"] = 0\n",
    "\n",
    "    data_night_index = data.query(f\"(h<{config['night_le_limits'][0]}|h>{config['night_le_limits'][1]})&swin_1_1_1<10\").index\n",
    "    data.loc[data_night_index, f\"le_nightFilter\"] = 0\n",
    "\n",
    "    # if 'nee' in data.columns:\n",
    "    #   data_night_index = data.query(f'nee>{config[\"day_nee_max\"]}&swin_1_1_1>=10').index\n",
    "    #   data.loc[data_night_index, f\"nee_nightFilter\"] = 0\n",
    "    logging.info(f\"meteorological_night_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "\n",
    "def meteorological_day_filter(data_in, filters_db_in, config):#, file_freq='30T'):\n",
    "    if \"swin_1_1_1\" not in data_in.columns:\n",
    "      logging.info(f\"meteorological_day_filter not applied, no SWIN found  \\n\")\n",
    "      return data_in, filters_db_in\n",
    "\n",
    "    if not config['use_day_filter']:\n",
    "      logging.info(f\"Day filter dissabled.\")\n",
    "      return data_in, filters_db_in\n",
    "\n",
    "    file_freq = data_in.index.freq\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "    col_of_interest = ['nee']\n",
    "\n",
    "    for col in col_of_interest:\n",
    "      if col not in data.columns:\n",
    "        print(f\"no {col} column\")\n",
    "        continue\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_dayFilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_dayFilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_dayFilter\"] = filter\n",
    "\n",
    "    if 'nee' in data.columns:\n",
    "      data_day_index = data.query(f'nee>{config[\"day_nee_max\"]}&swin_1_1_1>={config[\"day_swin_limit\"]}').index\n",
    "      data.loc[data_day_index, f\"nee_dayFilter\"] = 0\n",
    "    logging.info(f\"meteorological_day_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def meteorological_co2ss_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
    "    file_freq = data_in.index.freq\n",
    "    if 'CO2SS_min' not in config.keys():\n",
    "      return data_in, filters_db_in\n",
    "\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "    for col in [\"co2_flux\", 'nee']:\n",
    "\n",
    "      if col not in data.columns:\n",
    "        print(f\"no {col} column\")\n",
    "        continue\n",
    "\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_co2ssFilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_co2ssFilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_co2ssFilter\"] = filter\n",
    "\n",
    "      if 'co2_signal_strength' in data.columns:\n",
    "        data.loc[data['co2_signal_strength'] < config['CO2SS_min'], f'{col}_co2ssFilter'] = 0\n",
    "\n",
    "      else:\n",
    "        print(\"No co2_signal_strength found\")\n",
    "    logging.info(f\"meteorological_co2ss_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def meteorological_ch4ss_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
    "    file_freq = data_in.index.freq\n",
    "    if 'CH4SS_min' not in config.keys():\n",
    "      return data_in, filters_db_in\n",
    "\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "    for col in [\"ch4_flux\"]:\n",
    "\n",
    "      if col not in data.columns:\n",
    "        print(f\"no {col} column\")\n",
    "        continue\n",
    "\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_ch4ssFilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_ch4ssFilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_ch4ssFilter\"] = filter\n",
    "\n",
    "    if 'ch4_signal_strength' in data.columns:\n",
    "      data.loc[data['ch4_signal_strength'] < config['CH4SS_min'], 'ch4_flux_ch4ssFilter'] = 0\n",
    "    else:\n",
    "      print(\"No ch4_signal_strength found\")\n",
    "    logging.info(f\"meteorological_coh4ss_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def meteorological_rain_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
    "    file_freq = data_in.index.freq\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "    for col in [\"co2_flux\", 'h', 'le', 'nee', \"ch4_flux\"]:\n",
    "      if col not in data.columns:\n",
    "        print(f\"no {col}\")\n",
    "        continue\n",
    "\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_rainFilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_rainFilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_rainFilter\"] = filter\n",
    "\n",
    "    if 'p_rain_limit' in config.keys() and 'p_rain_1_1_1' in data.columns:\n",
    "      if 'co2_flux' in data.columns:\n",
    "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'co2_flux_rainFilter'] = 0\n",
    "      if 'h' in data.columns:\n",
    "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'h_rainFilter'] = 0\n",
    "      if 'le' in data.columns:\n",
    "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'le_rainFilter'] = 0\n",
    "      if 'nee' in data.columns:\n",
    "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'nee_rainFilter'] = 0\n",
    "      if 'ch4_flux' in data.columns:\n",
    "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'ch4_flux_rainFilter'] = 0\n",
    "\n",
    "      if 'rain_forward_flag' in config:\n",
    "        rain_forward_flag = config['rain_forward_flag']\n",
    "        for i in range(rain_forward_flag):\n",
    "          ind = data.loc[data['p_rain_1_1_1'] >  config['p_rain_limit']].index.shift(i, freq=file_freq)\n",
    "          ind = ind.intersection(data.index)\n",
    "          if len(ind) == 0:\n",
    "            continue\n",
    "          if 'nee' in data.columns:\n",
    "            data.loc[ind, 'nee_rainFilter'] = 0\n",
    "          if 'ch4_flux' in data.columns:\n",
    "            data.loc[ind, 'ch4_flux_rainFilter'] = 0\n",
    "          if 'co2_flux' in data.columns:\n",
    "            data.loc[ind, 'co2_flux_rainFilter'] = 0\n",
    "          if 'h' in data.columns:\n",
    "            data.loc[ind, 'h_rainFilter'] = 0\n",
    "          if 'le' in data.columns:\n",
    "            data.loc[ind, 'le_rainFilter'] = 0\n",
    "\n",
    "    logging.info(f\"meteorological_rain_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def quantile_filter(data_in, filters_db_in, config):\n",
    "    if len(config) == 0:\n",
    "      return data_in, filters_db_in\n",
    "\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "    for col, limits in config.items():\n",
    "      limit_down, limit_up = limits\n",
    "      if col not in data.columns:\n",
    "        print(f\"No column with name {col}, skipping...\")\n",
    "        continue\n",
    "\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_quantilefilter\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_quantilefilter\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_quantilefilter\"] = filter\n",
    "      up_limit = data.loc[data[f'{col}_quantilefilter'] == 1, col].quantile(limit_up)\n",
    "      down_limit = data.loc[data[f'{col}_quantilefilter'] == 1, col].quantile(limit_down)\n",
    "      f_inds = data.query(f\"{col}_quantilefilter==1\").index\n",
    "      print(\"Quantile filter cut values: \", down_limit, up_limit)\n",
    "      # data.loc[f_inds, f'{col}_quantilefilter'] = ((data.loc[f_inds, col] <= up_limit) & (data.loc[f_inds, col] >= down_limit)).astype(int)\n",
    "      data.loc[data[col] > up_limit, f'{col}_quantilefilter'] = 0\n",
    "      data.loc[data[col] < down_limit, f'{col}_quantilefilter'] = 0\n",
    "\n",
    "      # print(col, (data.loc[f_inds, col] < down_limit).sum(), (data.loc[f_inds, col] > up_limit).sum(), len(data.loc[f_inds, col].index), ((data.loc[f_inds, col] < up_limit) & (data.loc[f_inds, col] > down_limit)).astype(int).sum())\n",
    "      # print(filter.sum(), data[f'{col}_quantilefilter'].sum(), filter.sum() - data[f'{col}_quantilefilter'].sum())\n",
    "    logging.info(f\"quantile_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "\n",
    "def mad_hampel_filter(data_in, filters_db_in, config):\n",
    "    if len(config) == 0:\n",
    "      return data_in, filters_db_in\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "\n",
    "    for col, lconfig in config.items():\n",
    "      if col not in data.columns:\n",
    "        print(f\"No column with name {col}, skipping...\")\n",
    "        continue\n",
    "\n",
    "      hampel_window = lconfig['hampel_window']\n",
    "      z = lconfig['z']\n",
    "      filter = get_column_filter(data, filters_db, col)\n",
    "      if len(filter) == 0:\n",
    "        filter = [1]*len(data.index)\n",
    "\n",
    "      if f\"{col}_madhampel\" not in filters_db[col]:\n",
    "        filters_db[col].append(f\"{col}_madhampel\")\n",
    "      else:\n",
    "        print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "      data[f\"{col}_madhampel\"] = filter\n",
    "\n",
    "      # hampel_window = 20\n",
    "      print(f\"Processing {col}\")\n",
    "      outdata = bf.apply_hampel_after_mad(data.loc[data[f'{col}_madhampel']==1, :], [col], z=z, window_size=hampel_window)\n",
    "      data.loc[data[f'{col}_madhampel']==1, f'{col}_madhampel'] = outdata[f'{col}_filtered'].astype(int)\n",
    "      data[f\"{col}_madhampel\"] = data[f\"{col}_madhampel\"].astype(int)\n",
    "\n",
    "    logging.info(f\"mad_hampel_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "def manual_filter(data_in, filters_db_in, col_name, range, value ):\n",
    "    data = data_in.copy()\n",
    "    filters_db = filters_db_in.copy()\n",
    "    filter = get_column_filter(data, filters_db, col_name)\n",
    "    if len(filter) == 0:\n",
    "      filter = [1]*len(data.index)\n",
    "    data[f\"{col_name}_manualFilter\"] = filter\n",
    "    # if range not in data.index:\n",
    "    #   print('WARNING date range is not in index! Nothing is changed!')\n",
    "    #   return data, filters_db\n",
    "    try:\n",
    "      dt_start = pd.to_datetime(start, dayfirst=True)\n",
    "      dt_stop = pd.to_datetime(stop, dayfirst=True)\n",
    "      if dt_start > dt_stop:\n",
    "        raise KeyError(\"Check your dates\")\n",
    "\n",
    "      if dt_start < data.index[0] and (dt_stop <= data.index[-1] and dt_stop > data.index[0]):\n",
    "        dt_start = data.index[0]\n",
    "        print(f\"Actual manual start: {dt_start}\")\n",
    "\n",
    "      if dt_stop > data.index[-1] and (dt_start >= data.index[0] and dt_start < data.index[-1]):\n",
    "        dt_stop = data.index[-1]\n",
    "        print(f\"Actual manual stop: {dt_start}\")\n",
    "\n",
    "      range=pd.date_range(dt_start, dt_stop, freq=data.index.freq)\n",
    "      data.loc[range, f\"{col_name}_manualFilter\"] = value\n",
    "    except KeyError:\n",
    "      print(\"ERROR! Check the date range!\")\n",
    "      return data, filters_db\n",
    "\n",
    "    if f\"{col_name}_manualFilter\" not in filters_db[col_name]:\n",
    "        filters_db[col_name].append(f\"{col_name}_manualFilter\")\n",
    "    else:\n",
    "      print(\"filter already exist but will be overwritten\")\n",
    "    logging.info(f\"manual_filter applied with the next config: \\n {config}  \\n\")\n",
    "    return data, filters_db\n",
    "\n",
    "\n",
    "def winter_filter(data_in, filters_db_in, config, date_ranges):\n",
    "  data = data_in.copy()\n",
    "  filters_db = filters_db_in.copy()\n",
    "  if ('winter_nee_limits' not in config.keys()) and ('winter_ch4_flux_limits' not in config.keys()):\n",
    "    return data, filters_db\n",
    "\n",
    "  printed_flag_start = False\n",
    "  printed_flag_stop = False\n",
    "\n",
    "  if 'winter_nee_limits' in config.keys():\n",
    "      for col in ['nee', 'co2_flux']:\n",
    "        if col not in data.columns:\n",
    "          print(f\"No column with name {col}, skipping...\")\n",
    "          continue\n",
    "\n",
    "        filter = get_column_filter(data, filters_db, col)\n",
    "        if len(filter) == 0:\n",
    "          filter = [1]*len(data.index)\n",
    "        data[f\"{col}_winterFilter\"] = filter\n",
    "        try:\n",
    "          for start, stop  in date_ranges:\n",
    "            dt_start = pd.to_datetime(start, dayfirst=True)\n",
    "            dt_stop = pd.to_datetime(stop, dayfirst=True)\n",
    "\n",
    "            if dt_start > dt_stop:\n",
    "              raise KeyError(\"Check your dates, start > stop\")\n",
    "\n",
    "            if dt_stop <= data.index[0] or dt_start >= data.index[-1]:\n",
    "              print(f'Warning, empty range {dt_start} - {dt_stop}!')\n",
    "              continue\n",
    "\n",
    "            if dt_start < data.index[0] and (dt_stop <= data.index[-1] and dt_stop > data.index[0]):\n",
    "              dt_start = data.index[0]\n",
    "              if not printed_flag_start:\n",
    "                print(f\"Actual winter start: {dt_start}\")\n",
    "                printed_flag_start = True\n",
    "\n",
    "            if dt_stop > data.index[-1] and (dt_start >= data.index[0] and dt_start < data.index[-1]):\n",
    "              dt_stop = data.index[-1]\n",
    "              if not printed_flag_stop:\n",
    "                print(f\"Actual winter stop: {dt_start}\")\n",
    "                printed_flag_stop = True\n",
    "\n",
    "            range=pd.date_range(dt_start, dt_stop, freq=data.index.freq)\n",
    "\n",
    "            inds_down = data.loc[range].query(f\"{col}<{config['winter_nee_limits'][0]}\").index\n",
    "            inds_up = data.loc[range].query(f\"{col}>{config['winter_nee_limits'][1]}\").index\n",
    "            data.loc[inds_up, f\"{col}_winterFilter\"] = 0\n",
    "            data.loc[inds_down, f\"{col}_winterFilter\"] = 0\n",
    "        except KeyError:\n",
    "          print(\"ERROR! Check the date range!\")\n",
    "          return data, filters_db\n",
    "\n",
    "        if f\"{col}_winterFilter\" not in filters_db[col]:\n",
    "          filters_db[col].append(f\"{col}_winterFilter\")\n",
    "        else:\n",
    "          print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "  if 'winter_ch4_flux_limits' in config.keys():\n",
    "      for col in ['ch4_flux']:\n",
    "        if col not in data.columns:\n",
    "          print(f\"No column with name {col}, skipping...\")\n",
    "          continue\n",
    "\n",
    "        filter = get_column_filter(data, filters_db, col)\n",
    "        if len(filter) == 0:\n",
    "          filter = [1]*len(data.index)\n",
    "        data[f\"{col}_winterFilter\"] = filter\n",
    "        try:\n",
    "          for start, stop  in date_ranges:\n",
    "\n",
    "            dt_start = pd.to_datetime(start, dayfirst=True)\n",
    "            dt_stop = pd.to_datetime(stop, dayfirst=True)\n",
    "\n",
    "            if dt_start > dt_stop:\n",
    "              raise KeyError(\"Check your dates, start > stop\")\n",
    "\n",
    "            if dt_stop <= data.index[0] or dt_start >= data.index[-1]:\n",
    "              print(f'Warning, empty range {dt_start} - {dt_stop}!')\n",
    "              continue\n",
    "\n",
    "            if dt_start < data.index[0] and (dt_stop <= data.index[-1] and dt_stop > data.index[0]):\n",
    "              dt_start = data.index[0]\n",
    "              if not printed_flag_start:\n",
    "                print(f\"Actual winter start: {dt_start}\")\n",
    "                printed_flag_start = True\n",
    "\n",
    "            if dt_stop > data.index[-1] and (dt_start >= data.index[0] and dt_start < data.index[-1]):\n",
    "              dt_stop = data.index[-1]\n",
    "              if not printed_flag_stop:\n",
    "                print(f\"Actual winter stop: {dt_start}\")\n",
    "                printed_flag_stop = True\n",
    "\n",
    "            range=pd.date_range(dt_start, dt_stop, freq=data.index.freq)\n",
    "\n",
    "            inds_down = data.loc[range].query(f\"{col}<{config['winter_ch4_flux_limits'][0]}\").index\n",
    "            inds_up = data.loc[range].query(f\"{col}>{config['winter_ch4_flux_limits'][1]}\").index\n",
    "            data.loc[inds_up, f\"{col}_winterFilter\"] = 0\n",
    "            data.loc[inds_down, f\"{col}_winterFilter\"] = 0\n",
    "        except KeyError:\n",
    "          print(\"ERROR! Check the date range!\")\n",
    "          return data, filters_db\n",
    "\n",
    "        if f\"{col}_winterFilter\" not in filters_db[col]:\n",
    "          filters_db[col].append(f\"{col}_winterFilter\")\n",
    "        else:\n",
    "          print(\"filter already exist but will be overwritten\")\n",
    "\n",
    "  logging.info(f\"winter_filter applied with the next config: \\n {config}  \\n Date range: {date_ranges} \\n\")\n",
    "  return data, filters_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d90d8",
   "metadata": {
    "id": "WfWRVITABzrz"
   },
   "source": [
    "#Setting the parameters for loading and processing of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277e722",
   "metadata": {
    "id": "ox0UplWMe7wn"
   },
   "source": [
    "## Data loading config\n",
    "Here the parameters of the input files are specified: names, date-time format, etc.\n",
    "**It is necessary to change:**\n",
    "\n",
    "`###Type the names of your files and the path to them.`\n",
    "\n",
    "In `config['path']` should be either path to a file (`= ['1.csv']`) if the filename is 1.csv or the list of paths in case of several files (`= ['1.csv', '2.csv']`). If we import files via the !gdown command from the Google drive,  it is enough to specify in single quotes  *filename.extension*. Do not miss the extension .csv!\n",
    "\n",
    "**It is necessary to check:**\n",
    "\n",
    "`  format = \"%Y-%m-%d %H:%M\"  #\"%d/%m/%Y %H:%M\"  #\"%Y-%m-%d %H:%M:%S\"`\n",
    "\n",
    "Check the date order (year, month, day) and date-time separators in the input files by opening them in a text editor (Notepad). Possible options:\n",
    "\n",
    "1.  The date is written as 05/29/2024 and the time as 12:00. Then they are encoded as \"%d.%m.%Y %H:%M\" - this format is written below by default, nothing needs to be changed;\n",
    "2.  The date is written as 05/29/2024 and the time as 12:00. Change the format in the line below to \"%d/%m/%Y %H:%M\"\n",
    "3.  The date is written as 2024-05-29 and the time as 1200. Change the format in the line below to \"%Y-%m-%d %H%M\"\n",
    "4.  In other cases, behave similarly. If there are seconds in the time column, the format is encoded as \"%Y-%m-%d %H:%M:%S\".\n",
    "\n",
    "**Additional options (better not to change without PRO level):**\n",
    "\n",
    "`config['time']['converter']` must contain a function that takes a DataFrame as input and return a valid DateTime column as output, which will be used as a timestamp.\n",
    "\n",
    "`config['-9999_to_nan']` when `True` will replace -9999 to np.nan for the proper work of the algorithm.\n",
    "\n",
    "`config['repair_time']` when `True` will check the date-time column for gaps and monotony, and will perform regeneration by the first-last point taking into account the expected step length (calculated by the first pair of values in the series)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a523f",
   "metadata": {
    "id": "CXIuHMoSHMts"
   },
   "source": [
    "## Full output file loading options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd424b9",
   "metadata": {
    "id": "tVJ_DRBrlpYd"
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['debug'] = False  #True will load a small part of a file instead of a full length\n",
    "config['-9999_to_nan'] = True #change -9999 to np.nan\n",
    "config['repair_time'] = True #generate a new timestamp in case of errors\n",
    "\n",
    "#####################\n",
    "#in case of complex date-time columns\n",
    "config['time'] = {}\n",
    "config['time']['column_name'] = 'datetime'\n",
    "def my_datetime_converter(x):\n",
    "    date = x['date'].astype(str) #x['date'].dt.strftime('%d.%m.%Y') if is_datetime(x['date'].dtype) else x['date'].astype(str)\n",
    "    time = x['time'].astype(str) #x['time'].dt.strftime('%H:%M') if is_datetime(x['time'].dtype) else x['time'].astype(str)\n",
    "\n",
    "    x['tmp_datetime'] = date + \" \" + time\n",
    "    #Check the format of a date and time in the Full Output file\n",
    "    format = \"%d.%m.%Y %H:%M\"#\"%d/%m/%Y %H:%M\"# \"%Y-%m-%d %H:%M\"  #\"%d/%m/%Y %H:%M\"  #\"%Y-%m-%d %H:%M:%S\"\n",
    "    return pd.to_datetime(x['tmp_datetime'], format=format)#dayfirst=True)#, format=format)\n",
    "config['time']['converter'] = my_datetime_converter\n",
    "#####################\n",
    "\n",
    "###Type the names of your files and the path to them. If the files are loaded from the Google drive\n",
    "###using !gdown command, just change the name of the file below\n",
    "config['path'] = ['eddy_pro_full output_Fy4_2023_demo.csv']#['eddypro_GHG_biomet_CO2SS_Express_full_output_2023-03-29T020107_exp.csv']#['eddypro_noHMP_full_output_2014_1-5.csv', 'eddypro_noHMP_full_output_2014_5-12.csv']#['/content/eddypro_NCT_GHG_22-23dry_full_output.xlsx', '/content/eddypro_NCT_GHG_22wet_full_output.xlsx', '/content/eddypro_NCT_GHG_23wet_full output.xlsx']#'/content/new.csv'\n",
    "# config['path'] = '/content/DT_Full output.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f05fe",
   "metadata": {
    "id": "S2Qc-fltJLaF"
   },
   "source": [
    "## Biomet file loading options\n",
    "`use_biomet`: if there is no file with meteorological data, set it to False. Fluxes will be filtered and filled using the information from the full output file (air temperature and humidity)\n",
    "\n",
    "**It is necessary to change:**\n",
    "\n",
    "`###Type the names of your files and the path to them.`\n",
    "\n",
    "In config['path'] should be either path to a file (= ['1.csv']) if the filename is 1.csv or the list of paths in case of several files (= ['1.csv', '2.csv']). If we import files via the !gdown command from the Google drive, it is enough to specify in single quotes filename.extension. Do not miss the extension .csv!\n",
    "\n",
    "**It is necessary to check:**\n",
    "\n",
    "`  format = \"%Y-%m-%d %H%M\"  #\"%d.%m.%Y %H:%M\"  #yyyy-mm-dd HHMM`\n",
    "\n",
    "Check the order of the date (year, month, day) and date-time separators in the input files by opening them in a text editor I(Notepad). In the biomet file, by default, the date is written as 2011-11-12 and the time as 1200. It is encoded as \"%Y-%m-%d %H%M\". In other cases, change the date-time format code according to the instructions for the instruction in Data loading config section (see upper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9670f",
   "metadata": {
    "id": "H7E5LGx1DVsA"
   },
   "outputs": [],
   "source": [
    "config_meteo = {}\n",
    "config_meteo ['use_biomet'] = True\n",
    "config_meteo['debug'] = False  # True will load a small part of a file instead of a full length\n",
    "config_meteo['-9999_to_nan'] = True #change -9999 to np.nan\n",
    "config_meteo['repair_time'] = True #generate a new timestamp in case of errors\n",
    "\n",
    "#####################\n",
    "#in case of complex date-time columns\n",
    "config_meteo['time'] = {}\n",
    "config_meteo['time']['column_name'] = 'datetime'\n",
    "def my_datetime_converter(x):\n",
    "    format = \"%Y-%m-%d %H%M\"  #\"%d.%m.%Y %H:%M\"  #yyyy-mm-dd HHMM #Check the format of a date and time in the Biomet file\n",
    "    return pd.to_datetime(x[\"TIMESTAMP_1\"], format=format)\n",
    "config_meteo['time']['converter'] = my_datetime_converter\n",
    "#####################\n",
    "\n",
    "###Type the names of your files and the path to them. If the files are loaded from the Google drive\n",
    "###using !gdown command, just change the name of the file below\n",
    "config_meteo['path'] = 'BiometFy4_2023_demo.csv'#'BiometFy4_2016.csv'#'BiometNCT_2011-22.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e8068",
   "metadata": {
    "id": "DtxFTNnEfENz"
   },
   "source": [
    "## Selecting columns for graphs and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3e678",
   "metadata": {
    "id": "nLnivFTtg9cu"
   },
   "outputs": [],
   "source": [
    "#Gather the summary information about the parameters of interest:\n",
    "cols_to_investigate = []\n",
    "cols_to_investigate.append(\"co2_flux\")\n",
    "cols_to_investigate.append(\"ch4_flux\")\n",
    "cols_to_investigate.append(\"LE\")\n",
    "cols_to_investigate.append(\"H\")\n",
    "cols_to_investigate.append(\"co2_strg\")\n",
    "cols_to_investigate.append(\"Ta_1_1_1\")\n",
    "cols_to_investigate.append(\"RH_1_1_1\")\n",
    "cols_to_investigate.append(\"VPD_1_1_1\")\n",
    "cols_to_investigate.append(\"P_1_1_1\")\n",
    "cols_to_investigate.append(\"SWIN_1_1_1\")\n",
    "cols_to_investigate.append(\"PPFD_1_1_1\")\n",
    "# cols_to_investigate.append(\"co2_signal_strength\")\n",
    "# cols_to_investigate.append(\"ch4_signal_strength\")\n",
    "\n",
    "cols_to_investigate =  [k.lower()for k in cols_to_investigate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d03d7c",
   "metadata": {
    "id": "wVpYvr9_fKBU"
   },
   "source": [
    "## Setting up parameters for the data analysis\n",
    "\n",
    "All settings for co2_flux will be applied to nee if it is chosen to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbead80f",
   "metadata": {
    "id": "FH2uRGi4p5Zj"
   },
   "source": [
    "### Physical filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae8107",
   "metadata": {
    "id": "pPemVdWVbq2E"
   },
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "calc_nee = True\n",
    "\n",
    "# Output ID which will be added to the names of all output files and graphs\n",
    "ias_output_prefix = 'tv_fy4'\n",
    "ias_output_version = 'v01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5658446",
   "metadata": {
    "id": "5MK90gyzQryZ"
   },
   "source": [
    "Filtering by quality flags. Data with flags in the range (-inf, val] will be marked as valid, and data with a flag value greater than the threshold will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb63d4",
   "metadata": {
    "id": "ukl734CBblay"
   },
   "outputs": [],
   "source": [
    "qc_config = {}\n",
    "qc_config['h'] = 1  #Change when using 1-9 flag system\n",
    "qc_config['le'] = 1  #Change when using 1-9 flag system\n",
    "qc_config['co2_flux'] = 1  #Change when using 1-9 flag system\n",
    "qc_config['ch4_flux'] = 1  #Change when using 1-9 flag system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10acb67",
   "metadata": {
    "id": "QPIFpLN_-8Uf"
   },
   "source": [
    "Filtering fluxes in certain meteorological conditions (by values of meteorological variables), possible options:\n",
    "\n",
    "*   `CO2SS_min` - will remove CO2_FLUX when co2_signal_strength is below the set threshold\n",
    "*   `p_rain_limit` - will remove H, LE and CO2_FLUX, for P_rain_1_1_1 above the set threshold\n",
    "*   `rain_forward_flag` - will remove values for the set number of records forward from each value filtered in the previous step\n",
    "*   `RH_max` - will remove LE and CO2_FLUX values for which RH_1_1_1 is greater than the specified threshold\n",
    "*   `use_day_filter` - if True, daily (Swin>`day_swin_limit`) NEEs greater than the set threshold (NEE>`day_nee_max`) will be excluded\n",
    "*   `use_night_filter` - if True, night (Swin<`day_swin_limit`) NEEs less than the set threshold (NEE<`night_nee_min`) will be excluded\n",
    "*   `day_nee_max` - threshold for NEE during daytime (excluding intense emission during daytime)\n",
    "*   `night_nee_min` - threshold for NEE at night (excluding intense net absorption at night)\n",
    "*   `day_swin_limit` - threshold of incoming shortwave radiation, defining daytime data (can be changed for stations beyond the Arctic Circle)\n",
    "*   `night_h_limits`, `night_le_limits` - plausible night ranges for H and LE\n",
    "*   `winter_nee_limits` - plausible range of NEE in winter (winter period is set separately in the section \"winter period filtering, determine the dates!\")\n",
    "*   `winter_ch4_flux_limits` - plausible range of methane flux in winter\n",
    "*   `CH4SS_min` - will remove CH4_FLUX when ch4_signal_strength is below the specified value\n",
    "\n",
    "If any of the parameters are missing or commented in this cell, filtering is not applied.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d41d8",
   "metadata": {
    "id": "vxpiAbWk2yYr"
   },
   "outputs": [],
   "source": [
    "meteo_filter_config = {}\n",
    "meteo_filter_config['CO2SS_min'] = 80.\n",
    "\n",
    "# These filters may not be needed for the closed-path eddy covariance systems\n",
    "meteo_filter_config['p_rain_limit'] = .1\n",
    "meteo_filter_config['rain_forward_flag'] = 2\n",
    "# Relative humidity filter (intended to find the cases of formation of condensation): apply only if CO2SS and anemometer\n",
    "# diagnostics are absent and the data were not filtered by these indicators during the calculation of 30-min data\n",
    "# meteo_filter_config['RH_max'] = 98\n",
    "\n",
    "# Plausible values at day/night-time\n",
    "meteo_filter_config['use_day_filter'] = True\n",
    "meteo_filter_config['use_night_filter'] = True\n",
    "meteo_filter_config['day_nee_max'] = 5\n",
    "meteo_filter_config['night_nee_min'] = -5\n",
    "meteo_filter_config['day_swin_limit'] = 10\n",
    "meteo_filter_config['night_h_limits'] = [-50, 20]\n",
    "meteo_filter_config['night_le_limits'] = [-50, 20]\n",
    "\n",
    "# Plausible values at winter. For the grass ecosystems the upper limit is usually lower\n",
    "meteo_filter_config['winter_nee_limits'] = [-1, 5]\n",
    "meteo_filter_config['winter_ch4_flux_limits'] = [-1, 1]\n",
    "meteo_filter_config['CH4SS_min'] = 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637b28e",
   "metadata": {
    "id": "utUX7SA4qA_I"
   },
   "source": [
    "### Statistical filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17b48e",
   "metadata": {
    "id": "wWISuF-xQCwq"
   },
   "source": [
    "Filtering parameters by absolute values. For `rh_1_1_1` values above the boundary are replaced with the boundary values instead of discarding. For `ppfd_1_1_1`, `swin_1_1_1` the minimum values are processed similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2ac28",
   "metadata": {
    "id": "HQfIYFOd9uzi"
   },
   "outputs": [],
   "source": [
    "min_max_config  = {}\n",
    "min_max_config['co2_flux'] = [-40, 40]\n",
    "min_max_config['co2_strg'] = [-20, 20]\n",
    "min_max_config['h'] = [-100, 800]\n",
    "min_max_config['le'] = [-100, 1000]\n",
    "min_max_config['u_star'] = [0, 10]\n",
    "min_max_config['ta_1_1_1'] = [-50, 50]\n",
    "min_max_config['p_1_1_1'] = [0, 100]\n",
    "min_max_config['vpd_1_1_1'] = [0, 50]\n",
    "min_max_config['rh_1_1_1'] = [0, 100]#max\n",
    "min_max_config['swin_1_1_1'] = [0, 1200]#min\n",
    "min_max_config['ppfd_1_1_1'] = [0, 2400]#min\n",
    "min_max_config['rg_1_1_1'] = [0, 2400]#min\n",
    "min_max_config['ch4_flux'] = [-10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd6e30",
   "metadata": {
    "id": "vmyTKbV1RdjD"
   },
   "source": [
    "Filtering parameters based on the deviation from the average diurnal course in a moving window.\n",
    "* `sigmas` - plausible range of deviation from the average course; values outside the range are eliminated\n",
    "* `window` - window size in days for calculating the diurnal course\n",
    "* `min_periods` - minimum number of points for each 30-min interval in the window. If the window has less points then the course is not calculated, the filter is not applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab30962",
   "metadata": {
    "id": "xfRVNYbFYzG3"
   },
   "outputs": [],
   "source": [
    "window_filter_config = {}\n",
    "\n",
    "# Closed-path eddy covariance systems and less noisy data may need milder criteria (for example, 3 sigma)\n",
    "window_filter_config['co2_flux'] = {'sigmas': 2, 'window': 10,  'min_periods': 4}\n",
    "window_filter_config['ch4_flux'] = {'sigmas': 2, 'window': 10,  'min_periods': 4}\n",
    "\n",
    "# If reliable data are deleted, it is recommended to increase the value of 'sigmas'\n",
    "window_filter_config['ta_1_1_1'] = {'sigmas': 4, 'window': 10,  'min_periods': 4}\n",
    "window_filter_config['u_star'] = {'sigmas': 4, 'window': 10,  'min_periods': 4}\n",
    "for col in ['h', 'le', 'rh_1_1_1', 'vpd_1_1_1']:\n",
    "    window_filter_config[col] = {'sigmas': 7, 'window': 10,  'min_periods': 4}\n",
    "for col in ['swin_1_1_1', 'ppfd_1_1_1']:\n",
    "    window_filter_config[col] = {'sigmas': 8, 'window': 10,  'min_periods': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485648fe",
   "metadata": {
    "id": "KF_MGD7pSGre"
   },
   "source": [
    "Filtering parameters above and below threshold by quantiles (drop-down points are eliminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca1054",
   "metadata": {
    "id": "asO_t2tZmiD0"
   },
   "outputs": [],
   "source": [
    "quantile_filter_config = {}\n",
    "quantile_filter_config['co2_flux'] = [0.01, 0.99]\n",
    "quantile_filter_config['ch4_flux'] = [0.01, 0.99]\n",
    "quantile_filter_config['co2_strg'] = [0.01, 0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23846e4",
   "metadata": {
    "id": "cPiTN288UaP3"
   },
   "source": [
    "Parameters for filtering by deviation from the neighboring points: MAD and Hampel filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d9fcd",
   "metadata": {
    "id": "2b3eBVFUq3AU"
   },
   "outputs": [],
   "source": [
    "# madhampel_filter_config = {i:{'z': 5.5, 'hampel_window': 10} for i in cols_to_investigate if 'p_1_1_1' not in i}\n",
    "madhampel_filter_config = {}\n",
    "\n",
    "# Hard filtering: 'z'=4. Mild filtering: 'z'=7\n",
    "madhampel_filter_config['co2_flux'] = {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config['ch4_flux'] = {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config['le'] =  {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config['h'] =  {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config['co2_strg'] =  {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config[ 'ta_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config[ 'rh_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config[ 'vpd_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
    "madhampel_filter_config[ 'swin_1_1_1'] =  {'z': 8.0, 'hampel_window': 10}\n",
    "madhampel_filter_config[ 'ppfd_1_1_1'] =  {'z': 8.0, 'hampel_window': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf5c52",
   "metadata": {
    "id": "wVF1vDm4EauW"
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf65d07",
   "metadata": {
    "id": "LV9FvvtnVqdN"
   },
   "source": [
    "**It is necessary to change:**\n",
    "\n",
    "After !gdown, insert the characters after d/ and before the next / from the public link to the file located on Google Drive. For example, if the link is\n",
    "https://drive.google.com/file/d/1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB/view?usp=sharing,\n",
    "then the command will be written as\n",
    "`!gdown 1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB`\n",
    "\n",
    "`#Upload full output`  \n",
    "Here you need to write the symbols from the link to the full output file\n",
    "\n",
    "`#Upload biomet`  \n",
    "Here you need to type characters from the link to the biomet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d79ca",
   "metadata": {
    "id": "KMu4IqY45HG6"
   },
   "outputs": [],
   "source": [
    "# Upload full output\n",
    "# https://drive.google.com/file/d/1AD4U06Qre-PgKnsyRHX11RuruCNQzMvB/view?usp=sharing\n",
    "!gdown 1AD4U06Qre-PgKnsyRHX11RuruCNQzMvB\n",
    "\n",
    "# Upload biomet\n",
    "# https://drive.google.com/file/d/1_ZoFgNyOZEYNdjf6rFq66HR4UXDSQ5z3/view?usp=sharing\n",
    "!gdown 1_ZoFgNyOZEYNdjf6rFq66HR4UXDSQ5z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe182c9",
   "metadata": {
    "id": "Xw5TapK10EhR"
   },
   "outputs": [],
   "source": [
    "data, time = bg.load_df(config)\n",
    "data = data[next(iter(data))]  #it was dictionary\n",
    "data_freq = data.index.freq\n",
    "\n",
    "print(\"Timestamps interval for full_output: \", data.index[[0, -1]])\n",
    "logging.info(f\"Data loaded from {config['path']}\")\n",
    "logging.info(\"Time range for full_output: \"+ \" - \".join(data.index[[0,-1]].strftime('%Y-%m-%d %H:%M')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce97509",
   "metadata": {
    "id": "6j7ombDYqyC8"
   },
   "source": [
    "Regularization: checking the ordering of timestamps. Removing duplicates, fill in gaps. Additional checking in case of loading several files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a9fb6",
   "metadata": {
    "id": "65DLIIucNOPe"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  data_meteo, time_meteo  = bg.load_df(config_meteo)\n",
    "  data_meteo = data_meteo[next(iter(data_meteo))]  #it was dictionary\n",
    "  meteo_freq = data_meteo.index.freq\n",
    "  print(\"Time range for meteo: \", data_meteo.index[[0, -1]])\n",
    "  logging.info(f\"MeteoData loaded from {config_meteo['path']}\")\n",
    "  logging.info(\"Time range for meteo: \"+ \" - \".join(data_meteo.index[[0,-1]].strftime('%Y-%m-%d %H:%M')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd574a4",
   "metadata": {
    "id": "3fVgA8UTMfJ3"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  if data_freq != meteo_freq:\n",
    "    print(\"Resampling meteo data\")\n",
    "    logging.info(f\"Resampling meteo data\")\n",
    "    data_meteo = data_meteo.asfreq(data_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223b938",
   "metadata": {
    "id": "rZbqd6adhHEP"
   },
   "outputs": [],
   "source": [
    "print(\"Columns in FullOutput \\n\", data.columns.to_list())\n",
    "if config_meteo ['use_biomet']:\n",
    "  print(\"Columns in meteo \\n\", data_meteo.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598788f",
   "metadata": {
    "id": "FF78Wlq9rD_n"
   },
   "source": [
    "Merge into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed6a33",
   "metadata": {
    "id": "9v0rxHehhZEI"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  data = data.join(data_meteo, how='outer', rsuffix='_meteo')\n",
    "  data[time] = data.index\n",
    "  data = bg.repair_time(data, time)\n",
    "  if data[data_meteo.columns[-1]].isna().sum() == len(data.index):\n",
    "    print(\"Bad meteo data range, skipping! Setting config_meteo ['use_biomet']=False\")\n",
    "    config_meteo ['use_biomet'] = False\n",
    "\n",
    "points_per_day = int(pd.Timedelta('24H')/data_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e71d939",
   "metadata": {
    "id": "C8lLDYOWzH2d"
   },
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.lower()\n",
    "if not config_meteo ['use_biomet']:\n",
    "  for col in ['rh', 'vpd']:\n",
    "    data[col+\"_1_1_1\"] = data[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270564c1",
   "metadata": {
    "id": "ipknrLaeByCT"
   },
   "source": [
    "Checking for correct data type (example: presence of text instead of numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870a9a1",
   "metadata": {
    "id": "8LawdKUbB1_m"
   },
   "outputs": [],
   "source": [
    "cols_2_check = ['ppfd_in_1_1_1', 'u_star', 'swin_1_1_1', 'co2_signal_strength', 'rh_1_1_1', 'vpd_1_1_1', 'rg_1_1_1', 'p_rain_1_1_1', 'co2_signal_strength_7500_mean', 'CO2SS'.lower(), 'co2_signal_strength',\n",
    "                'ch4_signal_strength_7500_mean', 'ch4SS'.lower(), 'ch4_signal_strength',\n",
    "                'p_1_1_1', 'ta_1_1_1', 'co2_strg', 'le', 'h']\n",
    "\n",
    "\n",
    "biomet_columns = []\n",
    "if config_meteo ['use_biomet']:\n",
    "  biomet_columns = data_meteo.columns.str.lower()\n",
    "data_type_error_flag = False\n",
    "for col in cols_2_check:\n",
    "  if col not in data.columns:\n",
    "    continue\n",
    "  error_positions = data[col].fillna(0).apply(pd.to_numeric, errors='coerce').isna()\n",
    "  if error_positions.any():\n",
    "    logging.error(f\"\"\"Check input files for {col} column near:\\n {error_positions[error_positions==True].index.strftime('%d-%m-%Y %H:%M').values} in {'biomet' if len(biomet_columns)>0 and col in  biomet_columns else 'data'} file\"\"\")\n",
    "    data_type_error_flag = True\n",
    "if data_type_error_flag:\n",
    "  print(\"Data have some errors! Please check log file!\")\n",
    "  raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48e3fa",
   "metadata": {
    "id": "QDHkyl_PruXE"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbcf04",
   "metadata": {
    "id": "Nh5MosYXS6aj"
   },
   "source": [
    "Renaming the columns to a single format, calculating VPD <-> RH, SWIN <-> RG and PAR <-> SWIN if absent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9396c5",
   "metadata": {
    "id": "mAdYXJFdSRbJ"
   },
   "outputs": [],
   "source": [
    "have_rh_flag = False\n",
    "have_vpd_flag = False\n",
    "have_par_flag = False\n",
    "have_swin_flag = False\n",
    "have_rg_flag = False\n",
    "have_p_flag = False\n",
    "have_pr_flag = False\n",
    "have_ppfd_flag = False\n",
    "\n",
    "for col_name in data.columns:\n",
    "  if 'u*' in col_name:\n",
    "    print(f\"renaming {col_name} to u_star\")\n",
    "    data = data.rename(columns={col_name: 'u_star'})\n",
    "  if 'ppfd_in_1_1_1' in col_name:\n",
    "    print(f\"renaming {col_name} to ppfd_1_1_1\")\n",
    "    data = data.rename(columns={col_name: 'ppfd_1_1_1'})\n",
    "  if 'sw_in_1_1_1' in col_name:\n",
    "    print(f\"renaming {col_name} to swin_1_1_1\")\n",
    "    data = data.rename(columns={col_name: 'swin_1_1_1'})\n",
    "  if 'co2_signal_strength' in col_name:\n",
    "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
    "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
    "  if \"rh_1_1_1\" in col_name:\n",
    "    have_rh_flag =True\n",
    "  if \"vpd_1_1_1\" in col_name:\n",
    "    have_vpd_flag = True\n",
    "  if 'swin' in col_name or 'sw_in' in col_name:\n",
    "    have_swin_flag = True\n",
    "  if 'par' in col_name:\n",
    "    have_par_flag = True\n",
    "  if 'rg_1_1_1' in col_name:\n",
    "    have_rg_flag = True\n",
    "  if 'p_1_1_1' in col_name:\n",
    "    have_p_flag = True\n",
    "  if 'p_rain_1_1_1' in col_name:\n",
    "    have_pr_flag = True\n",
    "  if 'ppfd_1_1_1' in col_name:\n",
    "    have_ppfd_flag = True\n",
    "  if col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()] or 'co2_signal_strength' in col_name:\n",
    "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
    "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
    "  if col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()] or 'ch4_signal_strength' in col_name:\n",
    "    print(f\"renaming {col_name} to ch4_signal_strength\")\n",
    "    data = data.rename(columns={col_name: 'ch4_signal_strength'})\n",
    "\n",
    "\n",
    "if not (have_ppfd_flag or have_swin_flag):\n",
    "  print(\"NO PPFD and SWin\")\n",
    "else:\n",
    "    if not have_ppfd_flag:\n",
    "      data['ppfd_1_1_1'] = data['swin_1_1_1'] / 0.46\n",
    "    if not have_swin_flag:\n",
    "      data['swin_1_1_1'] = 0.46 * data['ppfd_1_1_1']\n",
    "    have_ppfd_flag = True\n",
    "    have_swin_flag = True\n",
    "\n",
    "\n",
    "if not (have_rg_flag or have_swin_flag):\n",
    "  print(\"NO RG AND SWIN\")\n",
    "else:\n",
    "  print(\"Checking RG-SWIN pair\")\n",
    "  if not have_rg_flag:\n",
    "    data['rg_1_1_1'] = data['swin_1_1_1']\n",
    "  if not have_swin_flag:\n",
    "    data['swin_1_1_1'] = data['rg_1_1_1']\n",
    "    have_swin_flag = True\n",
    "\n",
    "\n",
    "if not (have_p_flag or have_pr_flag):\n",
    "  print(\"NO P and P_RAIN\")\n",
    "else:\n",
    "  print(\"Checking P <-> P_rain pair\")\n",
    "  if not have_p_flag:\n",
    "    data['p_1_1_1'] = data['p_rain_1_1_1']\n",
    "  if not have_pr_flag:\n",
    "    data['p_rain_1_1_1'] = data['p_1_1_1']\n",
    "\n",
    "\n",
    "if not (have_vpd_flag or have_rh_flag):\n",
    "  print(\"NO RH AND VPD\")\n",
    "else:\n",
    "    if 'ta_1_1_1' in data.columns:\n",
    "      temp_k = (data['ta_1_1_1'] + 273.15)\n",
    "    else:\n",
    "      temp_k = data['air_temperature']\n",
    "    logE = 23.5518-(2937.4/temp_k)-4.9283*np.log10(temp_k)\n",
    "    ehpa = np.power(10, logE)\n",
    "    if not have_vpd_flag:\n",
    "      print(\"calculating vpd_1_1_1 from rh\")\n",
    "      data['vpd_1_1_1'] = ehpa - (ehpa*data['rh_1_1_1']/100)\n",
    "    if not have_rh_flag:\n",
    "      print(\"calculating rh_1_1_1 from vpd\")\n",
    "      data['rh_1_1_1'] = ehpa\n",
    "\n",
    "\n",
    "if not (have_par_flag or have_swin_flag):\n",
    "  print(\"NO PAR and SWin\")\n",
    "else:\n",
    "    if not have_par_flag:\n",
    "      data['par'] = data['swin_1_1_1'] / 0.47#SWin=PAR*0.47\n",
    "    if not have_swin_flag:\n",
    "      data['swin_1_1_1'] = 0.47 * data['par']\n",
    "\n",
    "\n",
    "\n",
    "for col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()]:\n",
    "  # print(data.columns.to_list())\n",
    "  if col_name in data.columns.to_list():\n",
    "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
    "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
    "\n",
    "for col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()]:\n",
    "  # print(data.columns.to_list())\n",
    "  if col_name in data.columns.to_list():\n",
    "    print(f\"renaming {col_name} to ch4_signal_strength\")\n",
    "    data = data.rename(columns={col_name: 'ch4_signal_strength'})\n",
    "\n",
    "if not config_meteo ['use_biomet'] or 'ta_1_1_1' not in data.columns:\n",
    "    data['ta_1_1_1'] = data['air_temperature'] - 273.15\n",
    "    logging.info(\"No Ta_1_1_1 column found, replaced by 'air_temperature'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2deebf",
   "metadata": {
    "id": "soyyX-MCbaXt"
   },
   "source": [
    "## Calculating NEE from CO2_flux and storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa05174",
   "metadata": {
    "id": "lqWwGSMObro4"
   },
   "source": [
    "Checking the storage. The storage calculated by one vertical level in EddyPro is not always reliable. Correctness is checked by the proper diurnal course: there should be an accumulation during the night, and a sharp decrease in the morning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363954b",
   "metadata": {
    "id": "2yqwO7Uhcjmz"
   },
   "source": [
    "Filtering co2_strg to remove values above and below threshold quantiles. Filling co2_strg gaps of 3 points or less with linear interpolation. The resulting filtered and filled co2_strg values are plotted. You make a decision whether to sum co2_flux and co2_strg to obtain NEE or to continue working with co2_flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a710cb",
   "metadata": {
    "id": "cjt05XXtbr69"
   },
   "outputs": [],
   "source": [
    "# Gaps shorter then 3 points are linearly interpolated\n",
    "if calc_nee and 'co2_strg' in data.columns:\n",
    "  tmp_data = data.copy()\n",
    "  tmp_data['co2_strg_tmp'] = tmp_data['co2_strg'].copy()\n",
    "  tmp_filter_db =  {'co2_strg_tmp': []}\n",
    "  if 'co2_strg' in  quantile_filter_config.keys():\n",
    "    tmp_q_config = {'co2_strg_tmp':quantile_filter_config['co2_strg']}\n",
    "  else:\n",
    "    tmp_q_config = {}\n",
    "  tmp_filter_db = {'co2_strg_tmp':[]}\n",
    "  tmp_data, tmp_filter_db = quantile_filter(tmp_data, tmp_filter_db, tmp_q_config)\n",
    "  tmp_data.loc[~get_column_filter(tmp_data, tmp_filter_db, 'co2_strg_tmp').astype(bool), 'co2_strg_tmp'] = np.nan\n",
    "  # tmp_data['co2_strg_tmp'] = tmp_data['co2_strg_tmp'].interpolate(limit=3)\n",
    "  # tmp_data['co2_strg_tmp'].fillna(bg.calc_rolling(tmp_data['co2_strg_tmp'], rolling_window=10 , step=points_per_day, min_periods=4))\n",
    "  basic_plot(tmp_data, ['co2_strg_tmp'], tmp_filter_db, steps_per_day=points_per_day)\n",
    "  print(tmp_q_config, tmp_filter_db, tmp_data['co2_strg_tmp_quantilefilter'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f94c6b",
   "metadata": {
    "id": "2IQ7W6pslYF-"
   },
   "outputs": [],
   "source": [
    "# Decide whether to sum co2_flux and co2_strg_filtered_filled for obtaining NEE\n",
    "calc_with_strg = False   # To calculate NEE with strg type True.\n",
    "logging.info(f\"calc_with_strg is set to {calc_with_strg}\")\n",
    "# For working with co2_flux, ignoring co2_strg, type False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564578ce",
   "metadata": {
    "id": "ueuvsNxYdtgs"
   },
   "outputs": [],
   "source": [
    "if calc_nee and 'co2_strg' in data.columns:\n",
    "  if calc_with_strg:\n",
    "    data['nee'] = (tmp_data['co2_flux'] + tmp_data['co2_strg_tmp']).copy()\n",
    "  else:\n",
    "    data['nee'] = data['co2_flux'].copy()\n",
    "  del tmp_data\n",
    "  if 'nee' not in cols_to_investigate:\n",
    "    cols_to_investigate.append('nee')\n",
    "  for filter_config in [qc_config, meteo_filter_config, min_max_config, window_filter_config, quantile_filter_config, madhampel_filter_config]:\n",
    "    if 'co2_flux' in filter_config:\n",
    "      filter_config['nee'] = filter_config['co2_flux']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73373eb8",
   "metadata": {
    "id": "mUgwuaFYribB"
   },
   "source": [
    "#Overview of statistics for the columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9addda2",
   "metadata": {
    "id": "dhcplCMbXtkK"
   },
   "outputs": [],
   "source": [
    "cols_to_investigate = [p for p in cols_to_investigate if p in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848e494",
   "metadata": {
    "id": "YfusqiotOi3n"
   },
   "outputs": [],
   "source": [
    "data.loc[:, cols_to_investigate].describe()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=min(3, len(cols_to_investigate)), nrows=int(np.ceil(len(cols_to_investigate)/3)), squeeze=False, figsize=(13, 8))\n",
    "\n",
    "for ind, ax in enumerate(axs.reshape(-1)):\n",
    "  if ind >= len(cols_to_investigate):\n",
    "    break\n",
    "  feature = cols_to_investigate[ind]\n",
    "  ax.boxplot(data[feature].to_numpy()[~np.isnan(data[feature].to_numpy())])\n",
    "  ax.set_title(f\"Boxplot for {feature}\")\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "data[cols_to_investigate].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda8fce0",
   "metadata": {
    "id": "0oJLXYGbr93S"
   },
   "source": [
    "# Physical data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ebf7b",
   "metadata": {
    "id": "apGNk8eBxgBv"
   },
   "outputs": [],
   "source": [
    "plot_data = data.copy()\n",
    "filters_db = {col: [] for col in plot_data.columns.to_list()}\n",
    "print(plot_data.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4f991",
   "metadata": {
    "id": "BL_6XxGGsCBK"
   },
   "source": [
    "## using quality flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66741c76",
   "metadata": {
    "id": "GGwe7_uU1C8U"
   },
   "outputs": [],
   "source": [
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, filters_db = qc_filter(plot_data, filters_db, qc_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04f041",
   "metadata": {
    "id": "M_gKSTNYyzjS"
   },
   "source": [
    "## accounting for CO2SS and CH4SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a3895",
   "metadata": {
    "id": "viq7BZue9Ett"
   },
   "outputs": [],
   "source": [
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, filters_db = meteorological_co2ss_filter(plot_data, filters_db, meteo_filter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73943b3",
   "metadata": {
    "id": "5RrPfxfiJGhN"
   },
   "outputs": [],
   "source": [
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, filters_db = meteorological_ch4ss_filter(plot_data, filters_db, meteo_filter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a30d4c",
   "metadata": {
    "id": "qwqVDeH6y73_"
   },
   "source": [
    "## plausible RH values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84525840",
   "metadata": {
    "id": "11isfvNZ9FGu"
   },
   "outputs": [],
   "source": [
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, filters_db = meteorological_rh_filter(plot_data, filters_db, meteo_filter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9305f5",
   "metadata": {
    "id": "oSX2h9QzzFkT"
   },
   "source": [
    "## rain events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377c38f",
   "metadata": {
    "id": "jz696mc09FlB"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  unroll_filters_db = filters_db.copy()\n",
    "  plot_data, filters_db = meteorological_rain_filter(plot_data, filters_db, meteo_filter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125aa85e",
   "metadata": {
    "id": "Xy2y00P1zJtZ"
   },
   "source": [
    "## day-time and night-time plausible ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25951968",
   "metadata": {
    "id": "ED_Qh6TS0Qkc"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  unroll_filters_db = filters_db.copy()\n",
    "  plot_data, filters_db = meteorological_night_filter(plot_data, filters_db, meteo_filter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4ad0c",
   "metadata": {
    "id": "X3Vguu8MK635"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  unroll_filters_db = filters_db.copy()\n",
    "  plot_data, filters_db = meteorological_day_filter(plot_data, filters_db, meteo_filter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07b70d",
   "metadata": {
    "id": "fzfTJdNe68Eu"
   },
   "source": [
    "## winter period filtering, determine the dates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4295c4",
   "metadata": {
    "id": "wJ87D57S7A91"
   },
   "outputs": [],
   "source": [
    "if ('winter_nee_limits' in meteo_filter_config.keys()) or ('winter_ch4_flux_limits' in meteo_filter_config.keys()):\n",
    "  plot_albedo(plot_data, filters_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7587a",
   "metadata": {
    "id": "Z_RAYINf67PO"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  unroll_filters_db = filters_db.copy()\n",
    "  date_ranges = [\n",
    "      ['01.01.2023 00:00', '26.03.2023 00:00'],\n",
    "      ['13.11.2023 00:00', '31.12.2023 00:00'],\n",
    "  ]\n",
    "  # date_ranges = []\n",
    "  # date_ranges.append(['25.8.2014 00:00', '26.8.2014 00:00'])\n",
    "  plot_data, filters_db = winter_filter(plot_data, filters_db, meteo_filter_config, date_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c68cdf",
   "metadata": {
    "id": "iipFLxf6fu5Y"
   },
   "source": [
    "Footprint filtering is expected further\n",
    "\n",
    "`fetch = 1 #or 0. 1 - remain, 0  eliminate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b9176",
   "metadata": {
    "id": "UAdRtCPGq6_y"
   },
   "source": [
    "# Statistical data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91104980",
   "metadata": {
    "id": "LcwZplknsHJv"
   },
   "source": [
    "## by minimum and maximum acceptable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5592c",
   "metadata": {
    "id": "FyJaM1zC1DDg"
   },
   "outputs": [],
   "source": [
    "# if config_meteo ['use_biomet']:\n",
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, filters_db = min_max_filter(plot_data, filters_db, min_max_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bf066",
   "metadata": {
    "id": "j62U1dw8sTEm"
   },
   "source": [
    "## by quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d3ced",
   "metadata": {
    "id": "aNQ4XDK01DME"
   },
   "outputs": [],
   "source": [
    "# if config_meteo ['use_biomet']:\n",
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, filters_db = quantile_filter(plot_data, filters_db, quantile_filter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdd8b3",
   "metadata": {
    "id": "7Sg76Bwasnb4"
   },
   "source": [
    "## by deviation from the average diurnal course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23232629",
   "metadata": {
    "id": "uoDvHhoQ2MMe"
   },
   "outputs": [],
   "source": [
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, filters_db = std_window_filter(plot_data, filters_db, window_filter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44632b87",
   "metadata": {
    "id": "iXl5RdINss9D"
   },
   "source": [
    "## MAD & Hampel outliers filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a61761",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gl9cImVr2MO3"
   },
   "outputs": [],
   "source": [
    "unroll_filters_db = filters_db.copy()\n",
    "plot_data, tmp_filter = mad_hampel_filter(plot_data, filters_db, madhampel_filter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22835fb9",
   "metadata": {
    "id": "iu8MLKyh1AFk"
   },
   "source": [
    "## Manual filtering\n",
    "\n",
    "If you need to remove some intervals manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5193d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ADy534At0_fN"
   },
   "outputs": [],
   "source": [
    "#  the filter excludes values from the first to the last one inclusively\n",
    "man_ranges = [\n",
    "    # ['1.5.2023 00:00', '1.6.2023 00:00'],\n",
    "    # ['25.8.2023 12:00', '25.8.2023 12:00'],\n",
    "]\n",
    "for start, stop in man_ranges:\n",
    "  plot_data, tmp_filter = manual_filter(plot_data, filters_db, col_name=\"nee\", range=[start, stop], value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b4d50",
   "metadata": {
    "id": "APyqyqSEHx3K"
   },
   "source": [
    "## To undo the last filter\n",
    "Doesn't work with the filters which were run several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703508d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IYmSC2gpH4zo"
   },
   "outputs": [],
   "source": [
    "#filters_db = unroll_filters_db.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917fe267",
   "metadata": {
    "id": "quGbtDaJ_gID"
   },
   "source": [
    "## Summary table of filter results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1ff98",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Pg78qGJ9_miW"
   },
   "outputs": [],
   "source": [
    "all_filters = {}\n",
    "for key, filters in filters_db.items():\n",
    "   if len(filters) > 0:\n",
    "    pl_data = plot_data.copy()\n",
    "    for filter_name in filters:\n",
    "      all_filters[filter_name] = []\n",
    "      all_filters[filter_name].append(len(pl_data.index))\n",
    "      filtered_amount = len(pl_data.query(f\"{filter_name}==0\").index)\n",
    "      all_filters[filter_name].append(filtered_amount)\n",
    "      # old_val =  len(pl_data.index)\n",
    "      pl_data = pl_data.query(f\"{filter_name}==1\")\n",
    "      # print(filter_name, filtered_amount, len(pl_data.index) - old_val)\n",
    "fdf_df = pd.DataFrame(all_filters)\n",
    "print(\"What percentage of data from all data (in %) was eliminated:\")\n",
    "print(fdf_df.iloc[1]/len(plot_data)*100)\n",
    "logging.info(\"What percentage of data from all data (in %) was eliminated:\")\n",
    "logging.info(fdf_df.iloc[1]/len(plot_data)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9398a3",
   "metadata": {
    "id": "gA_IPavss0bq"
   },
   "source": [
    "# Drawing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be749db",
   "metadata": {
    "id": "ijPM6mnJtMv8"
   },
   "source": [
    "## Plotting the results of data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7600ba",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "50Xhczc-BRc2"
   },
   "outputs": [],
   "source": [
    "plot_terator = iter(cols_to_investigate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9945d",
   "metadata": {
    "id": "uat4oESzU4__"
   },
   "source": [
    "To free up memory and ensure the proper work of the Colab, the graphs will be displayed one by one when the cell is rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834db32",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NhNoFAd7DqNN"
   },
   "outputs": [],
   "source": [
    "col2plot = next(plot_terator, False)\n",
    "col2plot = 'nee' #Insert the parameter of interest: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1, co2_signal_strength, ch4_flux\n",
    "# Or comment the previous line and rerun the cell for the switching to the next parameter\n",
    "if col2plot:\n",
    "  make_filtered_plot(plot_data, col2plot, filters_db)\n",
    "else:\n",
    "  print(\"No more data, start from the begining!\")\n",
    "  plot_terator = iter(cols_to_investigate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef11823",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZG_wF2qW-Qwb"
   },
   "outputs": [],
   "source": [
    "# #linear gapfilling, limit is the permitted maximal number of the consecutive missed values\n",
    "# for col in cols_to_investigate:\n",
    "#   plot_data[col] = plot_data[col].interpolate(limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b1cb9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VtJ8wyx2-XCX"
   },
   "outputs": [],
   "source": [
    "# #Gapfilling with the diurnal course\n",
    "# for col in cols_to_investigate:\n",
    "#   plot_data[col].fillna(bg.calc_rolling(plot_data[col], rolling_window=10, step=points_per_day, min_periods=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382614b1",
   "metadata": {
    "id": "MwuXRVTMtBz2"
   },
   "source": [
    "## Plotting the average diurnal course for the filtered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cebc6",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pWDTiucTgRlI"
   },
   "outputs": [],
   "source": [
    "plot_terator = iter(cols_to_investigate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c46b4",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "COKiwe7020D4"
   },
   "outputs": [],
   "source": [
    "#The example of the calculation of the diurnal course\n",
    "\n",
    "col2plot = next(plot_terator, False)\n",
    "#Can be chosen manually\n",
    "# col2plot = 'h'#\"co2_flux\"\n",
    "col2plot = ['nee', 'le'] #Insert the parameter of interest: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1\n",
    "#Or rerun the cell for the switching to the next parameter\n",
    "if col2plot:\n",
    "  basic_plot(plot_data, col2plot, filters_db, steps_per_day=points_per_day)\n",
    "else:\n",
    "  print(\"No more data, start from the begining!\")\n",
    "  plot_terator = iter(cols_to_investigate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee2a0f",
   "metadata": {
    "id": "RKEg6YBstXMp"
   },
   "source": [
    "## Heat maps of fluxes for the filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61092a15",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mCUJYURKEL-f"
   },
   "outputs": [],
   "source": [
    "for col in ['nee', 'le', 'h']: #Insert the parameter of interest: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1\n",
    "#Or rerun the cell for the switching to the next parameter\n",
    "  plot_nice_year_hist_plotly(plot_data, col ,time, filters_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26015886",
   "metadata": {
    "id": "EFscf-JZt3_R"
   },
   "source": [
    "# Recording the output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac6368",
   "metadata": {
    "id": "dokSxicNtdva"
   },
   "source": [
    "## A file for REddyProc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73fbe8",
   "metadata": {
    "id": "tDqsi61kSeak"
   },
   "source": [
    "Creating a header for the REddyProc file and save the required variables, the filtering is taken into account. The output file has the 3rd level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530ba62",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YVu2UrCzLqb4"
   },
   "outputs": [],
   "source": [
    "reddyproc_filename = f\"REddyProc_{ias_output_prefix}_{int(plot_data[time].dt.year.median())}.txt\"\n",
    "output_template = {'Year': ['-'],\t'DoY': ['-'],\t'Hour': ['-'],\t'NEE': ['umol_m-2_s-1'],\t'LE': ['Wm-2'],\t'H': ['Wm-2'],\t'Rg': ['Wm-2'],\t'Tair': ['degC'], \t'Tsoil': ['degC'],\t'rH': ['%'], \t'VPD': ['hPa'], \t'Ustar': ['ms-1'],\t'CH4flux': ['umol_m-2_s-1']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc07842",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GFulh7FtNWtM"
   },
   "outputs": [],
   "source": [
    "eddy_df = plot_data.copy()\n",
    "\n",
    "for column, filter in filters_db.items():\n",
    "  filter = get_column_filter(eddy_df, filters_db, column)\n",
    "  eddy_df.loc[~filter.astype(bool), column] = np.nan\n",
    "\n",
    "\n",
    "eddy_df['Year'] = eddy_df[time].dt.year\n",
    "eddy_df['DoY'] = eddy_df[time].dt.dayofyear\n",
    "eddy_df['Hour'] = eddy_df[time].dt.hour + eddy_df[time].dt.minute/60\n",
    "\n",
    "eddy_df['NEE'] = eddy_df['nee'].fillna(-9999)\n",
    "eddy_df['LE'] = eddy_df['le'].fillna(-9999)\n",
    "eddy_df['H'] = eddy_df['h'].fillna(-9999)\n",
    "if 'swin_1_1_1' in eddy_df.columns:\n",
    "  eddy_df['Rg'] = eddy_df['swin_1_1_1'].fillna(-9999)\n",
    "else:\n",
    "  print(\"WARNING! No swin_1_1_1!\")\n",
    "\n",
    "if config_meteo ['use_biomet']:\n",
    "  eddy_df['Tair'] = eddy_df['ta_1_1_1'].fillna(-9999)\n",
    "  eddy_df['rH'] = eddy_df['rh_1_1_1'].fillna(-9999)\n",
    "  eddy_df['VPD'] = eddy_df['vpd_1_1_1'].fillna(-9999)\n",
    "else:\n",
    "  eddy_df['Tair'] = (eddy_df['air_temperature'] - 273.15).fillna(-9999)\n",
    "  eddy_df['rH'] = eddy_df['rh'].fillna(-9999)\n",
    "  eddy_df['VPD'] = eddy_df['vpd'].fillna(-9999)\n",
    "\n",
    "if 'ts_1_1_1' in eddy_df.columns:\n",
    "  eddy_df['Tsoil'] = eddy_df['ts_1_1_1'].fillna(-9999)\n",
    "\n",
    "eddy_df['Ustar'] = eddy_df['u_star'].fillna(-9999)\n",
    "\n",
    "if 'ch4_flux' in eddy_df.columns:\n",
    "  eddy_df['CH4flux'] = eddy_df['ch4_flux'].fillna(-9999)\n",
    "\n",
    "i=0\n",
    "while eddy_df.iloc[i]['Hour'] != 0.5:\n",
    "  i += 1\n",
    "eddy_df = eddy_df.iloc[i:]\n",
    "\n",
    "if len(eddy_df.index) < 90 * points_per_day:\n",
    "  print(\"WARNING!  < 90 days in reddyproc file!\")\n",
    "\n",
    "pd.DataFrame({key: item for key, item in output_template.items() if key in eddy_df.columns}).to_csv(os.path.join('output', reddyproc_filename), index=False, sep=' ')\n",
    "eddy_df.to_csv(os.path.join('output', reddyproc_filename),  index=False, header=False, columns = [i for i in output_template.keys()  if i in eddy_df.columns], mode='a', sep=' ')\n",
    "del eddy_df\n",
    "logging.info(f\"REddyProc file saved to {os.path.join('output', reddyproc_filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b800a9",
   "metadata": {
    "id": "62o5-p8ZzR5T"
   },
   "source": [
    "## A file for Information and Analytical System used in the RuFlux network (IAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ffd62f",
   "metadata": {
    "id": "e50f7947"
   },
   "source": [
    "Level 2 file, written from the input data **without taking into account** filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fdad6",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yaLoIQmtzaYd"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "\tias_df = plot_data.copy()\n",
    "\tfor column, filter in filters_db.items():\n",
    "\t\tfilter = get_column_filter(ias_df, filters_db, column)\n",
    "\t\tias_df.loc[~filter.astype(bool), column] = np.nan\n",
    "\tias_df = ias_df.fillna(-9999)\n",
    "\n",
    "\tcol_match =  {\"co2_flux\" : \"FC_1_1_1\", \"qc_co2_flux\" : \"FC_SSITC_TEST_1_1_1\", \"LE\" : \"LE_1_1_1\",\n",
    "\t\t\"qc_LE\" : \"LE_SSITC_TEST_1_1_1\", \"H\" : \"H_1_1_1\", \"qc_H\" : \"H_SSITC_TEST_1_1_1\", \"Tau\" : \"TAU_1_1_1\",\n",
    "\t\t\"qc_Tau\" : \"TAU_SSITC_TEST_1_1_1\", \"co2_strg\" : \"SC_1_1_1\", \"co2_mole_fraction\" : \"CO2_1_1_1\",\n",
    "\t\t\"h2o_mole_fraction\" : \"H2O_1_1_1\", \"sonic_temperature\" : \"T_SONIC_1_1_1\", \"u_star\" : \"USTAR_1_1_1\",\n",
    "\t\t\"Ta_1_1_1\" : \"TA_1_1_1\", \"Pa_1_1_1\" : \"PA_1_1_1\", \"Swin_1_1_1\" : \"SW_IN_1_1_1\", \"Swout_1_1_1\" : \"SW_OUT_1_1_1\",\n",
    "\t\t\"Lwin_1_1_1\" : \"LW_IN_1_1_1\", \"Lwout_1_1_1\" : \"LW_OUT_1_1_1\", \"PPFD_1_1_1\" : \"PPFD_IN_1_1_1\",\n",
    "\t\t\"Rn_1_1_1\" : \"NETRAD_1_1_1\", \"MWS_1_1_1\" : \"WS_1_1_1\", \"Ts_1_1_1\" : \"TS_1_1_1\", \"Ts_2_1_1\" : \"TS_2_1_1\",\n",
    "\t\t\"Ts_3_1_1\" : \"TS_3_1_1\", \"Pswc_1_1_1\" : \"SWC_1_1_1\", \"Pswc_2_1_1\" : \"SWC_2_1_1\", \"Pswc_3_1_1\" : \"SWC_3_1_1\",\n",
    "\t\t\"SHF_1_1_1\" : \"G_1_1_1\", \"SHF_2_1_1\" : \"G_2_1_1\", \"SHF_3_1_1\" : \"G_3_1_1\", \"L\" : \"MO_LENGTH_1_1_1\",\n",
    "\t\t\"(z-d)/L\" : \"ZL_1_1_1\", \"x_peak\" : \"FETCH_MAX_1_1_1\", \"x_70%\" : \"FETCH_70_1_1_1\", \"x_90%\" : \"FETCH_90_1_1_1\",\n",
    "\t\t\"ch4_flux\" : \"FCH4_1_1_1\", \"qc_ch4_flux\" : \"FCH4_SSITC_TEST_1_1_1\", \"ch4_mole_fraction\" : \"CH4_1_1_1\", \"ch4_strg\" : \"SCH4_1_1_1\",\n",
    "\t\t\"ch4_signal_strength\" : \"CH4_RSSI_1_1_1\", \"co2_signal_strength\" : \"CO2_STR_1_1_1\", \"rh_1_1_1\": \"RH_1_1_1\", \"vpd_1_1_1\": \"VPD_1_1_1\"}\n",
    "\tcol_match = {key.lower(): item for key, item in col_match.items()}\n",
    "\n",
    "\tias_df = ias_df.rename(columns=col_match)\n",
    "\ttime_cols = ['TIMESTAMP_START', 'TIMESTAMP_END', 'DTime']\n",
    "\tvar_cols = [col_match[col] for col in col_match.keys() if col_match[col] in ias_df.columns]\n",
    "\n",
    "\tnew_time = pd.DataFrame(index=pd.date_range(start=f\"01.01.{ias_df[time].dt.year.min()}\", end=f\"01.01.{ias_df[time].dt.year.max()}\",\n",
    "                                                freq=ias_df.index.freq, inclusive='left'))\n",
    "\tias_df = new_time.join(ias_df, how='left')\n",
    "\tias_df[time] = ias_df.index\n",
    "\n",
    "\tias_df['TIMESTAMP_START'] = ias_df[time].dt.strftime('%Y%m%d%H%M')\n",
    "\tias_df['TIMESTAMP_END'] = (ias_df[time] + pd.Timedelta(0.5, \"H\")).dt.strftime('%Y%m%d%H%M')\n",
    "\tias_df['DTime'] = np.round(ias_df[time].dt.dayofyear + 1./48*2*ias_df[time].dt.hour + 1./48*(ias_df[time].dt.minute//30), decimals=3)\n",
    "\n",
    "\tif 'h_strg' in ias_df.columns:\n",
    "\t\tias_df['SH_1_1_1'] = ias_df['h_strg']\n",
    "\t\tvar_cols.append('SH_1_1_1')\n",
    "\tif 'le_strg' in ias_df.columns:\n",
    "\t\tias_df['SLE_1_1_1'] = ias_df['le_strg']\n",
    "\t\tvar_cols.append('SLE_1_1_1')\n",
    "\n",
    "\tif 'SW_IN_1_1_1' in ias_df.columns:\n",
    "\t\tias_df['SW_IN_1_1_1'] = data['swin_1_1_1']\n",
    "\n",
    "\tias_year = ias_df[time].dt.year.min()\n",
    "\tvar_cols.sort()\n",
    "\tcol_list_ias = time_cols + var_cols + [time]\n",
    "\tprint(col_list_ias)\n",
    "\tias_df = ias_df[col_list_ias]\n",
    "\n",
    "\tfor year in ias_df.index.year.unique():\n",
    "\t\tias_filename = f\"{ias_output_prefix}_{year}_{ias_output_version}.csv\"\n",
    "\t\tsave_data = ias_df.loc[ias_df[time].dt.year==year]\n",
    "\t\tsave_data = save_data.drop(time, axis=1)\n",
    "\t\tsave_data = save_data.fillna(-9999)\n",
    "\t\tif len(save_data.index) >= 5:\n",
    "\t\t\tsave_data.to_csv(os.path.join('output',ias_filename), index=False)\n",
    "\t\t\tlogging.info(f\"IAS file saved to {os.path.join('output',ias_filename)}.csv\")\n",
    "\t\telse:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tos.remove(os.path.join('output',ias_filename))\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(e)\n",
    "\n",
    "\t\t\tprint(f\"not enough data for {year}\")\n",
    "\t\t\tlogging.info(f\"{year} not saved, not enough data!\")\n",
    "\t# ias_filename = f\"{ias_output_prefix}_{ias_year}_{ias_output_version}.csv\"\n",
    "\t# ias_df.to_csv(os.path.join('output',ias_filename), index=False)\n",
    "\t# logging.info(f\"IAS file saved to {os.path.join('output',ias_filename)}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619ca8c",
   "metadata": {
    "id": "Pm8hiMrb_wRW"
   },
   "source": [
    "## File for FAT\n",
    "Level 3 file (filtered data) ready for input into the Flux Analysis Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117f901",
   "metadata": {
    "id": "0ll51nOal6Lz"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154aad4f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w9hkPLkB_zd1"
   },
   "outputs": [],
   "source": [
    "if config_meteo ['use_biomet']:\n",
    "  fat_output_template = {'DoY': ['--'], r'u*': ['m s-1'],\t'H': ['W m-2'], 'lE': ['-'],\t'NEE': ['umol m-2 s-1'],\t'PPFD': ['umol m-2 s-1'], 'Ta':['oC'], 'VPD':['kPa'], 'PPFD_gapfilling': ['umol m-2 s-1'], 'Ta_gapfilling': ['oC'], 'VPD_gapfilling': ['kPa'], 'period': ['--']}\n",
    "\n",
    "  fat_df = plot_data.copy()\n",
    "\n",
    "\n",
    "  for column, filter in filters_db.items():\n",
    "    filter = get_column_filter(fat_df, filters_db, column)\n",
    "    fat_df.loc[~filter.astype(bool), column] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "  fat_df['DoY'] = np.round(fat_df[time].dt.dayofyear + fat_df[time].dt.hour/24. + fat_df[time].dt.minute/24./60., decimals=3)\n",
    "  fat_df[r'u*'] = fat_df['u_star'].fillna(-99999)\n",
    "  fat_df['H'] = fat_df['h'].fillna(-99999)\n",
    "  fat_df['lE'] = fat_df['le'].fillna(-99999)\n",
    "  fat_df['NEE'] = fat_df['nee'].fillna(-99999)\n",
    "  if 'ppfd_1_1_1' in fat_df.columns:\n",
    "    fat_df['PPFD'] = fat_df['ppfd_1_1_1'].fillna(-99999)\n",
    "    fat_df['PPFD_gapfilling'] = fat_df['ppfd_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['ppfd_1_1_1'], rolling_window=10 , step=points_per_day, min_periods=4)).fillna(-99999)\n",
    "  else:\n",
    "    logging.info(f\"FAT file will have no PPFD\")\n",
    "    fat_output_template.pop('PPFD')\n",
    "\n",
    "  if not config_meteo ['use_biomet']:\n",
    "    fat_df['ta_1_1_1'] = fat_df['air_temperature'] - 273.15\n",
    "\n",
    "  fat_df['Ta'] = fat_df['ta_1_1_1'].fillna(-99999)\n",
    "  fat_df['VPD'] = fat_df['vpd_1_1_1'].fillna(-99999)\n",
    "\n",
    "  fat_df['period'] = fat_df.index.month%12//3 + 1\n",
    "\n",
    "  fat_df['Ta_gapfilling'] = fat_df['ta_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['ta_1_1_1'], rolling_window=10 , step=points_per_day, min_periods=4)).fillna(-99999)\n",
    "  fat_df['VPD_gapfilling'] = fat_df['vpd_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['vpd_1_1_1'], rolling_window=10 , step=points_per_day, min_periods=4)).fillna(-99999)\n",
    "\n",
    "  for year in fat_df.index.year.unique():\n",
    "    fat_filename = f\"FAT_{ias_output_prefix}_{year}.csv\"\n",
    "    pd.DataFrame(fat_output_template).to_csv(os.path.join('output',fat_filename), index=False)\n",
    "    save_data = fat_df.loc[fat_df[time].dt.year==year]\n",
    "    if len(save_data.index) >= 5:\n",
    "      save_data.to_csv(os.path.join('output',fat_filename),  index=False, header=False, columns = [i for i in fat_output_template.keys()], mode='a')#, sep=' ')\n",
    "    else:\n",
    "      try:\n",
    "        os.remove(os.path.join('output',fat_filename))\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "      print(f\"not enough data for {year}\")\n",
    "      logging.info(f\"{year} not saved, not enough data!\")\n",
    "  del fat_df\n",
    "  logging.info(f\"FAT file saved to {fat_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84bd51c",
   "metadata": {
    "id": "GQ1bpermu8eq"
   },
   "source": [
    "## File with all filtered data\n",
    "The file contains the original variables (fluxes, meteorological variables). The \"tmp_datetime\" column is the result of forming a single date-time from the two columns of the full output file - date, time. The \"datetime\" column is the result of the date-time correction for the tmp_datetime column. \"datetime_meteo\" is the result of the date-time correction for the \"timestamp_1\" column. The file contains flags for each filter to each variable (fluxes, meteorology) in binary format: 1 means that filter was not applied, 0 - applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc97114",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pk1lGANovC5U"
   },
   "outputs": [],
   "source": [
    "full_column_list = [c for c in plot_data.columns]\n",
    "full_column_list = full_column_list.insert(0, full_column_list.pop(full_column_list.index(time)))\n",
    "if 'date' in plot_data.columns:\n",
    "  plot_data.loc[plot_data['date'].isna(), 'date'] = plot_data[time].dt.date\n",
    "if 'time' in plot_data.columns:\n",
    "  plot_data.loc[plot_data['time'].isna(), 'time'] = plot_data[time].dt.time\n",
    "plot_data.fillna(-9999).to_csv(os.path.join('output','output_all.csv'), index=None, columns=full_column_list)\n",
    "logging.info(f\"Basic file saved to {os.path.join('output','output_all.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fee148",
   "metadata": {
    "id": "-MSrgUD0-19l"
   },
   "source": [
    "## Summary file of filtered results\n",
    "Short output file after filtering. Contains important input variables (meteorological parameters and fluxes), filtered important variables (index _filtered), integral flag for each variable, average diurnal cycles in 30- and 10-day windows for filtered variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878d3d2",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "22dPWc2u-6IG"
   },
   "outputs": [],
   "source": [
    "columns_to_save = ['Date', 'Time', 'DoY', 'ta', 'rh', 'vpd', 'swin', 'ppfd', 'p', 'h', 'le', 'co2_flux', 'co2_strg', 'ch4_flux', 'u_star']\n",
    "\n",
    "basic_df = plot_data.copy()\n",
    "\n",
    "basic_df['Date'] = basic_df[time].dt.date\n",
    "basic_df['Time'] = basic_df[time].dt.time\n",
    "basic_df['DoY'] = np.round(basic_df[time].dt.dayofyear + basic_df[time].dt.hour/24. + basic_df[time].dt.minute/24./60., decimals=3)\n",
    "\n",
    "if not config_meteo ['use_biomet']:\n",
    "  basic_df['ta_1_1_1'] = basic_df['air_temperature'] - 273.15\n",
    "#meteo\n",
    "for col in ['ta', 'rh', 'vpd', 'swin', 'ppfd', 'p']:\n",
    "  # print(f\"{col}(_[1-9]){{1,4}})\")\n",
    "  col_pos = [bool(re.fullmatch(f\"{col}(_[1-9]){{1,4}}\", col_in)) for col_in in  basic_df.columns]\n",
    "  if not any(col_pos):\n",
    "    continue\n",
    "  else:\n",
    "    real_col_name = basic_df.columns[np.argmax(col_pos)]\n",
    "    basic_df[col] = basic_df[real_col_name]\n",
    "\n",
    "#Filtered fluxes\n",
    "for col in ['nee', 'h', 'le', 'co2_strg', 'ch4_flux']:\n",
    "  if col not in basic_df.columns:\n",
    "    continue\n",
    "  basic_df[f\"{col}_filtered\"] = basic_df[col]\n",
    "  filter = get_column_filter(basic_df, filters_db, col)\n",
    "  basic_df.loc[~filter.astype(bool), f\"{col}_filtered\"] = np.nan\n",
    "  columns_to_save.append(f\"{col}_filtered\")\n",
    "\n",
    "#Filtered meteo\n",
    "for col in ['ta', 'rh', 'vpd', 'swin', 'ppfd', 'p']:\n",
    "  if col not in basic_df.columns:\n",
    "    continue\n",
    "  basic_df[f\"{col}_filtered\"] = basic_df[col]\n",
    "  filter = get_column_filter(basic_df, filters_db, col)\n",
    "  basic_df.loc[~filter.astype(bool), f\"{col}_filtered\"] = np.nan\n",
    "  columns_to_save.append(f\"{col}_filtered\")\n",
    "\n",
    "#Flags\n",
    "for col in ['ta', 'rh', 'vpd', 'swin', 'ppfd', 'p', 'h', 'le', 'co2_flux', 'co2_strg', 'nee', 'ch4_flux']: #['nee', 'ch4', 'le', 'h']:\n",
    "  if col not in basic_df.columns:\n",
    "    continue\n",
    "  basic_df[f\"{col}_integral_flag\"] = get_column_filter(basic_df, filters_db, col)\n",
    "  columns_to_save.append(f\"{col}_integral_flag\")\n",
    "\n",
    "# for key, item in filters_db.items():\n",
    "#   columns_to_save = columns_to_save + item\n",
    "\n",
    "for col in ['h', 'le', 'nee', 'rg', 'ppfd', 'ta', 'rh', 'vpd', 'ch4_flux']:\n",
    "  if f\"{col}_filtered\" not in basic_df.columns:\n",
    "    print(f\"No {col}_filtered in file\")\n",
    "    continue\n",
    "  col_out = col\n",
    "  if col == \"ppfd\":\n",
    "    col_out = \"rg\"\n",
    "  basic_df[f'{col_out}_10d'] = bg.calc_rolling(basic_df[f\"{col}_filtered\"], rolling_window=10 , step=points_per_day, min_periods=7)\n",
    "  basic_df[f'{col_out}_30d'] = bg.calc_rolling(basic_df[f\"{col}_filtered\"], rolling_window=30 , step=points_per_day, min_periods=7)\n",
    "  columns_to_save.append(f'{col_out}_10d')\n",
    "  columns_to_save.append(f'{col_out}_30d')\n",
    "\n",
    "basic_df = basic_df[[col for col in columns_to_save if col in basic_df.columns]]\n",
    "basic_df = basic_df.fillna(-9999)\n",
    "basic_df.to_csv(os.path.join('output','output_summary.csv'), index=None)\n",
    "logging.info(f\"New basic file saved to {os.path.join('output','output_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd7502",
   "metadata": {
    "id": "775a473e"
   },
   "source": [
    "# Processing with the REddyProc tool\n",
    "This block performs 1) filtering by the friction velocity threshold (u* threshold), 2) filling gaps in meteorological variables and 30-minute fluxes, 3) separating NEE into gross primary production (GPP) and ecosystem respiration (Reco), 4) calculating daily, monthly, annual averages and the average diurnal cycle by months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ef7b8",
   "metadata": {
    "id": "a8aa54de"
   },
   "source": [
    "## Technical block\n",
    "Required and automatically launched if a Google Colab environment is detected.\n",
    "Loads scripts used in cells into a directory `src` and prepares the R environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784204d3",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "06859169"
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# def section_*(): + ipynb to py convert?\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    class StopExecution(Exception):\n",
    "        def _render_traceback_(self):\n",
    "            return ['Colab env not detected. Current cell is only for Colab.']\n",
    "    raise StopExecution()\n",
    "\n",
    "cur_dir = %pwd\n",
    "assert cur_dir == '/content'\n",
    "\n",
    "!mkdir -p src/repo1/\n",
    "%cd src/repo1/\n",
    "\n",
    "!git -c init.defaultBranch=main init\n",
    "!git sparse-checkout init\n",
    "!git sparse-checkout set \"src\"\n",
    "!git remote add origin https://github.com/PlaZMaD/climate.git\n",
    "!git fetch --depth 1 origin v0.9.4\n",
    "!git -c advice.detachedHead=false checkout FETCH_HEAD\n",
    "\n",
    "%cd /content\n",
    "!cp -r src/repo1/src .\n",
    "\n",
    "# 1.3.2 vs 1.3.3 have slightly different last columns\n",
    "# alternative for windows\n",
    "# install.packages('https://cran.r-project.org/bin/windows/contrib/4.1/REddyProc_1.3.2.zip', repos = NULL, type = \"binary\")\n",
    "setup_colab_r_code = \"\"\"\n",
    "install_if_missing <- function(package, version, repos) {\n",
    "    if (!require(package, character.only = TRUE)) {\n",
    "        remotes::install_version(package, version = version, upgrade = \"never\", repos = repos)\n",
    "        library(package, character.only = TRUE)\n",
    "    }\n",
    "}\n",
    "# sink redirect is required to improve ipynb output\n",
    "sink(stdout(), type = \"message\")\n",
    "install_if_missing(\"REddyProc\", \"1.3.3\", repos = 'http://cran.rstudio.com/')\n",
    "sink()\n",
    "\"\"\"\n",
    "from rpy2 import robjects\n",
    "robjects.r(setup_colab_r_code)\n",
    "\n",
    "from src.ipynb_helpers import enable_word_wrap\n",
    "enable_word_wrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f997d99",
   "metadata": {
    "id": "034b04a5"
   },
   "source": [
    "## Filtering and gapfilling\n",
    "\n",
    "`ig.eddyproc_options` are the settings that correspond to the options of [the online tool](https://www.bgc-jena.mpg.de/REddyProc/ui/REddyProc.php).\n",
    "\n",
    "**It is necessary to check:**  \n",
    "\n",
    "Enabling detection of the conditions of weak turbulence  \n",
    "`is_to_apply_u_star_filtering=True`\n",
    "\n",
    "The Eddy Covariance method is applicable only in the conditions of the developed turbulence. When the friction velocity (*uStar* column) is below a certain threshold, the CO2 flux may be underestimated. Flux data in these conditions are replaced by gaps.\n",
    "\n",
    "Selection of the method of marking seasons, for each of which the saturation level is determined separately. `Continuous` - the beginning of seasons is in March, June, September, and December, December is included in the *next* year. `WithinYear` treats each year separately. `User` divides the year by the user-defined *season* column.  \n",
    "`u_star_seasoning=\"Continuous\"`\n",
    "\n",
    "Compared to the original REddyProc tool, this notebook adds the ability to substitute a custom threshold value in cases where the threshold cannot be calculated (e.g., if data are lacking or solar income radiation *Rg* is missing). For grassland ecosystems, you can use a minimal threshold of `0.01`, for forest ecosystems - `0.1`, to disable substitution - `None`.  \n",
    "`ustar_threshold_fallback=0.01`\n",
    "\n",
    "REddyProc by default applies the uStar threshold only at night, which requires the *Rg* column to determine day/night-time. The following experimental option allows you to ignore the absence of *Rg* and apply the threshold to all data, regardless of the time of day  \n",
    "`ustar_allowed_on_days=True`\n",
    "\n",
    " If an error occurs during filtering, a warning appears in the cell log and the cell reruns with a transition to the gap filling. Gap filling in the 30-minute fluxes corresponds to the online tool and is enabled by default.\n",
    "\n",
    "Enable and select one or both methods of partitioning the CO2 flux into the gross primary production (GPP) and the ecosystem respiration (Reco). The method `Reichstein05` performs the separation based on the night-time data, the method `Lasslop10` uses the day-time data.  \n",
    "`is_to_apply_partitioning=True`  \n",
    "`partitioning_methods=[\"Reichstein05\", \"Lasslop10\"]`\n",
    "\n",
    "If there is no data on Rg, the partitioning will not be performed! To correctly run the cell in this case, set  \n",
    "`is_to_apply_partitioning=False`\n",
    "\n",
    "Latitude, longitude, time zone  \n",
    "`latitude = 56.5`  \n",
    "`longitude = 32.6`  \n",
    "`timezone = +3`  \n",
    "\n",
    "To use air or soil temperature for gapfilling  \n",
    "`temperature_data_variable=\"Tair\"`\n",
    "or  \n",
    "`temperature_data_variable=\"Tsoil\"`\n",
    "\n",
    "**Options that are not intended to be changed in this notebook:**\n",
    "\n",
    "EddyProc only has the moving point method `RTw` for determining the uStar threshold  \n",
    "`u_star_method=\"RTw\"`  \n",
    "Iteratively assessing the accuracy of the calculated u* threshold (bootstrap)  \n",
    "`is_bootstrap_u_star=False`  \n",
    "Filling gaps in 30-minute fluxes  \n",
    "`is_to_apply_gap_filling=True`  \n",
    "\n",
    "**Additional options (consistent with previous sections):**  \n",
    "\n",
    "The name of the station, added to the names of the output files:  \n",
    "`site_id=ias_output_prefix`  \n",
    "The file from which the time series are loaded:  \n",
    "`input_file=\"REddyProc.txt\"`  \n",
    "The directory to which the tool writes test graphs, basic statistics on gaps, filled series:  \n",
    "`output_dir=\"output/reddyproc\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119217a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "278caec5"
   },
   "outputs": [],
   "source": [
    "from src.ipynb_globals import *\n",
    "from types import SimpleNamespace\n",
    "from src.reddyproc.reddyproc_bridge import reddyproc_and_postprocess\n",
    "import src.ipynb_globals as ig\n",
    "from src.helpers.io_helpers import ensure_empty_dir\n",
    "\n",
    "ig.eddyproc = SimpleNamespace()\n",
    "ig.eddyproc.options = SimpleNamespace(\n",
    "    site_id=ias_output_prefix,\n",
    "\n",
    "    is_to_apply_u_star_filtering=True,\n",
    "    # if default REP cannot detect threshold, this value may be used instead; None to disable\n",
    "    ustar_threshold_fallback=0.01,\n",
    "    # default REP detects nights by Rg; when Rg is missing, this is experimental fallback to apply uStar over all data\n",
    "    ustar_allowed_on_days=True,\n",
    "\n",
    "    # u_star_seasoning: one of \"WithinYear\", \"Continuous\", \"User\"\n",
    "    u_star_seasoning=\"Continuous\",\n",
    "\n",
    "    is_to_apply_partitioning=True,\n",
    "\n",
    "    # partitioning_methods: one or both of \"Reichstein05\", \"Lasslop10\"\n",
    "    partitioning_methods=[\"Reichstein05\", \"Lasslop10\"],\n",
    "    latitude=56.5,\n",
    "    longitude=32.6,\n",
    "    timezone=+3.0,\n",
    "\n",
    "    # \"Tsoil\"\n",
    "    temperature_data_variable=\"Tair\",\n",
    "\n",
    "    # do not change\n",
    "    u_star_method=\"RTw\",\n",
    "    is_bootstrap_u_star=False,\n",
    "    is_to_apply_gap_filling=True,\n",
    "    input_file=f\"output/{reddyproc_filename}\",\n",
    "    output_dir=\"output/reddyproc\",\n",
    "    log_fname_end='_log.txt'\n",
    ")\n",
    "\n",
    "ensure_empty_dir(ig.eddyproc.options.output_dir)\n",
    "ig.eddyproc.out_info, ig.eddyproc.options = reddyproc_and_postprocess(ig.eddyproc.options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e61de",
   "metadata": {
    "id": "0bed439c"
   },
   "source": [
    "## Test graphs\n",
    "Displays individual graphs from the online tool in a convenient form for checking.\n",
    "The filled data, graphs and statistics can be downloaded in one archive by clicking the **Download eddyproc outputs** button.\n",
    "\n",
    "**Additional options:**  \n",
    "The order and set of graphs are formed automatically in the variable `output_sequence`, which can be changed or re-declared using tags.\n",
    "Tags for this particular version of the notebook will be visible after the cell is run by calling `display_tag_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d2505",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "e66a94ab"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import src.ipynb_globals as ig\n",
    "from src.helpers.io_helpers import create_archive\n",
    "from src.reddyproc.postprocess_graphs import EProcOutputHandler, EProcImgTagHandler, EProcOutputGen\n",
    "from src.colab_routines import colab_add_download_button, colab_no_scroll\n",
    "\n",
    "tag_handler = EProcImgTagHandler(main_path='output/reddyproc', eproc_options=ig.eddyproc, img_ext='.png')\n",
    "eog = EProcOutputGen(tag_handler)\n",
    "\n",
    "output_sequence: Tuple[Union[List[str], str], ...] = (\n",
    "    \"## Heat maps\",\n",
    "    eog.hmap_compare_row('NEE_*'),\n",
    "    eog.hmap_compare_row('LE_f'),\n",
    "    eog.hmap_compare_row('H_f'),\n",
    "    \"## Diurnal course\",\n",
    "    eog.diurnal_cycle_row('NEE_*'),\n",
    "    eog.diurnal_cycle_row('LE_f'),\n",
    "    eog.diurnal_cycle_row('H_f'),\n",
    "    \"## 30-minute fluxes and daily averages\",\n",
    "    eog.flux_compare_row('NEE_*'),\n",
    "    eog.flux_compare_row('LE_f'),\n",
    "    eog.flux_compare_row('H_f')\n",
    ")\n",
    "\n",
    "eio = EProcOutputHandler(output_sequence=output_sequence, tag_handler=tag_handler, out_info=ig.eddyproc.out_info)\n",
    "eio.prepare_images_safe()\n",
    "ig.arc_exclude_files = eio.img_proc.raw_img_duplicates\n",
    "\n",
    "eproc_arc_path = Path('output/reddyproc') / Path(ig.eddyproc.out_info.fnames_prefix + '.zip')\n",
    "create_archive(arc_path=eproc_arc_path, folders='output/reddyproc', top_folder='output/reddyproc',\n",
    "               include_fmasks=['*.png', '*.csv', '*.txt'], exclude_files=eio.img_proc.raw_img_duplicates)\n",
    "\n",
    "colab_add_download_button(eproc_arc_path, 'Download eddyproc outputs')\n",
    "\n",
    "colab_no_scroll()\n",
    "eio.display_images_safe()\n",
    "\n",
    "tag_handler.display_tag_info(eio.extended_tags())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90da109",
   "metadata": {
    "id": "HEead6faY22W"
   },
   "source": [
    "# Downloading results\n",
    "\n",
    "The results of all the segments of the notebook can be downloaded in one archive using the **Download outputs** button.  \n",
    "\n",
    "If the button below does not appear, you need to run the cell again or download the output files in the Files section, output directory. In the summary files with indexes in the name _hourly (daily variations of filtered and filled variables), _daily (average daily values), _monthly (average monthly values) and _yearly (values for the year, if there is less data - for the entire processing period) the index _sqc means the percentage of values remaining after filtering (but without taking into account the REddyProc filter on u*), and the columns with indexes _f mean the final filled data after all the cells of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a2433",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "E4rv4ucOX8Yz"
   },
   "outputs": [],
   "source": [
    "from src.helpers.io_helpers import create_archive\n",
    "from pathlib import Path\n",
    "import src.ipynb_globals as ig\n",
    "\n",
    "arc_path=Path('output') / 'FluxFilter_output.zip'\n",
    "create_archive(arc_path=arc_path, folders=['output', 'output/reddyproc'], top_folder='output',\n",
    "               include_fmasks=['*.png', '*.csv', '*.txt', '*.log'], exclude_files=ig.arc_exclude_files)\n",
    "colab_add_download_button(arc_path, 'Download outputs')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
