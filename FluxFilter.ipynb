{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqQVYpfkwA8E"
      },
      "source": [
        "# **FluxFilter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE87fcFbwlIu"
      },
      "source": [
        "## **Введение**\n",
        "Этот скрипт разработан для визуализации, фильтрации и заполнения 30-минутных данных с вышек, работающих по методу турбулентных пульсаций (eddy covariance). Скрипт работает с данными об экосистемных потоках тепла и парниковых газов, а также 30-минутными метеорологическими данными. Иначе, это инструмент перевода данных из уровня 1 в уровни 2, 3 и 4.\n",
        "* Под уровнем 1 подразумеваются потоки, как они рассчитаны EddyPro (LI-COR Inc., США) и \"сырая\" 30-минутная метеорология в том виде, как она записывается на регистраторы на вышках.\n",
        "* Под уровнем 2 подразумеваются незаполненные 30-минутные данные, ответственный за станцию исключает периоды заведомо плохой работы приборов (т.е. данные за эти периоды заполнены кодом пропуска -9999). Такие данные собираются для Информационно-Аналитической системы (ИАС, разработчик ИКИ РАН).\n",
        "* Под уровнем 3 подразумеваются данные уровня 2, также незаполненные, но прошедшие тщательную физическую, статистическую и (в случае надобности) визуальную фильтрацию.\n",
        "* Под уровнем 4 подразумеваются заполненные данные.\n",
        "\n",
        "*Самая новая версия скрипта находится в репозитории https://github.com/PlaZMaD/climate/releases*  \n",
        "*Для запуска достаточно нажать в Google Colab \"Среда выполнения - Выполнить все\"*\n",
        "\n",
        "## **Входные файлы**\n",
        "Форматы входных файлов: выходной файл EddyPro - full output (см. [мануал EddyPro](https://licor.app.boxenterprise.net/s/1ium2zmwm6hl36yz9bu4)) для потоков, а также biomet-файл EddyPro (см. [его же](https://licor.app.boxenterprise.net/s/1ium2zmwm6hl36yz9bu4)) для метеорологии. Турбулентные потоки и u* берутся из файла full output, а все метеорологические переменные (температура и относительная влажность воздуха и т.д.) берутся из файла biomet. Основные требования ко входным файлам:\n",
        "*   Файлы должны быть в формате .csv (текстовый файл, разделенный запятыми).\n",
        "*   Заголовки столбцов должны быть строго по руководству EddyPro, в скрипте переменные идентифицируются по названиям колонок (co2_flux для потока CO2 в full output, Ta_1_1_1 для температуры воздуха в biomet и т.д.).\n",
        "*   Код пропуска во входных файлах должен быть -9999\n",
        "*   Единицы для переменных файла biomet должны быть как основные единицы для файла biomet по руководству EddyPro. Исключение: температура воздуха/почвы должна быть в градусах Цельсия\n",
        "*   Файл-пример full output можно скачать [здесь](https://drive.google.com/file/d/1TyuHYZ0uh5teRiRFAga0XIqfU4vYW4-N/view?usp=sharing)\n",
        "*   Файл-пример biomet можно скачать [здесь](https://drive.google.com/file/d/1FjiBcSspDBlYlcg9Vzy71Sm49gOFZGBF/view?usp=sharing)\n",
        "*   В файле full output должны быть 3 строки заголовка и названия переменных должны быть записаны во 2-й строке\n",
        "*   В файле biomet должны быть 2 строки заголовка и названия переменных должны быть записаны в 1-й строке. По умолчанию без проблем читаются файлы, у которых дата и время записаны в колонке TIMESTAMP_1 в формате yyyy-mm-dd HHMM\n",
        "\n",
        "## **Выходные файлы**\n",
        "Форматы выходных файлов (собраны в архиве FluxFilter_output.zip):\n",
        "1.   Файл базы данных ИАС уровня 2;\n",
        "2.   Входной файл для инструмента фильтрации по u*, заполнения пропусков и разделения потоков [REddyProcWeb](https://www.bgc-jena.mpg.de/5624918/Input-Format) (Институт Макса Планка, Германия). Этот же файл используется как входной для раздела \"Обработка утилитами REddyProc\" данного скрипта.\n",
        "3. Входной файл для инструмента заполнения пропусков [Flux Analysis Tool](https://atmenv.envi.osakafu-u.ac.jp/staff/ueyama/softwares/) (M. Ueyama, Япония)\n",
        "4. Файл basic – все исходные переменные и флаги применения фильтров.\n",
        "5. Файл basic_new – запись для основных переменных исходных данных, отфильтрованных данных, флаг применения каждого фильтра, средние суточные ходы в окне 30 дней.\n",
        "6. Лог - записи в ходе работы скрипта, введенные для фильтрации параметры в данном пробеге\n",
        "7. Директория reddyproc содержит результаты заполнения переменных в таком же формате, что и оригинальный инструмент  REddyProc\n",
        "(https://www.bgc-jena.mpg.de/5624929/Output-Format). Помимо этого, в папке reddyproc содержатся обобщающие файлы с индексами _hourly (суточные ходы оригинальных и заполненных переменных),_daily (средние суточные значения), _monthly (средние месячные значения) и _yearly (значения за год, если данных меньше - за весь период обработки).\n",
        "\n",
        "## **Загрузка входных файлов**\n",
        "*   загрузить на google-диск файлы full output и biomet,\n",
        "*   открыть к ним доступ\n",
        "*   в конфиге загрузки данных заменить названия входных файлов на импортируемые\n",
        "*   в конфиге загрузки данных проверить формат входных даты и времени\n",
        "*   скопировать часть публичной ссылки в раздел Загружаем данные в команду !gdown\n",
        "\n",
        "## **Перед фильтрацией**\n",
        "*   Можно загружать несколько файлов full output и biomet, они будут автоматически расположены по возрастанию дат-времени и слиты в одну таблицу\n",
        "*   Осуществляется проверка меток времени для каждого входного файла (регуляризация)\n",
        "*   Рассчитываются VPD <-> RH, SWIN <-> RG <-> PAR в случае отсутствия\n",
        "*   Можно работать с потоком CO2 либо проверить данные о накоплении, прибавить их к потоку CO2 и работать с NEE\n",
        "\n",
        "## **Как происходит фильтрация**\n",
        "Скрипт позволяет выявить и удалить некачественные и выбивающиеся значения с помощью  1) физической, 2) статистической  фильтрации, проходящей под визуальным контролем - с просмотром графиков до фильтраций и после.\n",
        "1. Физическая фильтрация включает удаление плохих значений потоков с флагом EddyPro больше порогового, при уровне сигнала газоанализатора (CO2SS) ниже порогового значения, в дождь и после дождей, при высокой влажности, по ночным и дневным допустимым диапазонам, по допустимому диапазону зимой.\n",
        "2. Статистическая фильтрация включает удаление выбивающихся значений (outliers/spikes/выбросы/пики/спайки) с помощью фильтров по минимальным и максимальным допустимым значениям, по перцентилям, по отклонениям от среднего суточного хода в окне несколько дней, отклонениям от средних в скользящем окне на несколько точек MAD (Sachs, 2006) и HAMPEL (Pearson et al., 2016).\n",
        "3. Опцию визуальной фильтрации данных (ручное удаление точек при просмотре графика) Google Colab не позволяет реализовать, но в версии для запуска в среде программирования визуальная фильтрация планируется.\n",
        "4. Можно исключить данные по списку интервалов (исключить с ... - по ...), например, калибровки по журналу технических работ.\n",
        "\n",
        "## **Скачивание выходных файлов**\n",
        "Все выходные файлы можно скачать в последнем разделе \"Выгрузка результатов\", нажав кнопку \"Download outputs\".\n",
        "\n",
        "(с)Евгений Курбатов, Вадим Мамкин, Ольга Куричева\n",
        "(с)Инструмент REddyProc: Wutzler T, Lucas-Moffat A, Migliavacca M, Knauer J, Sickel K, Sigut, Menzer O & Reichstein M (2018) Basic and extensible post-processing of eddy covariance flux data with REddyProc. Biogeosciences, Copernicus, 15, doi: 10.5194/bg-15-5015-2018\n",
        "(с)Адаптация REddyProc и постобработка: Олег Дещеревский\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj6Z0gnhVM-R"
      },
      "source": [
        "# Технический блок\n",
        "Импорт библиотек и определение функций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZliIHxRJiqk"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "# key = userdata.get('registry_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT2KP0eYk1r3"
      },
      "outputs": [],
      "source": [
        "!mkdir output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-a6ANTGBsqg"
      },
      "outputs": [],
      "source": [
        "%pip install plotly-resampler dateparser >> /dev/null\n",
        "# %pip install --index-url https://public:{key}@gitlab.com/api/v4/projects/55331319/packages/pypi/simple --no-deps bglabutils==0.0.21 >> /dev/null\n",
        "%pip install --index-url https://gitlab.com/api/v4/projects/55331319/packages/pypi/simple --no-deps bglabutils==0.0.21 >> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywv5kp0rzanK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pylab as plt\n",
        "import os\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "import dateutil\n",
        "from copy import deepcopy as copy\n",
        "\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "import plotly_resampler\n",
        "import dateparser\n",
        "\n",
        "import bglabutils.basic as bg\n",
        "import bglabutils.filters as bf\n",
        "\n",
        "import logging\n",
        "import re\n",
        "# import bglabutils.boosting as bb\n",
        "# import textwrap\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, filename=\"/content/output/log.log\", filemode=\"w\", force=True)\n",
        "logging.info(\"START\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_5uwjkzfk45"
      },
      "source": [
        "## Функции для отрисовки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AXOLjh5VeMp"
      },
      "outputs": [],
      "source": [
        "def colapse_filters(data, filters_db_in):\n",
        "  out_filter = {}\n",
        "  for feature, filters in filters_db_in.items():\n",
        "    if len(filters)>0:\n",
        "      out_filter[feature] = data[filters[0]].astype(int) if len(filters)==1 else np.logical_and.reduce((data[filters].astype(int)), axis=1).astype(int)\n",
        "  return out_filter\n",
        "\n",
        "def get_column_filter(data, filters_db_in, column_name):\n",
        "  if column_name not in filters_db_in.keys():\n",
        "    return np.array([1]*len(data.index))\n",
        "  if len(filters_db_in[column_name]) > 0:\n",
        "    return colapse_filters(data, filters_db_in)[column_name]\n",
        "  else:\n",
        "    return np.array([1]*len(data.index))\n",
        "\n",
        "def basic_plot( data, col2plot, filters_db=None,  min_days=8, window_days = 10, steps_per_day=2*24, use_resample=False):\n",
        "\n",
        "  multiplot = isinstance(col2plot, list)\n",
        "\n",
        "  window_days = window_days   # дней в окне\n",
        "  min_days = window_days//2 - 1\n",
        "  pl_data = data.copy()\n",
        "\n",
        "  layout = go.Layout(\n",
        "      paper_bgcolor='rgba(0,0,0,0)',\n",
        "      plot_bgcolor='rgba(0,0,0,0)'\n",
        "  )\n",
        "  color_data = 'darkorange'\n",
        "  color_line = 'darkslateblue'\n",
        "\n",
        "  add_color_data = copy(px.colors.qualitative.Pastel1)\n",
        "  add_color_line = copy(px.colors.qualitative.Prism)\n",
        "\n",
        "  add_color_data.insert(0, color_data)\n",
        "  add_color_line.insert(0, color_line)\n",
        "\n",
        "  fig = go.Figure(layout=layout)\n",
        "  if multiplot:\n",
        "    fig = make_subplots(rows=len(col2plot), cols=1, shared_xaxes=True, figure=fig, subplot_titles=[i.upper() for i in col2plot])\n",
        "  else:\n",
        "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, figure=fig, row_heights=[.8, .2], subplot_titles=[col2plot.upper(), 'Residuals'])\n",
        "\n",
        "  fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey', minor_ticks='inside', minor_tickcolor='Grey')\n",
        "  fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey')\n",
        "  # fig.update_layout(\n",
        "  #     title = col2plot,\n",
        "  #     xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
        "  # )\n",
        "  if not multiplot:\n",
        "    cols = [col2plot]\n",
        "  else:\n",
        "    cols = col2plot\n",
        "\n",
        "  fig.update_layout(\n",
        "    # title = \" \".join(cols),\n",
        "    xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
        "  )\n",
        "  for row, col2plot in enumerate(cols):\n",
        "    if filters_db is not None:\n",
        "      filters =  get_column_filter(pl_data, filters_db, col2plot)\n",
        "      pl_data.loc[~filters.astype(bool), col2plot] = np.nan\n",
        "\n",
        "    if steps_per_day % 2 == 0:\n",
        "      closed='left'\n",
        "    else:\n",
        "      closed='both'\n",
        "    rolling_mean = bg.calc_rolling(pl_data[col2plot], step=steps_per_day, rolling_window=window_days, min_periods=min_days)\n",
        "\n",
        "    fig.add_trace(go.Scattergl(x=pl_data.index, y=pl_data[col2plot], mode='markers', name=col2plot, marker_color=add_color_data[row]), row=row+1, col=1)\n",
        "    fig.add_trace(go.Scattergl(x=rolling_mean.index, y=rolling_mean, mode='lines', name=f'{col2plot} mean {window_days} days', opacity=.7, line_color=add_color_line[row]), row=row+1, col=1)\n",
        "    if not multiplot:\n",
        "      fig.add_trace(go.Scattergl(x=rolling_mean.index, y=rolling_mean - pl_data[col2plot], mode='lines', name=f'residuals'), row=2, col=1)\n",
        "\n",
        "  if use_resample:\n",
        "    fig = plotly_resampler.FigureResampler(fig, default_n_shown_samples=5000)\n",
        "\n",
        "  fig_name = f\"_{int(np.median(pl_data.index.year))}\"\n",
        "  if \"ias_output_prefix \" in locals() or \"ias_output_prefix\" in globals():\n",
        "    fig_name = fig_name + \"_\" + ias_output_prefix\n",
        "  fig_config = {'toImageButtonOptions': {'filename': '_'.join(cols)+fig_name,}}\n",
        "  fig.show(config=fig_config)\n",
        "\n",
        "\n",
        "\n",
        "def plot_nice_year_hist_plotly(df, to_plot, time_col, filters_db):\n",
        "    pl_data = df.copy()#[to_plot]\n",
        "    # point\n",
        "    if filters_db is not None:\n",
        "      print()\n",
        "      filters =  get_column_filter(df, filters_db, to_plot)\n",
        "      pl_data['filter'] = filters\n",
        "      pl_data.loc[~filters.astype(bool), to_plot] = np.nan\n",
        "    # print(pl_data.loc[pd.to_datetime('26 June 2016 1:30'), ['nee', 'nee_nightFilter', 'swin_1_1_1', 'filter']].to_string())\n",
        "    fig = go.Figure()\n",
        "    fig.update_layout(title = f'{to_plot}')\n",
        "    fig.add_trace(go.Heatmap(x=pl_data[time_col].dt.date, y=pl_data[time_col].dt.hour + 0.5*(pl_data[time_col].dt.minute//30), z=pl_data[to_plot]))\n",
        "    fig_config = {'toImageButtonOptions': {'filename': f'{to_plot}_{int(np.median(pl_data.index.year))}',}}\n",
        "\n",
        "    fig.show(config=fig_config)\n",
        "\n",
        "\n",
        "def make_filtered_plot(data_pl, col, filters_db):\n",
        "  data = data_pl.copy()\n",
        "  layout = go.Layout(\n",
        "      paper_bgcolor='rgba(0,0,0,0)',\n",
        "      plot_bgcolor='rgba(0,0,0,0)'\n",
        "  )\n",
        "  add_color_dot = copy(px.colors.qualitative.Dark24)\n",
        "  fig = go.Figure(layout=layout)\n",
        "  fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey', minor_ticks='inside', minor_tickcolor='Grey')\n",
        "  fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey')\n",
        "\n",
        "  data['full_filter'] =  get_column_filter (data, filters_db, col)\n",
        "  data['full_filter'] = data['full_filter'].astype(int)\n",
        "  pl_data = data.query(f\"full_filter==0\")\n",
        "  color_ind = 0\n",
        "  fig.add_trace(go.Scattergl(x=data.query(\"full_filter==1\").index, y=data.query(\"full_filter==1\")[col], mode='markers', name=\"Good data\", marker_color=add_color_dot[color_ind] ))\n",
        "  color_ind += 1\n",
        "\n",
        "  if len(filters_db[col]) > 0:\n",
        "    for filter_name in filters_db[col]:\n",
        "      fig.add_trace(go.Scattergl(x=pl_data.query(f\"{filter_name}==0\").index, y=pl_data.query(f\"{filter_name}==0\")[col], mode='markers',   name=filter_name, marker_color=add_color_dot[color_ind]))\n",
        "      color_ind += 1\n",
        "      pl_data = pl_data.query(f\"{filter_name}==1\")\n",
        "\n",
        "  fig.update_layout(\n",
        "      title = f'{col2plot}',\n",
        "      xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
        "  )\n",
        "  fileName = \"basic\"\n",
        "  if \"ias_output_prefix \" in locals() or \"ias_output_prefix\" in globals():\n",
        "    fileName = ias_output_prefix\n",
        "  fileName += f'_{int(np.median(data.index.year))}_{col}'\n",
        "  fig_config = {'toImageButtonOptions': {'filename': fileName,}}\n",
        "  fig.show(config=fig_config)\n",
        "\n",
        "\n",
        "def plot_albedo (plot_data, filters_db):\n",
        "  pl_data = plot_data.copy()\n",
        "\n",
        "  layout = go.Layout(\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    plot_bgcolor='rgba(0,0,0,0)'\n",
        "    )\n",
        "\n",
        "\n",
        "  if ('swin_1_1_1' not in pl_data.columns) or ('swout_1_1_1' not in pl_data.columns):\n",
        "    print(\"No swin_1_1_1/sout_1_1_1\")\n",
        "    return 0\n",
        "  pl_data['albedo'] = pl_data['swout_1_1_1'].div(pl_data['swin_1_1_1'])\n",
        "  pl_data.loc[pl_data['swin_1_1_1']<=0, 'albedo'] = np.nan\n",
        "  pl_data.loc[pl_data['swout_1_1_1']<=0, 'albedo'] = np.nan\n",
        "\n",
        "  pl_ind  = pl_data[pl_data['albedo']<pl_data['albedo'].quantile(0.95)].index\n",
        "  fig = go.Figure(layout=layout)\n",
        "  fig.add_trace(go.Scattergl(x=pl_data.loc[pl_ind].index, y=pl_data.loc[pl_ind, 'albedo'], name=\"Albedo\"))\n",
        "  fig.update_layout(title = 'Albedo')\n",
        "  fig_config = {'toImageButtonOptions': {'filename': 'albedo',}}\n",
        "  fig.show(config=fig_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKznP_r1foao"
      },
      "source": [
        "## Функции для фильтрации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuUwWEPRaVT5"
      },
      "outputs": [],
      "source": [
        "def min_max_filter(data_in, filters_db_in, config):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    for col, limits in config.items():\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "\n",
        "      data[f\"{col}_minmaxfilter\"] = filter\n",
        "\n",
        "      if col not in ['rh_1_1_1', 'swin_1_1_1', 'ppfd_1_1_1', 'swin_1_1_1']:\n",
        "        data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
        "      else:\n",
        "        if col == 'rh_1_1_1':\n",
        "          data[col] = data[col].clip(upper=limits[1])\n",
        "          data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
        "        else:\n",
        "          data[col] = data[col].clip(lower=limits[0])\n",
        "          if col not in ['swin_1_1_1']:\n",
        "            data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
        "          else:\n",
        "            data.loc[data.query(f\"{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
        "\n",
        "      if f\"{col}_minmaxfilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_minmaxfilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"min_max_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def qc_filter(data_in, filters_db_in, config):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col, limits in config.items():\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      data[f\"{col}_qcfilter\"] = filter\n",
        "      if f\"qc_{col}\" not in data.columns and col != 'nee':\n",
        "        print(f\"No qc_{col} in data\")\n",
        "        continue\n",
        "      if col != 'nee':\n",
        "        data.loc[data[f\"qc_{col}\"] > config[col], f\"{col}_qcfilter\"] = 0\n",
        "      else:\n",
        "        data.loc[data[f\"qc_co2_flux\"] > config['co2_flux'], f\"nee_qcfilter\"] = 0\n",
        "\n",
        "      if f\"{col}_qcfilter\" not in filters_db[col]:\n",
        "          filters_db[col].append(f\"{col}_qcfilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"qc_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def std_window_filter(data_in, filters_db_in, config):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    for col, lconfig in config.items():\n",
        "      sigmas = lconfig['sigmas']\n",
        "      window_size = lconfig['window']\n",
        "      min_periods = lconfig['min_periods']#(window_size//2-1)\n",
        "      points_per_day = int(pd.Timedelta('24h')/data_in.index.freq)#lconfig['points_per_day']\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      data[f\"{col}_stdwindowfilter\"] = filter\n",
        "      data['tmp_col'] = data[col]\n",
        "      data.loc[~filter.astype(bool), 'tmp_col'] = np.nan\n",
        "      rolling_mean = bg.calc_rolling(data['tmp_col'], rolling_window=window_size, step=points_per_day, min_periods= min_periods)\n",
        "      residuals = data['tmp_col'] - rolling_mean\n",
        "      rolling_sigma = residuals.rolling(window=window_size * points_per_day, center=True, closed='both',  min_periods=window_size * points_per_day//2).std()\n",
        "      data = data.drop(columns='tmp_col')\n",
        "      # print(rolling_sigma, rolling_mean)\n",
        "      upper_bound = rolling_mean + rolling_sigma * sigmas\n",
        "      lower_bound = rolling_mean - rolling_sigma * sigmas\n",
        "      upper_inds = upper_bound[upper_bound < data[col]].index\n",
        "      lower_inds = lower_bound[lower_bound > data[col]].index\n",
        "      data.loc[upper_inds , f\"{col}_stdwindowfilter\"] = 0\n",
        "      data.loc[lower_inds , f\"{col}_stdwindowfilter\"] = 0\n",
        "      # # print(len(lower_inds), len(upper_inds))\n",
        "      # plt.plot(rolling_mean)\n",
        "      # plt.title(col)\n",
        "      # plt.show()\n",
        "\n",
        "      if f\"{col}_stdwindowfilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_stdwindowfilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"std_window_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def meteorological_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    file_freq = data_in.index.freq\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "\n",
        "    for col in [\"co2_flux\", 'h', 'le', 'ch4_flux']:\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_physFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_physFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_physFilter\"] = filter\n",
        "\n",
        "    if 'co2_signal_strength' in data.columns and 'CO2SS_min' in config.keys():\n",
        "      data.loc[data['co2_signal_strength'] < config['CO2SS_min'], 'co2_flux_physFilter'] = 0\n",
        "    else:\n",
        "      print(\"No co2_signal_strength found\")\n",
        "\n",
        "    if 'ch4_signal_strength' in data.columns and 'CH4SS_min' in config.keys():\n",
        "      data.loc[data['ch4_signal_strength'] < config['CH4SS_min'], 'ch4_flux_physFilter'] = 0\n",
        "    else:\n",
        "      print(\"No ch4_signal_strength found\")\n",
        "\n",
        "    if 'p_rain_limit' in config.keys():\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'co2_flux_physFilter'] = 0\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'h_physFilter'] = 0\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'le_physFilter'] = 0\n",
        "      if 'rain_forward_flag' in config:\n",
        "        rain_forward_flag = config['rain_forward_flag']\n",
        "        for i in range(rain_forward_flag):\n",
        "          ind = data.loc[data['p_rain_1_1_1'] >  config['p_rain_limit']].index.shift(i, freq=file_freq)\n",
        "          data.loc[ind, 'co2_flux_physFilter'] = 0\n",
        "          data.loc[ind, 'h_physFilter'] = 0\n",
        "          data.loc[ind, 'le_physFilter'] = 0\n",
        "\n",
        "    if 'RH_max' in config.keys():\n",
        "      RH_max = config['RH_max']\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'co2_flux_physFilter'] = 0\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'le_physFilter'] = 0\n",
        "    logging.info(f\"meteorological_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_rh_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    file_freq = data_in.index.freq\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"co2_flux\", 'le', 'nee']:\n",
        "\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col}\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_rhFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_rhFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_rhFilter\"] = filter\n",
        "\n",
        "    if 'RH_max' in config.keys() and 'rh_1_1_1' in data.columns:\n",
        "      RH_max = config['RH_max']\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'co2_flux_rhFilter'] = 0\n",
        "      if 'nee' in data.columns:\n",
        "        data.loc[data['rh_1_1_1']>RH_max, 'nee_rhFilter'] = 0\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'le_rhFilter'] = 0\n",
        "    logging.info(f\"meteorological_rh_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def meteorological_night_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    if \"swin_1_1_1\" not in data_in.columns:\n",
        "      logging.info(f\"meteorological_night_filter not applied, no SWIN found  \\n\")\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    file_freq = data_in.index.freq\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    col_of_interest = [\"h\", 'le', 'nee', 'co2_flux']\n",
        "\n",
        "    for col in col_of_interest:\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_nightFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_nightFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_nightFilter\"] = filter\n",
        "\n",
        "    if \"nee\" in data.columns:\n",
        "      data_night_index = data.query(\"swin_1_1_1<10&nee<0\").index\n",
        "      data.loc[data_night_index, f\"nee_nightFilter\"] = 0\n",
        "\n",
        "    if \"co2_flux\" in data.columns:\n",
        "      data_night_index = data.query(\"swin_1_1_1<10&co2_flux<0\").index\n",
        "      data.loc[data_night_index, f\"co2_flux_nightFilter\"] = 0\n",
        "\n",
        "    data_night_index = data.query(f\"(h<{config['night_h_limits'][0]}|h>{config['night_h_limits'][1]})&swin_1_1_1<10\").index\n",
        "    data.loc[data_night_index, f\"h_nightFilter\"] = 0\n",
        "\n",
        "    data_night_index = data.query(f\"(h<{config['night_le_limits'][0]}|h>{config['night_le_limits'][1]})&swin_1_1_1<10\").index\n",
        "    data.loc[data_night_index, f\"le_nightFilter\"] = 0\n",
        "\n",
        "    # if 'nee' in data.columns:\n",
        "    #   data_night_index = data.query(f'nee>{config[\"day_nee_limit\"]}&swin_1_1_1>=10').index\n",
        "    #   data.loc[data_night_index, f\"nee_nightFilter\"] = 0\n",
        "    logging.info(f\"meteorological_night_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def meteorological_day_filter(data_in, filters_db_in, config):#, file_freq='30T'):\n",
        "    if \"swin_1_1_1\" not in data_in.columns:\n",
        "      logging.info(f\"meteorological_night_filter not applied, no SWIN found  \\n\")\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    file_freq = data_in.index.freq\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    col_of_interest = ['nee']\n",
        "\n",
        "    for col in col_of_interest:\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_dayFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_dayFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_dayFilter\"] = filter\n",
        "\n",
        "    if 'nee' in data.columns:\n",
        "      data_day_index = data.query(f'nee>{config[\"day_nee_limit\"]}&swin_1_1_1>=10').index\n",
        "      data.loc[data_day_index, f\"nee_dayFilter\"] = 0\n",
        "    logging.info(f\"meteorological_day_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_co2ss_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    file_freq = data_in.index.freq\n",
        "    if 'CO2SS_min' not in config.keys():\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"co2_flux\", 'nee']:\n",
        "\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_co2ssFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_co2ssFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_co2ssFilter\"] = filter\n",
        "\n",
        "      if 'co2_signal_strength' in data.columns:\n",
        "        data.loc[data['co2_signal_strength'] < config['CO2SS_min'], f'{col}_co2ssFilter'] = 0\n",
        "\n",
        "      else:\n",
        "        print(\"No co2_signal_strength found\")\n",
        "    logging.info(f\"meteorological_co2ss_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_ch4ss_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    file_freq = data_in.index.freq\n",
        "    if 'CH4SS_min' not in config.keys():\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"ch4_flux\"]:\n",
        "\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_ch4ssFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_ch4ssFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_ch4ssFilter\"] = filter\n",
        "\n",
        "    if 'ch4_signal_strength' in data.columns:\n",
        "      data.loc[data['ch4_signal_strength'] < config['CH4SS_min'], 'ch4_flux_ch4ssFilter'] = 0\n",
        "    else:\n",
        "      print(\"No ch4_signal_strength found\")\n",
        "    logging.info(f\"meteorological_coh4ss_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_rain_filter(data_in, filters_db_in, config):#, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    file_freq = data_in.index.freq\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"co2_flux\", 'h', 'le', 'nee', \"ch4_flux\"]:\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col}\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_rainFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_rainFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_rainFilter\"] = filter\n",
        "\n",
        "    if 'p_rain_limit' in config.keys() and 'p_rain_1_1_1' in data.columns:\n",
        "      if 'co2_flux' in data.columns:\n",
        "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'co2_flux_rainFilter'] = 0\n",
        "      if 'h' in data.columns:\n",
        "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'h_rainFilter'] = 0\n",
        "      if 'le' in data.columns:\n",
        "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'le_rainFilter'] = 0\n",
        "      if 'nee' in data.columns:\n",
        "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'nee_rainFilter'] = 0\n",
        "      if 'ch4_flux' in data.columns:\n",
        "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'ch4_flux_rainFilter'] = 0\n",
        "\n",
        "      if 'rain_forward_flag' in config:\n",
        "        rain_forward_flag = config['rain_forward_flag']\n",
        "        for i in range(rain_forward_flag):\n",
        "          ind = data.loc[data['p_rain_1_1_1'] >  config['p_rain_limit']].index.shift(i, freq=file_freq)\n",
        "          ind = ind.intersection(data.index)\n",
        "          if len(ind) == 0:\n",
        "            continue\n",
        "          if 'nee' in data.columns:\n",
        "            data.loc[ind, 'nee_rainFilter'] = 0\n",
        "          if 'ch4_flux' in data.columns:\n",
        "            data.loc[ind, 'ch4_flux_rainFilter'] = 0\n",
        "          if 'co2_flux' in data.columns:\n",
        "            data.loc[ind, 'co2_flux_rainFilter'] = 0\n",
        "          if 'h' in data.columns:\n",
        "            data.loc[ind, 'h_rainFilter'] = 0\n",
        "          if 'le' in data.columns:\n",
        "            data.loc[ind, 'le_rainFilter'] = 0\n",
        "\n",
        "    logging.info(f\"meteorological_rain_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def quantile_filter(data_in, filters_db_in, config):\n",
        "    if len(config) == 0:\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col, limits in config.items():\n",
        "      limit_down, limit_up = limits\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_quantilefilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_quantilefilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_quantilefilter\"] = filter\n",
        "      up_limit = data.loc[data[f'{col}_quantilefilter'] == 1, col].quantile(limit_up)\n",
        "      down_limit = data.loc[data[f'{col}_quantilefilter'] == 1, col].quantile(limit_down)\n",
        "      f_inds = data.query(f\"{col}_quantilefilter==1\").index\n",
        "      print(\"Quantile filter cut values: \", down_limit, up_limit)\n",
        "      # data.loc[f_inds, f'{col}_quantilefilter'] = ((data.loc[f_inds, col] <= up_limit) & (data.loc[f_inds, col] >= down_limit)).astype(int)\n",
        "      data.loc[data[col] > up_limit, f'{col}_quantilefilter'] = 0\n",
        "      data.loc[data[col] < down_limit, f'{col}_quantilefilter'] = 0\n",
        "\n",
        "      # print(col, (data.loc[f_inds, col] < down_limit).sum(), (data.loc[f_inds, col] > up_limit).sum(), len(data.loc[f_inds, col].index), ((data.loc[f_inds, col] < up_limit) & (data.loc[f_inds, col] > down_limit)).astype(int).sum())\n",
        "      # print(filter.sum(), data[f'{col}_quantilefilter'].sum(), filter.sum() - data[f'{col}_quantilefilter'].sum())\n",
        "    logging.info(f\"quantile_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def mad_hampel_filter(data_in, filters_db_in, config):\n",
        "    if len(config) == 0:\n",
        "      return data_in, filters_db_in\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col, lconfig in config.items():\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "\n",
        "      hampel_window = lconfig['hampel_window']\n",
        "      z = lconfig['z']\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_madhampel\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_madhampel\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_madhampel\"] = filter\n",
        "\n",
        "      # hampel_window = 20\n",
        "      print(f\"Processing {col}\")\n",
        "      outdata = bf.apply_hampel_after_mad(data.loc[data[f'{col}_madhampel']==1, :], [col], z=z, window_size=hampel_window)\n",
        "      data.loc[data[f'{col}_madhampel']==1, f'{col}_madhampel'] = outdata[f'{col}_filtered'].astype(int)\n",
        "      data[f\"{col}_madhampel\"] = data[f\"{col}_madhampel\"].astype(int)\n",
        "\n",
        "    logging.info(f\"mad_hampel_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def manual_filter(data_in, filters_db_in, col_name, range, value ):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    filter = get_column_filter(data, filters_db, col_name)\n",
        "    if len(filter) == 0:\n",
        "      filter = [1]*len(data.index)\n",
        "    data[f\"{col_name}_manualFilter\"] = filter\n",
        "    # if range not in data.index:\n",
        "    #   print('WARNING date range is not in index! Nothing is changed!')\n",
        "    #   return data, filters_db\n",
        "    try:\n",
        "      dt_start = pd.to_datetime(start, dayfirst=True)\n",
        "      dt_stop = pd.to_datetime(stop, dayfirst=True)\n",
        "      if dt_start > dt_stop:\n",
        "        raise KeyError(\"Check your dates\")\n",
        "\n",
        "      if dt_start < data.index[0] and (dt_stop <= data.index[-1] and dt_stop > data.index[0]):\n",
        "        dt_start = data.index[0]\n",
        "        print(f\"Actual manual start: {dt_start}\")\n",
        "\n",
        "      if dt_stop > data.index[-1] and (dt_start >= data.index[0] and dt_start < data.index[-1]):\n",
        "        dt_stop = data.index[-1]\n",
        "        print(f\"Actual manual stop: {dt_start}\")\n",
        "\n",
        "      range=pd.date_range(dt_start, dt_stop, freq=data.index.freq)\n",
        "      data.loc[range, f\"{col_name}_manualFilter\"] = value\n",
        "    except KeyError:\n",
        "      print(\"ERROR! Check the date range!\")\n",
        "      return data, filters_db\n",
        "\n",
        "    if f\"{col_name}_manualFilter\" not in filters_db[col_name]:\n",
        "        filters_db[col_name].append(f\"{col_name}_manualFilter\")\n",
        "    else:\n",
        "      print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"manual_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def winter_filter(data_in, filters_db_in, config, date_ranges):\n",
        "  data = data_in.copy()\n",
        "  filters_db = filters_db_in.copy()\n",
        "  if ('winter_nee_limits' not in config.keys()) and ('winter_ch4_flux_limits' not in config.keys()):\n",
        "    return data, filters_db\n",
        "\n",
        "  printed_flag_start = False\n",
        "  printed_flag_stop = False\n",
        "\n",
        "  if 'winter_nee_limits' in config.keys():\n",
        "      for col in ['nee', 'co2_flux']:\n",
        "        if col not in data.columns:\n",
        "          print(f\"No column with name {col}, skipping...\")\n",
        "          continue\n",
        "\n",
        "        filter = get_column_filter(data, filters_db, col)\n",
        "        if len(filter) == 0:\n",
        "          filter = [1]*len(data.index)\n",
        "        data[f\"{col}_winterFilter\"] = filter\n",
        "        try:\n",
        "          for start, stop  in date_ranges:\n",
        "            dt_start = pd.to_datetime(start, dayfirst=True)\n",
        "            dt_stop = pd.to_datetime(stop, dayfirst=True)\n",
        "\n",
        "            if dt_start > dt_stop:\n",
        "              raise KeyError(\"Check your dates, start > stop\")\n",
        "\n",
        "            if dt_stop <= data.index[0] or dt_start >= data.index[-1]:\n",
        "              print(f'Warning, empty range {dt_start} - {dt_stop}!')\n",
        "              continue\n",
        "\n",
        "            if dt_start < data.index[0] and (dt_stop <= data.index[-1] and dt_stop > data.index[0]):\n",
        "              dt_start = data.index[0]\n",
        "              if not printed_flag_start:\n",
        "                print(f\"Actual winter start: {dt_start}\")\n",
        "                printed_flag_start = True\n",
        "\n",
        "            if dt_stop > data.index[-1] and (dt_start >= data.index[0] and dt_start < data.index[-1]):\n",
        "              dt_stop = data.index[-1]\n",
        "              if not printed_flag_stop:\n",
        "                print(f\"Actual winter stop: {dt_start}\")\n",
        "                printed_flag_stop = True\n",
        "\n",
        "            range=pd.date_range(dt_start, dt_stop, freq=data.index.freq)\n",
        "\n",
        "            inds_down = data.loc[range].query(f\"{col}<{config['winter_nee_limits'][0]}\").index\n",
        "            inds_up = data.loc[range].query(f\"{col}>{config['winter_nee_limits'][1]}\").index\n",
        "            data.loc[inds_up, f\"{col}_winterFilter\"] = 0\n",
        "            data.loc[inds_down, f\"{col}_winterFilter\"] = 0\n",
        "        except KeyError:\n",
        "          print(\"ERROR! Check the date range!\")\n",
        "          return data, filters_db\n",
        "\n",
        "        if f\"{col}_winterFilter\" not in filters_db[col]:\n",
        "          filters_db[col].append(f\"{col}_winterFilter\")\n",
        "        else:\n",
        "          print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "  if 'winter_ch4_flux_limits' in config.keys():\n",
        "      for col in ['ch4_flux']:\n",
        "        if col not in data.columns:\n",
        "          print(f\"No column with name {col}, skipping...\")\n",
        "          continue\n",
        "\n",
        "        filter = get_column_filter(data, filters_db, col)\n",
        "        if len(filter) == 0:\n",
        "          filter = [1]*len(data.index)\n",
        "        data[f\"{col}_winterFilter\"] = filter\n",
        "        try:\n",
        "          for start, stop  in date_ranges:\n",
        "\n",
        "            dt_start = pd.to_datetime(start, dayfirst=True)\n",
        "            dt_stop = pd.to_datetime(stop, dayfirst=True)\n",
        "\n",
        "            if dt_start > dt_stop:\n",
        "              raise KeyError(\"Check your dates, start > stop\")\n",
        "\n",
        "            if dt_stop <= data.index[0] or dt_start >= data.index[-1]:\n",
        "              print(f'Warning, empty range {dt_start} - {dt_stop}!')\n",
        "              continue\n",
        "\n",
        "            if dt_start < data.index[0] and (dt_stop <= data.index[-1] and dt_stop > data.index[0]):\n",
        "              dt_start = data.index[0]\n",
        "              if not printed_flag_start:\n",
        "                print(f\"Actual winter start: {dt_start}\")\n",
        "                printed_flag_start = True\n",
        "\n",
        "            if dt_stop > data.index[-1] and (dt_start >= data.index[0] and dt_start < data.index[-1]):\n",
        "              dt_stop = data.index[-1]\n",
        "              if not printed_flag_stop:\n",
        "                print(f\"Actual winter stop: {dt_start}\")\n",
        "                printed_flag_stop = True\n",
        "\n",
        "            range=pd.date_range(dt_start, dt_stop, freq=data.index.freq)\n",
        "\n",
        "            inds_down = data.loc[range].query(f\"{col}<{config['winter_ch4_flux_limits'][0]}\").index\n",
        "            inds_up = data.loc[range].query(f\"{col}>{config['winter_ch4_flux_limits'][1]}\").index\n",
        "            data.loc[inds_up, f\"{col}_winterFilter\"] = 0\n",
        "            data.loc[inds_down, f\"{col}_winterFilter\"] = 0\n",
        "        except KeyError:\n",
        "          print(\"ERROR! Check the date range!\")\n",
        "          return data, filters_db\n",
        "\n",
        "        if f\"{col}_winterFilter\" not in filters_db[col]:\n",
        "          filters_db[col].append(f\"{col}_winterFilter\")\n",
        "        else:\n",
        "          print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "  logging.info(f\"winter_filter applied with the next config: \\n {config}  \\n\")\n",
        "  return data, filters_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfWRVITABzrz"
      },
      "source": [
        "#Формируем конфиг для загрузки и обработки данных\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox0UplWMe7wn"
      },
      "source": [
        "## Конфиг загрузки данных\n",
        "**Необходимо поменять:**\n",
        "\n",
        "`###Запишите название Ваших файлов и путь к ним.`\n",
        "\n",
        "В `config['path']` должен быть либо путь до файла (`= ['1.csv']`) при имени файла 1.csv, либо лист путей в случае загрузки нескольких файлов (`= ['1.csv', '2.csv']`). При импорте через !gdown файла с google-диска достаточно указать в одинарной кавычке *имя файла.расширение*. Не забывайте расширение .csv!\n",
        "\n",
        "**Необходимо проверить:**\n",
        "\n",
        "`  format = \"%Y-%m-%d %H:%M\"  #\"%d/%m/%Y %H:%M\"  #\"%Y-%m-%d %H:%M:%S\"`\n",
        "\n",
        "Проверьте порядок записи даты (год, месяц, день) и разделители даты-времени во входных файлах, открыв их в текстовом редакторе. Возможные варианты:\n",
        "1.  Дата записана как 29.05.2024 и время как 12:00. Тогда они кодируются как\n",
        "\"%d.%m.%Y %H:%M\" – этот формат записан ниже по умолчанию, менять ничего не надо;\n",
        "2.  Дата записана как 29/05/2024 и время как 12:00. Измените в строке ниже формат на \"%d/%m/%Y %H:%M\"\n",
        "3.  Дата записана как 2024-05-29 и время как 1200. Измените в строке ниже формат на \"%Y-%m-%d %H%M\"\n",
        "4.  В остальных случаях действуйте по аналогии. Если в графе time есть секунды, то формат кодируется как \"%Y-%m-%d %H:%M:%S\".\n",
        "\n",
        "**Дополнительные опции (без уровня PRO лучше не менять)**:\n",
        "\n",
        "В `config['time']['converter']` должна находиться функция, которая примет на входа DataFrame и на выходе вернет корректную колонку формата DateTime, которая будет использоваться как временная метка.\n",
        "\n",
        "`config['-9999_to_nan']` будучи установленным `True` заменит -9999 на np.nan для адекватной работы алгоритмов.\n",
        "\n",
        "`config['repair_time']` будучи установленным `True` - проверит колонку с датой-временем на пропуски и монотонность, проведет регенерацию по первой-последней точке с учетом предполагаемой длины шага (вычисляется по паре первых значений ряда)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXIuHMoSHMts"
      },
      "source": [
        "## Конфиг загрузки файла full output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVJ_DRBrlpYd"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "config['debug'] = False  # True загрузит небольшой кусок файла, а не целый\n",
        "config['-9999_to_nan'] = True #заменяем -9999  на np.nan\n",
        "config['repair_time'] = True #генерируем новые временные метки в случае ошибок\n",
        "\n",
        "#####################\n",
        "#на случай сложных колонок времени\n",
        "config['time'] = {}\n",
        "config['time']['column_name'] = 'datetime'\n",
        "def my_datetime_converter(x):\n",
        "    date = x['date'].astype(str) #x['date'].dt.strftime('%d.%m.%Y') if is_datetime(x['date'].dtype) else x['date'].astype(str)\n",
        "    time = x['time'].astype(str) #x['time'].dt.strftime('%H:%M') if is_datetime(x['time'].dtype) else x['time'].astype(str)\n",
        "\n",
        "    x['tmp_datetime'] = date + \" \" + time\n",
        "    #Проверить формат даты-времени в FullOutput\n",
        "    format = \"%d.%m.%Y %H:%M\"#\"%d/%m/%Y %H:%M\"# \"%Y-%m-%d %H:%M\"  #\"%d/%m/%Y %H:%M\"  #\"%Y-%m-%d %H:%M:%S\"\n",
        "    return pd.to_datetime(x['tmp_datetime'], format=format)#dayfirst=True)#, format=format)\n",
        "config['time']['converter'] = my_datetime_converter\n",
        "#####################\n",
        "\n",
        "###Запишите название Ваших файлов и путь к ним. Если файлы будут импортированы с google-диска\n",
        "###через команду !gdown, то достаточно заменить название файла\n",
        "config['path'] = ['eddy_pro result_SSB 2023.csv']#['eddypro_GHG_biomet_CO2SS_Express_full_output_2023-03-29T020107_exp.csv']#['eddypro_noHMP_full_output_2014_1-5.csv', 'eddypro_noHMP_full_output_2014_5-12.csv']#['/content/eddypro_NCT_GHG_22-23dry_full_output.xlsx', '/content/eddypro_NCT_GHG_22wet_full_output.xlsx', '/content/eddypro_NCT_GHG_23wet_full output.xlsx']#'/content/new.csv'\n",
        "# config['path'] = '/content/DT_Full output.xlsx'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Qc-fltJLaF"
      },
      "source": [
        "## Конфиг загрузки файла biomet\n",
        "**Необходимо поменять:**\n",
        "\n",
        "`###Запишите название Ваших файлов и путь к ним.`\n",
        "\n",
        "В config['path'] должен быть либо путь до файла (= ['1.csv']) при имени файла 1.csv, либо лист путей в случае загрузки нескольких файлов (= ['1.csv', '2.csv']). При импорте через !gdown файла с google-диска достаточно указать в одинарной кавычке имя файла.расширение. Не забывайте расширение .csv!\n",
        "\n",
        "**Необходимо проверить:**\n",
        "\n",
        "`  format = \"%Y-%m-%d %H%M\"  #\"%d.%m.%Y %H:%M\"  #yyyy-mm-dd HHMM`\n",
        "\n",
        "Проверьте порядок записи даты (год, месяц, день) и разделители даты-времени во входных файлах, открыв их в текстовом редакторе. В биомет-файле по умолчанию дата записана как 2011-11-12 и время как 1200. Кодируется как \"%Y-%m-%d %H%M\". В других случаях поменяйте код формата дат-времени согласно инструкции для блока Конфиг загрузки файла full output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7E5LGx1DVsA"
      },
      "outputs": [],
      "source": [
        "config_meteo = {}\n",
        "config_meteo ['use_biomet'] = True\n",
        "config_meteo['debug'] = False  # True загрузит небольшой кусок файла, а не целый\n",
        "config_meteo['-9999_to_nan'] = True #заменяем -9999  на np.nan\n",
        "config_meteo['repair_time'] = True #генерируем новые временные метки в случае ошибок\n",
        "\n",
        "#####################\n",
        "#на случай сложных колонок времени\n",
        "config_meteo['time'] = {}\n",
        "config_meteo['time']['column_name'] = 'datetime'\n",
        "def my_datetime_converter(x):\n",
        "    format = \"%Y-%m-%d %H%M\"  #\"%d.%m.%Y %H:%M\"  #yyyy-mm-dd HHMM\n",
        "    return pd.to_datetime(x[\"TIMESTAMP_1\"], format=format)\n",
        "config_meteo['time']['converter'] = my_datetime_converter\n",
        "#####################\n",
        "\n",
        "###Запишите название Ваших файлов и путь к ним. Если файлы будут импортированы с google-диска\n",
        "###через команду !gdown, то достаточно заменить название файла\n",
        "config_meteo['path'] = 'BiometFy4_2023.csv'#'BiometFy4_2016.csv'#'BiometNCT_2011-22.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtxFTNnEfENz"
      },
      "source": [
        "## Выбор колонок для графиков и фильтраций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLnivFTtg9cu"
      },
      "outputs": [],
      "source": [
        "#Соберем обзорную информацию о нужных величинах:\n",
        "cols_to_investigate = []\n",
        "cols_to_investigate.append(\"co2_flux\")\n",
        "cols_to_investigate.append(\"ch4_flux\")\n",
        "cols_to_investigate.append(\"LE\")\n",
        "cols_to_investigate.append(\"H\")\n",
        "cols_to_investigate.append(\"co2_strg\")\n",
        "cols_to_investigate.append(\"Ta_1_1_1\")\n",
        "cols_to_investigate.append(\"RH_1_1_1\")\n",
        "cols_to_investigate.append(\"VPD_1_1_1\")\n",
        "cols_to_investigate.append(\"P_1_1_1\")\n",
        "cols_to_investigate.append(\"SWIN_1_1_1\")\n",
        "cols_to_investigate.append(\"PPFD_1_1_1\")\n",
        "# cols_to_investigate.append(\"co2_signal_strength\")\n",
        "# cols_to_investigate.append(\"ch4_signal_strength\")\n",
        "\n",
        "cols_to_investigate =  [k.lower()for k in cols_to_investigate]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVpYvr9_fKBU"
      },
      "source": [
        "## Настройка параметров анализа данных\n",
        "\n",
        "Все настройки для co2_flux будут применены для nee, в случае его расчета"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH2uRGi4p5Zj"
      },
      "source": [
        "### Фильтрация физическая"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPemVdWVbq2E"
      },
      "outputs": [],
      "source": [
        "window_size = 10\n",
        "calc_nee = True\n",
        "\n",
        "# Индекс станции для названий выходных файлов, рисунков\n",
        "ias_output_prefix = 'tv_fy4'\n",
        "\n",
        "ias_output_version = 'v01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MK90gyzQryZ"
      },
      "source": [
        "Конфиг фильтрации по флагам качества, данные с флагами в интервале (-inf, val] будут помечены как валидные, а данные с значением флага больше порога будут исключены."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukl734CBblay"
      },
      "outputs": [],
      "source": [
        "qc_config = {}\n",
        "qc_config['h'] = 1  #Если система флагов была 1-9, поправить\n",
        "qc_config['le'] = 1  #Если система флагов была 1-9, поправить\n",
        "qc_config['co2_flux'] = 1  #Если система флагов была 1-9, поправить\n",
        "qc_config['ch4_flux'] = 1  #Если система флагов была 1-9, поправить"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPIFpLN_-8Uf"
      },
      "source": [
        "Конфиг фильтрации по метеопараметрам, возможные опции:\n",
        "\n",
        "*   `CO2SS_min` - уберет co2_signal_strength ниже указанного значения\n",
        "*   `p_rain_limit` - уберет H, LE и CO2_FLUX для P_rain_1_1_1 выше указанного лимита\n",
        "*   `rain_forward_flag` - уберет  значения на указанное число записей вперед от каждого отфильтрованного на прошлом шаге значения\n",
        "*   `RH_max` - уберет значения LE и CO2_FLUX для которых RH_1_1_1 больше указанного порога\n",
        "* `day_nee_limit` порог для nee в дневное время (исключение положительных NEE днем)\n",
        "* `night_h_limits` `night_le_limits` допустимые ночные диапазоны LE и H\n",
        "\n",
        "Ночные (Swin<10)  NEE<0 исключаются\n",
        "\n",
        "При отсутствии в конфиге какого-либо из параметров фильтрация не применяется.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxpiAbWk2yYr"
      },
      "outputs": [],
      "source": [
        "meteo_filter_config = {}\n",
        "meteo_filter_config['CO2SS_min'] = 80.\n",
        "\n",
        "# Фильтр может не понадобиться для систем закрытого типа\n",
        "meteo_filter_config['p_rain_limit'] = .1\n",
        "\n",
        "# Фильтр может не понадобиться для систем закрытого типа\n",
        "meteo_filter_config['rain_forward_flag'] = 2\n",
        "\n",
        "# нужно применять только тогда, когда нет CO2SS (образование конденсата) и диагностики анемометра\n",
        "# и данные не были отфильтрованы по этим показателям на этапе расчета в EddyPro\n",
        "# meteo_filter_config['RH_max'] = 98\n",
        "\n",
        "# Какие значения допускаются днем\n",
        "meteo_filter_config['day_nee_limit'] = 5\n",
        "\n",
        "meteo_filter_config['night_h_limits'] = [-50, 20]\n",
        "meteo_filter_config['night_le_limits'] = [-50, 20]\n",
        "\n",
        "# Какие значения допускаются зимой. Для травянистых экосистем правый порог обычно ниже\n",
        "meteo_filter_config['winter_nee_limits'] = [0, 5]\n",
        "\n",
        "meteo_filter_config['winter_ch4_flux_limits'] = [-1, 1]\n",
        "\n",
        "meteo_filter_config['CH4SS_min'] = 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utUX7SA4qA_I"
      },
      "source": [
        "### Фильтрация статистическая"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWISuF-xQCwq"
      },
      "source": [
        "Конфиг фильтрации по абсолютным значениям.\n",
        "Для `rh_1_1_1` значения выше границы не отбрасываются, а заменяются на пограничные. Для `ppfd_1_1_1`, `swin_1_1_1` аналогично обрабатываются минимальные значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQfIYFOd9uzi"
      },
      "outputs": [],
      "source": [
        "min_max_config  = {}\n",
        "min_max_config['co2_flux'] = [-40, 40]\n",
        "min_max_config['co2_strg'] = [-20, 20]\n",
        "min_max_config['h'] = [-100, 800]\n",
        "min_max_config['le'] = [-100, 1000]\n",
        "min_max_config['u_star'] = [0, 10]\n",
        "min_max_config['ta_1_1_1'] = [-50, 50]\n",
        "min_max_config['p_1_1_1'] = [0, 100]\n",
        "min_max_config['vpd_1_1_1'] = [0, 50]\n",
        "min_max_config['rh_1_1_1'] = [0, 100]#max\n",
        "min_max_config['swin_1_1_1'] = [0, 1200]#min\n",
        "min_max_config['ppfd_1_1_1'] = [0, 2400]#min\n",
        "min_max_config['rg_1_1_1'] = [0, 2400]#min\n",
        "min_max_config['ch4_flux'] = [-10, 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmyTKbV1RdjD"
      },
      "source": [
        "Конфиг фильтрации по отклонению от среднего хода.\n",
        "* `window` - размер окна в днях для подсчета среднего хода\n",
        "* `points_per_day` - число измерений в сутках\n",
        "* `sigmas` - допустимый интервал отклонения от среднего хода; вне интервала значения помечаются как отфильтрованные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfRVNYbFYzG3"
      },
      "outputs": [],
      "source": [
        "window_filter_config = {}\n",
        "\n",
        "# Для систем закрытого типа фильтр может быть мягче (например, 3 sigma)\n",
        "window_filter_config['co2_flux'] = {'sigmas': 2, 'window': 10,  'min_periods': 4}\n",
        "\n",
        "window_filter_config['ch4_flux'] = {'sigmas': 2, 'window': 10,  'min_periods': 4}\n",
        "\n",
        "# Если удаляются надежные значения - нужно увеличить 'sigmas'\n",
        "window_filter_config['ta_1_1_1'] = {'sigmas': 4, 'window': 10,  'min_periods': 4}\n",
        "\n",
        "window_filter_config['u_star'] = {'sigmas': 4, 'window': 10,  'min_periods': 4}\n",
        "\n",
        "for col in ['h', 'le', 'rh_1_1_1', 'vpd_1_1_1']:\n",
        "    window_filter_config[col] = {'sigmas': 7, 'window': 10,  'min_periods': 4}\n",
        "\n",
        "for col in ['swin_1_1_1', 'ppfd_1_1_1']:\n",
        "    # Если удаляются надежные значения - нужно увеличить 'sigmas'\n",
        "    window_filter_config[col] = {'sigmas': 7, 'window': 10,  'min_periods': 4}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF_MGD7pSGre"
      },
      "source": [
        "Конфиг фильтрации выше-ниже порога по перцентилям (выпадающие строки отфильтровываются)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asO_t2tZmiD0"
      },
      "outputs": [],
      "source": [
        "quantile_filter_config = {}\n",
        "quantile_filter_config['co2_flux'] = [0.01, 0.99]\n",
        "quantile_filter_config['ch4_flux'] = [0.01, 0.99]\n",
        "quantile_filter_config['co2_strg'] = [0.01, 0.99]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPiTN288UaP3"
      },
      "source": [
        "Конфиг для статистической фильтрации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b3eBVFUq3AU"
      },
      "outputs": [],
      "source": [
        "# madhampel_filter_config = {i:{'z': 5.5, 'hampel_window': 10} for i in cols_to_investigate if 'p_1_1_1' not in i}\n",
        "madhampel_filter_config = {}\n",
        "\n",
        "# Более жесткая фильтрация: 'z'=4. Более мягкая: 'z'=7\n",
        "madhampel_filter_config['co2_flux'] = {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['ch4_flux'] = {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['le'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['h'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['co2_strg'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'ta_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'rh_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'vpd_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'swin_1_1_1'] =  {'z': 7.0, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'ppfd_1_1_1'] =  {'z': 7.0, 'hampel_window': 10}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVF1vDm4EauW"
      },
      "source": [
        "# Загружаем данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV9FvvtnVqdN"
      },
      "source": [
        "**Необходимо поменять:**\n",
        "\n",
        "После !gdown вставьте символы после d/ и до следующего / из публичной ссылки на файл, лежащий на google-диске. К примеру, если ссылка\n",
        "https://drive.google.com/file/d/1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB/view?usp=sharing,\n",
        "то команда будет записана как\n",
        "`!gdown 1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB`\n",
        "\n",
        "`#Загрузка файла full output`\n",
        "Здесь нужно прописать символы из ссылки на файл full output\n",
        "\n",
        "`#Загрузка файла biomet`\n",
        "Здесь нужно прописать символы из ссылки на файл biomet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMu4IqY45HG6"
      },
      "outputs": [],
      "source": [
        "# Загрузка файла full output\n",
        "# https://drive.google.com/file/d/1CGJmXyFu_pmzTLitG5aU8fwY8gW3CI1n/view?usp=sharing\n",
        "!gdown 1CGJmXyFu_pmzTLitG5aU8fwY8gW3CI1n\n",
        "\n",
        "# Загрузка файла biomet\n",
        "# https://drive.google.com/file/d/19XsOw5rRJMVMyG1ntRpibfkUpRAP2H4k/view?usp=sharing\n",
        "!gdown 19XsOw5rRJMVMyG1ntRpibfkUpRAP2H4k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw5TapK10EhR"
      },
      "outputs": [],
      "source": [
        "data, time = bg.load_df(config)\n",
        "data = data[next(iter(data))]  #т.к. изначально у нас словарь\n",
        "data_freq = data.index.freq\n",
        "\n",
        "print(\"Диапазон времени full_output: \", data.index[[0, -1]])\n",
        "logging.info(f\"Data loaded from {config['path']}\")\n",
        "logging.info(\"Time range for full_output: \"+ \" - \".join(data.index[[0,-1]].strftime('%Y-%m-%d %H:%M')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j7ombDYqyC8"
      },
      "source": [
        "Проверяем корректность временных меток. Убираем повторы, дополняем пропуски. На случай загрузки нескольких файлов. При загрузке одного делается автоматически."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65DLIIucNOPe"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  data_meteo, time_meteo  = bg.load_df(config_meteo)\n",
        "  data_meteo = data_meteo[next(iter(data_meteo))]  #т.к. изначально у нас словарь\n",
        "  meteo_freq = data_meteo.index.freq\n",
        "  print(\"Диапазон времени метео: \", data_meteo.index[[0, -1]])\n",
        "  logging.info(f\"MeteoData loaded from {config_meteo['path']}\")\n",
        "  logging.info(\"Time range for meteo: \"+ \" - \".join(data_meteo.index[[0,-1]].strftime('%Y-%m-%d %H:%M')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fVgA8UTMfJ3"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  if data_freq != meteo_freq:\n",
        "    print(\"Resampling meteo data\")\n",
        "    logging.info(f\"Resampling meteo data\")\n",
        "    data_meteo = data_meteo.asfreq(data_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZbqd6adhHEP"
      },
      "outputs": [],
      "source": [
        "print(\"Колонки в FullOutput \\n\", data.columns.to_list())\n",
        "if config_meteo ['use_biomet']:\n",
        "  print(\"Колонки в метео \\n\", data_meteo.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF78Wlq9rD_n"
      },
      "source": [
        "Сливаем в один DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v0rxHehhZEI"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  data = data.join(data_meteo, how='outer', rsuffix='_meteo')\n",
        "  data[time] = data.index\n",
        "  data = bg.repair_time(data, time)\n",
        "  if data[data_meteo.columns[-1]].isna().sum() == len(data.index):\n",
        "    print(\"Bad meteo data range, skipping! Setting config_meteo ['use_biomet']=False\")\n",
        "    config_meteo ['use_biomet'] = False\n",
        "\n",
        "points_per_day = int(pd.Timedelta('24H')/data_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8lLDYOWzH2d"
      },
      "outputs": [],
      "source": [
        "data.columns= data.columns.str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDHkyl_PruXE"
      },
      "source": [
        "# Предобработка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh5MosYXS6aj"
      },
      "source": [
        "Переименовываем колонки для единого формата, рассчитываем VPD <-> RH, SWIN <-> RG и PAR <-> SWIN в случае отсутствия.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAdYXJFdSRbJ",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "have_rh_flag = False\n",
        "have_vpd_flag = False\n",
        "have_par_flag = False\n",
        "have_swin_flag = False\n",
        "have_rg_flag = False\n",
        "have_p_flag = False\n",
        "have_pr_flag = False\n",
        "have_ppfd_flag = False\n",
        "\n",
        "for col_name in data.columns:\n",
        "  if 'u*' in col_name:\n",
        "    print(f\"renaming {col_name} to u_star\")\n",
        "    data = data.rename(columns={col_name: 'u_star'})\n",
        "  if 'ppfd_in_1_1_1' in col_name:\n",
        "    print(f\"renaming {col_name} to ppfd_1_1_1\")\n",
        "    data = data.rename(columns={col_name: 'ppfd_1_1_1'})\n",
        "  if 'sw_in_1_1_1' in col_name:\n",
        "    print(f\"renaming {col_name} to swin_1_1_1\")\n",
        "    data = data.rename(columns={col_name: 'swin_1_1_1'})\n",
        "  if 'co2_signal_strength' in col_name:\n",
        "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
        "  if \"rh_1_1_1\" in col_name:\n",
        "    have_rh_flag =True\n",
        "  if \"vpd_1_1_1\" in col_name:\n",
        "    have_vpd_flag = True\n",
        "  if 'swin' in col_name or 'sw_in' in col_name:\n",
        "    have_swin_flag = True\n",
        "  if 'par' in col_name:\n",
        "    have_par_flag = True\n",
        "  if 'rg_1_1_1' in col_name:\n",
        "    have_rg_flag = True\n",
        "  if 'p_1_1_1' in col_name:\n",
        "    have_p_flag = True\n",
        "  if 'p_rain_1_1_1' in col_name:\n",
        "    have_pr_flag = True\n",
        "  if 'ppfd_1_1_1' in col_name:\n",
        "    have_ppfd_flag = True\n",
        "  if col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()] or 'co2_signal_strength' in col_name:\n",
        "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
        "  if col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()] or 'ch4_signal_strength' in col_name:\n",
        "    print(f\"renaming {col_name} to ch4_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'ch4_signal_strength'})\n",
        "\n",
        "\n",
        "if not (have_ppfd_flag or have_swin_flag):\n",
        "  print(\"NO PPFD and SWin\")\n",
        "else:\n",
        "    if not have_ppfd_flag:\n",
        "      data['ppfd_1_1_1'] = data['swin_1_1_1'] / 0.47\n",
        "    if not have_swin_flag:\n",
        "      data['swin_1_1_1'] = 0.47 * data['ppfd_1_1_1']\n",
        "    have_ppfd_flag = True\n",
        "    have_swin_flag = True\n",
        "\n",
        "\n",
        "if not (have_rg_flag or have_swin_flag):\n",
        "  print(\"NO RG AND SWIN\")\n",
        "else:\n",
        "  print(\"Checking RG-SWIN pair\")\n",
        "  if not have_rg_flag:\n",
        "    data['rg_1_1_1'] = data['swin_1_1_1']\n",
        "  if not have_swin_flag:\n",
        "    data['swin_1_1_1'] = data['rg_1_1_1']\n",
        "    have_swin_flag = True\n",
        "\n",
        "\n",
        "if not (have_p_flag or have_pr_flag):\n",
        "  print(\"NO P and P_RAIN\")\n",
        "else:\n",
        "  print(\"Checking P <-> P_rain pair\")\n",
        "  if not have_p_flag:\n",
        "    data['p_1_1_1'] = data['p_rain_1_1_1']\n",
        "  if not have_pr_flag:\n",
        "    data['p_rain_1_1_1'] = data['p_1_1_1']\n",
        "\n",
        "\n",
        "if not (have_vpd_flag or have_rh_flag):\n",
        "  print(\"NO RH AND VPD\")\n",
        "else:\n",
        "    temp_k = (data['ta_1_1_1'] + 273.15)\n",
        "    logE = 23.5518-(2937.4/temp_k)-4.9283*np.log10(temp_k)\n",
        "    ehpa = np.power(10, logE)\n",
        "    if not have_vpd_flag:\n",
        "      print(\"calculating vpd_1_1_1 from rh\")\n",
        "      data['vpd_1_1_1'] = ehpa - (ehpa*data['rh_1_1_1']/100)\n",
        "    if not have_rh_flag:\n",
        "      print(\"calculating rh_1_1_1 from vpd\")\n",
        "      data['rh_1_1_1'] = ehpa\n",
        "\n",
        "\n",
        "if not (have_par_flag or have_swin_flag):\n",
        "  print(\"NO PAR and SWin\")\n",
        "else:\n",
        "    if not have_par_flag:\n",
        "      data['par'] = data['swin_1_1_1'] / 0.47#SWin=PAR*0.47\n",
        "    if not have_swin_flag:\n",
        "      data['swin_1_1_1'] = 0.47 * data['par']\n",
        "\n",
        "\n",
        "\n",
        "for col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()]:\n",
        "  # print(data.columns.to_list())\n",
        "  if col_name in data.columns.to_list():\n",
        "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
        "\n",
        "for col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()]:\n",
        "  # print(data.columns.to_list())\n",
        "  if col_name in data.columns.to_list():\n",
        "    print(f\"renaming {col_name} to ch4_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'ch4_signal_strength'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soyyX-MCbaXt"
      },
      "source": [
        "## Получение NEE из потока CO2 и накопления"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqWwGSMObro4"
      },
      "source": [
        "Проверка накопления. Рассчитанное по одному уровню в EddyPro не всегда корректно. Корректность проверяется суточным ходом: должен быть рост запаса в течение ночи, резкое уменьшение утром."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yqwO7Uhcjmz"
      },
      "source": [
        "Фильтрация co2_strg с удалением значений выше и ниже пороговых перцентилей. Заполнение пропусков co2_strg  длиной 3 точки и менее – линейной интерполяцией. Полученные отфильтрованные и заполненные значения co2_strg показываются на графике. Принятие решения, суммировать ли co2_flux и co2_strg для получения NEE или работать дальше с co2_flux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjt05XXtbr69",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Пробелы длиной 3 и меньше заполняются линейно\n",
        "if calc_nee and 'co2_strg' in data.columns:\n",
        "  tmp_data = data.copy()\n",
        "  tmp_data['co2_strg_tmp'] = tmp_data['co2_strg'].copy()\n",
        "  tmp_filter_db =  {'co2_strg_tmp': []}\n",
        "  if 'co2_strg' in  quantile_filter_config.keys():\n",
        "    tmp_q_config = {'co2_strg_tmp':quantile_filter_config['co2_strg']}\n",
        "  else:\n",
        "    tmp_q_config = {}\n",
        "  tmp_filter_db = {'co2_strg_tmp':[]}\n",
        "  tmp_data, tmp_filter_db = quantile_filter(tmp_data, tmp_filter_db, tmp_q_config)\n",
        "  tmp_data.loc[~get_column_filter(tmp_data, tmp_filter_db, 'co2_strg_tmp').astype(bool), 'co2_strg_tmp'] = np.nan\n",
        "  # tmp_data['co2_strg_tmp'] = tmp_data['co2_strg_tmp'].interpolate(limit=3)\n",
        "  # tmp_data['co2_strg_tmp'].fillna(bg.calc_rolling(tmp_data['co2_strg_tmp'], rolling_window=10 , step=points_per_day, min_periods=4))\n",
        "  basic_plot(tmp_data, ['co2_strg_tmp'], tmp_filter_db, steps_per_day=points_per_day)\n",
        "  print(tmp_q_config, tmp_filter_db, tmp_data['co2_strg_tmp_quantilefilter'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IQ7W6pslYF-"
      },
      "outputs": [],
      "source": [
        "# Решаем, суммировать ли исходный co2_flux и co2_strg_filtered_filled для получения NEE\n",
        "calc_with_strg = True   #В случае, если дальше работаем с NEE, оставить True.\n",
        "logging.info(f\"calc_with_strg is set to {calc_with_strg}\")\n",
        "# Для того, чтобы работать дальше с co2_flux, игнорируя co2_strg, поставить False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueuvsNxYdtgs"
      },
      "outputs": [],
      "source": [
        "if calc_nee and 'co2_strg' in data.columns:\n",
        "  if calc_with_strg:\n",
        "    data['nee'] = (tmp_data['co2_flux'] + tmp_data['co2_strg_tmp']).copy()\n",
        "  else:\n",
        "    data['nee'] = data['co2_flux'].copy()\n",
        "  del tmp_data\n",
        "  if 'nee' not in cols_to_investigate:\n",
        "    cols_to_investigate.append('nee')\n",
        "  for filter_config in [qc_config, meteo_filter_config, min_max_config, window_filter_config, quantile_filter_config, madhampel_filter_config]:\n",
        "    if 'co2_flux' in filter_config:\n",
        "      filter_config['nee'] = filter_config['co2_flux']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUgwuaFYribB"
      },
      "source": [
        "#Обзор статистики по интересующим колонкам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhcplCMbXtkK"
      },
      "outputs": [],
      "source": [
        "cols_to_investigate = [p for p in cols_to_investigate if p in data.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfusqiotOi3n"
      },
      "outputs": [],
      "source": [
        "data.loc[:, cols_to_investigate].describe()\n",
        "\n",
        "fig, axs = plt.subplots(ncols=min(3, len(cols_to_investigate)), nrows=int(np.ceil(len(cols_to_investigate)/3)), squeeze=False, figsize=(13, 8))\n",
        "\n",
        "for ind, ax in enumerate(axs.reshape(-1)):\n",
        "  if ind >= len(cols_to_investigate):\n",
        "    break\n",
        "  feature = cols_to_investigate[ind]\n",
        "  ax.boxplot(data[feature].to_numpy()[~np.isnan(data[feature].to_numpy())])\n",
        "  ax.set_title(f\"Boxplot for {feature}\")\n",
        "plt.tight_layout()\n",
        "fig.show()\n",
        "\n",
        "data[cols_to_investigate].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oJLXYGbr93S"
      },
      "source": [
        "#Фильтрация данных физическая"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apGNk8eBxgBv"
      },
      "outputs": [],
      "source": [
        "plot_data = data.copy()\n",
        "filters_db = {col: [] for col in plot_data.columns.to_list()}\n",
        "print(plot_data.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL_6XxGGsCBK"
      },
      "source": [
        "## по флагам качества"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGwe7_uU1C8U"
      },
      "outputs": [],
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = qc_filter(plot_data, filters_db, qc_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_gKSTNYyzjS"
      },
      "source": [
        "## по порогу CO2SS и CH4SS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viq7BZue9Ett"
      },
      "outputs": [],
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = meteorological_co2ss_filter(plot_data, filters_db, meteo_filter_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RrPfxfiJGhN"
      },
      "outputs": [],
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = meteorological_ch4ss_filter(plot_data, filters_db, meteo_filter_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwqVDeH6y73_"
      },
      "source": [
        "## по допустимым значениям RH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11isfvNZ9FGu"
      },
      "outputs": [],
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = meteorological_rh_filter(plot_data, filters_db, meteo_filter_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSX2h9QzzFkT"
      },
      "source": [
        "## по наличию дождя"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz696mc09FlB"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  plot_data, filters_db = meteorological_rain_filter(plot_data, filters_db, meteo_filter_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy2y00P1zJtZ"
      },
      "source": [
        "## по ночным и дневным допустимым диапазонам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED_Qh6TS0Qkc"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  plot_data, filters_db = meteorological_night_filter(plot_data, filters_db, meteo_filter_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Vguu8MK635"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  plot_data, filters_db = meteorological_day_filter(plot_data, filters_db, meteo_filter_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzfTJdNe68Eu"
      },
      "source": [
        "## фильтрация зимних периодов, уточните даты!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ87D57S7A91"
      },
      "outputs": [],
      "source": [
        "if ('winter_nee_limits' in meteo_filter_config.keys()) or ('winter_ch4_flux_limits' in meteo_filter_config.keys()):\n",
        "  plot_albedo(plot_data, filters_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_RAYINf67PO",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  date_ranges = [\n",
        "      ['01.01.2023 00:00', '26.03.2023 00:00'],\n",
        "      ['13.11.2023 00:00', '31.12.2023 00:00'],\n",
        "  ]\n",
        "  # date_ranges = []\n",
        "  # date_ranges.append(['25.8.2014 00:00', '26.8.2014 00:00'])\n",
        "  plot_data, filters_db = winter_filter(plot_data, filters_db, meteo_filter_config, date_ranges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iipFLxf6fu5Y"
      },
      "source": [
        "Фильтрация по футпринту\n",
        "будет в следующей версии скрипта\n",
        "\n",
        "`fetch = 1 #или 0. 1 – остаются, 0 – убираются `"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAdRtCPGq6_y"
      },
      "source": [
        "# Фильтрация данных статистическая"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcwZplknsHJv"
      },
      "source": [
        "## по минимальным и максимальным допустимым значениям"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyJaM1zC1DDg"
      },
      "outputs": [],
      "source": [
        "# if config_meteo ['use_biomet']:\n",
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = min_max_filter(plot_data, filters_db, min_max_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j62U1dw8sTEm"
      },
      "source": [
        "## по перцентилям"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNQ4XDK01DME"
      },
      "outputs": [],
      "source": [
        "# if config_meteo ['use_biomet']:\n",
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = quantile_filter(plot_data, filters_db, quantile_filter_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sg76Bwasnb4"
      },
      "source": [
        "## по отклонению от среднего хода"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoDvHhoQ2MMe"
      },
      "outputs": [],
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = std_window_filter(plot_data, filters_db, window_filter_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXl5RdINss9D"
      },
      "source": [
        "## Фильтрация выбросов MAD & Hampel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl9cImVr2MO3"
      },
      "outputs": [],
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, tmp_filter = mad_hampel_filter(plot_data, filters_db, madhampel_filter_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu8MLKyh1AFk"
      },
      "source": [
        "## Ручная фильтрация\n",
        "\n",
        "Если нужно убрать какой-то срок/сроки вручную\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADy534At0_fN"
      },
      "outputs": [],
      "source": [
        "#  фильтр уберет значения от первого до второго включительно\n",
        "man_ranges = [\n",
        "    # ['1.5.2023 00:00', '1.6.2023 00:00'],\n",
        "    # ['25.8.2023 12:00', '25.8.2023 12:00'],\n",
        "]\n",
        "for start, stop in man_ranges:\n",
        "  plot_data, tmp_filter = manual_filter(plot_data, filters_db, col_name=\"nee\", range=[start, stop], value=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APyqyqSEHx3K"
      },
      "source": [
        "## На случай необходимости откатить последний фильтр\n",
        "Не работает с повторно-запущенными несколько раз"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYmSC2gpH4zo"
      },
      "outputs": [],
      "source": [
        "#filters_db = unroll_filters_db.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quGbtDaJ_gID"
      },
      "source": [
        "## Сводная таблица результатов фильтрации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg78qGJ9_miW"
      },
      "outputs": [],
      "source": [
        "all_filters = {}\n",
        "for key, filters in filters_db.items():\n",
        "   if len(filters) > 0:\n",
        "    pl_data = plot_data.copy()\n",
        "    for filter_name in filters:\n",
        "      all_filters[filter_name] = []\n",
        "      all_filters[filter_name].append(len(pl_data.index))\n",
        "      filtered_amount = len(pl_data.query(f\"{filter_name}==0\").index)\n",
        "      all_filters[filter_name].append(filtered_amount)\n",
        "      # old_val =  len(pl_data.index)\n",
        "      pl_data = pl_data.query(f\"{filter_name}==1\")\n",
        "      # print(filter_name, filtered_amount, len(pl_data.index) - old_val)\n",
        "fdf_df = pd.DataFrame(all_filters)\n",
        "print(\"Какая часть данных от общего количества (в %) была отфильтрована:\")\n",
        "print(fdf_df.iloc[1]/len(plot_data)*100)\n",
        "logging.info(\"Какая часть данных от общего количества (в %) была отфильтрована:\")\n",
        "logging.info(fdf_df.iloc[1]/len(plot_data)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA_IPavss0bq"
      },
      "source": [
        "# Отрисовка рядов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijPM6mnJtMv8"
      },
      "source": [
        "## Отрисовка результатов фильтрации данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Xhczc-BRc2"
      },
      "outputs": [],
      "source": [
        "plot_terator = iter(cols_to_investigate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uat4oESzU4__"
      },
      "source": [
        "Для экономии памяти и адекватной работы колаба графики будут выводиться поочередно при повторном запуске ячейки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhNoFAd7DqNN"
      },
      "outputs": [],
      "source": [
        "col2plot = next(plot_terator, False)\n",
        "col2plot = 'swin_1_1_1' #Подставить нужное: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1, co2_signal_strength\n",
        "# Или закомментировать одну строку выше и запускать повторно для переключения к следующему параметру\n",
        "if col2plot:\n",
        "  make_filtered_plot(plot_data, col2plot, filters_db)\n",
        "else:\n",
        "  print(\"No more data, start from the begining!\")\n",
        "  plot_terator = iter(cols_to_investigate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG_wF2qW-Qwb"
      },
      "outputs": [],
      "source": [
        "# #линейное заполнение пропусков, limit - сколько может быть пропущенных подряд\n",
        "# for col in cols_to_investigate:\n",
        "#   plot_data[col] = plot_data[col].interpolate(limit=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtJ8wyx2-XCX"
      },
      "outputs": [],
      "source": [
        "# #Заполнение ходом\n",
        "# for col in cols_to_investigate:\n",
        "#   plot_data[col].fillna(bg.calc_rolling(plot_data[col], rolling_window=10, step=points_per_day, min_periods=7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwuXRVTMtBz2"
      },
      "source": [
        "## Отрисовка среднего хода для отфильтрованных рядов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWDTiucTgRlI"
      },
      "outputs": [],
      "source": [
        "plot_terator = iter(cols_to_investigate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COKiwe7020D4"
      },
      "outputs": [],
      "source": [
        "#Пример вычисления среднего хода\n",
        "\n",
        "col2plot = next(plot_terator, False)\n",
        "#Можно задать вручную\n",
        "# col2plot = 'h'#\"co2_flux\"\n",
        "col2plot = ['nee', 'le'] #Подставить нужное: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1\n",
        "#Или просто запускать повторно для переключения к следующему параметру\n",
        "if col2plot:\n",
        "  basic_plot(plot_data, col2plot, filters_db, steps_per_day=points_per_day)\n",
        "else:\n",
        "  print(\"No more data, start from the begining!\")\n",
        "  plot_terator = iter(cols_to_investigate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKEg6YBstXMp"
      },
      "source": [
        "## Тепловые карты потоков для отфильтрованных данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCUJYURKEL-f",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "for col in ['nee', 'le', 'h']: #Подставить нужное: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1\n",
        "#Или просто запускать повторно для переключения к следующему параметру\n",
        "  plot_nice_year_hist_plotly(plot_data, col ,time, filters_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dokSxicNtdva"
      },
      "source": [
        "# Сохранение данных в формате REddyProc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDqsi61kSeak"
      },
      "source": [
        "Создадим шаблон шапки для файла REddyProc и сохраним требуемые переменные, не забыв учесть фильтрацию. Выходной файл - уровня 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVu2UrCzLqb4"
      },
      "outputs": [],
      "source": [
        "reddyproc_filename = f\"REddyProc_{ias_output_prefix}_{int(plot_data[time].dt.year.median())}.txt\"\n",
        "output_template = {'Year': ['-'],\t'DoY': ['-'],\t'Hour': ['-'],\t'NEE': ['umolm-2s-1'],\t'LE': ['Wm-2'],\t'H': ['Wm-2'],\t'Rg': ['Wm-2'],\t'Tair': ['degC'], \t'Tsoil': ['degC'],\t'rH': ['%'], \t'VPD': ['hPa'], \t'Ustar': ['ms-1']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFulh7FtNWtM"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  eddy_df = plot_data.copy()\n",
        "\n",
        "  for column, filter in filters_db.items():\n",
        "    filter = get_column_filter(eddy_df, filters_db, column)\n",
        "    eddy_df.loc[~filter.astype(bool), column] = np.nan\n",
        "\n",
        "\n",
        "  eddy_df['Year'] = eddy_df[time].dt.year\n",
        "  eddy_df['DoY'] = eddy_df[time].dt.dayofyear\n",
        "  eddy_df['Hour'] = eddy_df[time].dt.hour + eddy_df[time].dt.minute/60\n",
        "\n",
        "  eddy_df['NEE'] = eddy_df['nee'].fillna(-9999)\n",
        "  eddy_df['LE'] = eddy_df['le'].fillna(-9999)\n",
        "  eddy_df['H'] = eddy_df['h'].fillna(-9999)\n",
        "  if 'swin_1_1_1' in eddy_df.columns:\n",
        "    eddy_df['Rg'] = eddy_df['swin_1_1_1'].fillna(-9999)\n",
        "  else:\n",
        "    print(\"WARNING! No swin_1_1_1!\")\n",
        "  eddy_df['Tair'] = eddy_df['ta_1_1_1'].fillna(-9999)\n",
        "  if 'ts_1_1_1' in eddy_df.columns:\n",
        "    eddy_df['Tsoil'] = eddy_df['ts_1_1_1'].fillna(-9999)\n",
        "  eddy_df['rH'] = eddy_df['rh_1_1_1'].fillna(-9999)\n",
        "  eddy_df['VPD'] = eddy_df['vpd_1_1_1'].fillna(-9999)\n",
        "  eddy_df['Ustar'] = eddy_df['u_star'].fillna(-9999)\n",
        "\n",
        "  i=0\n",
        "  while eddy_df.iloc[i]['Hour'] != 0.5:\n",
        "    i += 1\n",
        "  eddy_df = eddy_df.iloc[i:]\n",
        "\n",
        "  if len(eddy_df.index) < 90 * points_per_day:\n",
        "    print(\"WARNING!  < 90 days in reddyproc file!\")\n",
        "\n",
        "  pd.DataFrame({key: item for key, item in output_template.items() if key in eddy_df.columns}).to_csv(os.path.join('output', reddyproc_filename), index=False, sep=' ')\n",
        "  eddy_df.to_csv(os.path.join('output', reddyproc_filename),  index=False, header=False, columns = [i for i in output_template.keys()  if i in eddy_df.columns], mode='a', sep=' ')\n",
        "  del eddy_df\n",
        "  logging.info(f\"REddyProc file saved to {os.path.join('output', reddyproc_filename)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62o5-p8ZzR5T"
      },
      "source": [
        "# Сохранение данных ИАС"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50f7947"
      },
      "source": [
        "Файл уровня 2, записывается из первоначально введенных данных **без учета** фильтраций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaLoIQmtzaYd"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "\tias_df = plot_data.copy()\n",
        "\tfor column, filter in filters_db.items():\n",
        "\t\tfilter = get_column_filter(ias_df, filters_db, column)\n",
        "\t\tias_df.loc[~filter.astype(bool), column] = np.nan\n",
        "\tias_df = ias_df.fillna(-9999)\n",
        "\n",
        "\tcol_match =  {\"co2_flux\" : \"FC_1_1_1\", \"qc_co2_flux\" : \"FC_SSITC_TEST_1_1_1\", \"LE\" : \"LE_1_1_1\",\n",
        "\t\t\"qc_LE\" : \"LE_SSITC_TEST_1_1_1\", \"H\" : \"H_1_1_1\", \"qc_H\" : \"H_SSITC_TEST_1_1_1\", \"Tau\" : \"TAU_1_1_1\",\n",
        "\t\t\"qc_Tau\" : \"TAU_SSITC_TEST_1_1_1\", \"co2_strg\" : \"SC_1_1_1\", \"co2_mole_fraction\" : \"CO2_1_1_1\",\n",
        "\t\t\"h2o_mole_fraction\" : \"H2O_1_1_1\", \"sonic_temperature\" : \"T_SONIC_1_1_1\", \"u*\" : \"USTAR_1_1_1\",\n",
        "\t\t\"Ta_1_1_1\" : \"TA_1_1_1\", \"Pa_1_1_1\" : \"PA_1_1_1\", \"Swin_1_1_1\" : \"SW_IN_1_1_1\", \"Swout_1_1_1\" : \"SW_OUT_1_1_1\",\n",
        "\t\t\"Lwin_1_1_1\" : \"LW_IN_1_1_1\", \"Lwout_1_1_1\" : \"LW_OUT_1_1_1\", \"PPFD_1_1_1\" : \"PPFD_IN_1_1_1\",\n",
        "\t\t\"Rn_1_1_1\" : \"NETRAD_1_1_1\", \"MWS_1_1_1\" : \"WS_1_1_1\", \"Ts_1_1_1\" : \"TS_1_1_1\", \"Ts_2_1_1\" : \"TS_2_1_1\",\n",
        "\t\t\"Ts_3_1_1\" : \"TS_3_1_1\", \"Pswc_1_1_1\" : \"SWC_1_1_1\", \"Pswc_2_1_1\" : \"SWC_2_1_1\", \"Pswc_3_1_1\" : \"SWC_3_1_1\",\n",
        "\t\t\"SHF_1_1_1\" : \"G_1_1_1\", \"SHF_2_1_1\" : \"G_2_1_1\", \"SHF_3_1_1\" : \"G_3_1_1\", \"L\" : \"MO_LENGTH_1_1_1\",\n",
        "\t\t\"(z-d)/L\" : \"ZL_1_1_1\", \"x_peak\" : \"FETCH_MAX_1_1_1\", \"x_70%\" : \"FETCH_70_1_1_1\", \"x_90%\" : \"FETCH_90_1_1_1\", \"ch4_flux\" : \"FCH4_1_1_1\", \"qc_ch4_flux\" : \"FCH4_SSITC_TEST_1_1_1\", \"ch4_mole_fraction\" : \"CH4_1_1_1\", \"ch4_strg\" : \"SCH4_1_1_1\",  \"ch4_signal_strength\" : \"CH4_RSSI_1_1_1\", \"co2_signal_strength\" : \"CO2_STR_1_1_1\"}\n",
        "\tcol_match = {key.lower(): item for key, item in col_match.items()}\n",
        "\n",
        "\tias_df = ias_df.rename(columns=col_match)\n",
        "\ttime_cols = ['TIMESTAMP_START', 'TIMESTAMP_END', 'DTime']\n",
        "\tvar_cols = [col_match[col] for col in col_match.keys() if col_match[col] in ias_df.columns]\n",
        "\n",
        "\tias_df['TIMESTAMP_START'] = ias_df[time].dt.strftime('%Y%m%d%H%M')\n",
        "\tias_df['TIMESTAMP_END'] = (ias_df[time] + pd.Timedelta(0.5, \"H\")).dt.strftime('%Y%m%d%H%M')\n",
        "\tias_df['DTime'] = np.round(ias_df[time].dt.dayofyear + 1./48*2*ias_df[time].dt.hour + 1./48*(ias_df[time].dt.minute//30), decimals=3)\n",
        "\n",
        "\tif 'h_strg' in ias_df.columns:\n",
        "\t\tias_df['SH'] = ias_df['h_strg']\n",
        "\t\tvar_cols.append('SH')\n",
        "\tif 'le_strg' in ias_df.columns:\n",
        "\t\tias_df['SLE'] = ias_df['le_strg']\n",
        "\t\tvar_cols.append('SLE')\n",
        "\n",
        "\tif 'SW_IN_1_1_1' in ias_df.columns:\n",
        "\t\tias_df['SW_IN_1_1_1'] = data['swin_1_1_1']\n",
        "\n",
        "\tias_year = ias_df[time].dt.year.min()\n",
        "\tvar_cols.sort()\n",
        "\tcol_list_ias = time_cols + var_cols\n",
        "\tprint(col_list_ias)\n",
        "\tias_df = ias_df[col_list_ias]\n",
        "\tias_filename = f\"{ias_output_prefix}_{ias_year}_{ias_output_version}.csv\"\n",
        "\tias_df.to_csv(os.path.join('output',ias_filename), index=False)\n",
        "\tlogging.info(f\"IAS file saved to {os.path.join('output',ias_filename)}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm8hiMrb_wRW"
      },
      "source": [
        "# Сохранение данных FAT\n",
        "Файл уровня 3 (отфильтрованные данные), который годится для ввода в инструмент заполнения пропусков Flux Analysis Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ll51nOal6Lz"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9hkPLkB_zd1"
      },
      "outputs": [],
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  fat_output_template = {'DoY': ['--'], r'u*': ['m s-1'],\t'H': ['W m-2'], 'lE': ['-'],\t'NEE': ['umol m-2 s-1'],\t'PPFD': ['umol m-2 s-1'], 'Ta':['oC'], 'VPD':['kPa'], 'PPFD_gapfilling': ['umol m-2 s-1'], 'Ta_gapfilling': ['oC'], 'VPD_gapfilling': ['kPa'], 'period': ['--']}\n",
        "\n",
        "  fat_df = plot_data.copy()\n",
        "\n",
        "\n",
        "  for column, filter in filters_db.items():\n",
        "    filter = get_column_filter(fat_df, filters_db, column)\n",
        "    fat_df.loc[~filter.astype(bool), column] = np.nan\n",
        "\n",
        "\n",
        "\n",
        "  fat_df['DoY'] = np.round(fat_df[time].dt.dayofyear + 0.02*fat_df[time].dt.hour + 0.01*(fat_df[time].dt.minute//30), decimals=2)\n",
        "  fat_df[r'u*'] = fat_df['u_star'].fillna(-99999)\n",
        "  fat_df['H'] = fat_df['h'].fillna(-99999)\n",
        "  fat_df['lE'] = fat_df['le'].fillna(-99999)\n",
        "  fat_df['NEE'] = fat_df['nee'].fillna(-99999)\n",
        "  if 'ppfd_1_1_1' in fat_df.columns:\n",
        "    fat_df['PPFD'] = fat_df['ppfd_1_1_1'].fillna(-99999)\n",
        "    fat_df['PPFD_gapfilling'] = fat_df['ppfd_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['ppfd_1_1_1'], rolling_window=10 , step=points_per_day, min_periods=4)).fillna(-99999)\n",
        "  else:\n",
        "    logging.info(f\"FAT file will have no PPFD\")\n",
        "    fat_output_template.pop('PPFD')\n",
        "\n",
        "  fat_df['Ta'] = fat_df['ta_1_1_1'].fillna(-99999)\n",
        "  fat_df['VPD'] = fat_df['vpd_1_1_1'].fillna(-99999)\n",
        "\n",
        "  fat_df['period'] = fat_df.index.month%12//3 + 1\n",
        "\n",
        "  fat_df['Ta_gapfilling'] = fat_df['ta_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['ta_1_1_1'], rolling_window=10 , step=points_per_day, min_periods=4)).fillna(-99999)\n",
        "  fat_df['VPD_gapfilling'] = fat_df['vpd_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['vpd_1_1_1'], rolling_window=10 , step=points_per_day, min_periods=4)).fillna(-99999)\n",
        "\n",
        "  for year in fat_df.index.year.unique():\n",
        "    fat_filename = f\"FAT_{ias_output_prefix}_{year}.csv\"\n",
        "    pd.DataFrame(fat_output_template).to_csv(os.path.join('output',fat_filename), index=False)\n",
        "    save_data = fat_df.loc[fat_df[time].dt.year==year]\n",
        "    if len(save_data.index) >= 5:\n",
        "      save_data.to_csv(os.path.join('output',fat_filename),  index=False, header=False, columns = [i for i in fat_output_template.keys()], mode='a')#, sep=' ')\n",
        "    else:\n",
        "      try:\n",
        "        os.remove(os.path.join('output',fat_filename))\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "      print(f\"not enough data for {year}\")\n",
        "      logging.info(f\"{year} not saved, not enough data!\")\n",
        "  del fat_df\n",
        "  logging.info(f\"FAT file saved to {fat_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ1bpermu8eq"
      },
      "source": [
        "# Общий файл\n",
        "Ранняя версия выходного файла. Файл содержит записи о применении фильтров в бинарном формате (1 – фильтр применен, 0 – не применен)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk1lGANovC5U"
      },
      "outputs": [],
      "source": [
        "plot_data.fillna(-9999).to_csv(os.path.join('output','output_all.csv'), index=None)\n",
        "logging.info(f\"Basic file saved to {os.path.join('output','output_all.csv')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MSrgUD0-19l"
      },
      "source": [
        "# Модифицированный общий файл\n",
        "Основной выходной файл после фильтрациий. Содерит исходные данные, отфильтрованные данные, флаг применения каждого фильтра, средние суточные ходы в окне 30 дней"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22dPWc2u-6IG",
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "columns_to_save = ['Date', 'Time', 'DoY', 'ta', 'rh', 'vpd', 'swin', 'ppfd', 'p', 'h', 'le', 'co2_flux', 'co2_strg']\n",
        "\n",
        "basic_df = plot_data.copy()\n",
        "\n",
        "basic_df['Date'] = basic_df[time].dt.date\n",
        "basic_df['Time'] = basic_df[time].dt.time\n",
        "basic_df['DoY'] = np.round(basic_df[time].dt.dayofyear + 0.02*basic_df[time].dt.hour + 0.01*(basic_df[time].dt.minute//30), decimals=2)\n",
        "\n",
        "#метео\n",
        "for col in ['ta', 'rh', 'vpd', 'swin', 'ppfd', 'p']:\n",
        "  # print(f\"{col}(_[1-9]){{1,4}})\")\n",
        "  col_pos = [bool(re.fullmatch(f\"{col}(_[1-9]){{1,4}}\", col_in)) for col_in in  basic_df.columns]\n",
        "  if not any(col_pos):\n",
        "    continue\n",
        "  else:\n",
        "    real_col_name = basic_df.columns[np.argmax(col_pos)]\n",
        "    basic_df[col] = basic_df[real_col_name]\n",
        "\n",
        "#Фильтрованные потоки\n",
        "for col in ['nee', 'h', 'le', 'co2_strg']:\n",
        "  if col not in basic_df.columns:\n",
        "    continue\n",
        "  basic_df[f\"{col}_filtered\"] = basic_df[col]\n",
        "  filter = get_column_filter(basic_df, filters_db, col)\n",
        "  basic_df.loc[~filter.astype(bool), f\"{col}_filtered\"] = np.nan\n",
        "  columns_to_save.append(f\"{col}_filtered\")\n",
        "\n",
        "#Фильтрованные метео\n",
        "for col in ['ta', 'rh', 'vpd', 'swin', 'ppfd', 'p']:\n",
        "  if col not in basic_df.columns:\n",
        "    continue\n",
        "  basic_df[f\"{col}_filtered\"] = basic_df[col]\n",
        "  filter = get_column_filter(basic_df, filters_db, col)\n",
        "  basic_df.loc[~filter.astype(bool), f\"{col}_filtered\"] = np.nan\n",
        "  columns_to_save.append(f\"{col}_filtered\")\n",
        "\n",
        "#флаги\n",
        "for col in ['ta', 'rh', 'vpd', 'swin', 'ppfd', 'p', 'h', 'le', 'co2_flux', 'co2_strg', 'nee', 'ch4']: #['nee', 'ch4', 'le', 'h']:\n",
        "  if col not in basic_df.columns:\n",
        "    continue\n",
        "  basic_df[f\"{col}_integral_flag\"] = get_column_filter(basic_df, filters_db, col)\n",
        "  columns_to_save.append(f\"{col}_integral_flag\")\n",
        "\n",
        "# for key, item in filters_db.items():\n",
        "#   columns_to_save = columns_to_save + item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for col in ['h', 'le', 'nee', 'rg', 'ppfd', 'ta', 'rh', 'vpd']:\n",
        "  if f\"{col}_filtered\" not in basic_df.columns:\n",
        "    print(f\"No {col}_filtered in file\")\n",
        "    continue\n",
        "  col_out = col\n",
        "  if col == \"ppfd\":\n",
        "    col_out = \"rg\"\n",
        "  basic_df[f'{col_out}_10d'] = bg.calc_rolling(basic_df[f\"{col}_filtered\"], rolling_window=10 , step=points_per_day, min_periods=7)\n",
        "  basic_df[f'{col_out}_30d'] = bg.calc_rolling(basic_df[f\"{col}_filtered\"], rolling_window=30 , step=points_per_day, min_periods=7)\n",
        "  columns_to_save.append(f'{col_out}_10d')\n",
        "  columns_to_save.append(f'{col_out}_30d')\n",
        "\n",
        "basic_df = basic_df[[col for col in columns_to_save if col in basic_df.columns]]\n",
        "basic_df = basic_df.fillna(-9999)\n",
        "basic_df.to_csv(os.path.join('output','output_summary.csv'), index=None)\n",
        "logging.info(f\"New basic file saved to {os.path.join('output','output_summary.csv')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "775a473e",
        "lines_to_next_cell": 0
      },
      "source": [
        "# Обработка утилитами REddyProc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8aa54de",
        "lines_to_next_cell": 0
      },
      "source": [
        "## Технический блок\n",
        "Необходим и автоматически запускается, если детектируется окружение Google Colab.  \n",
        "Загружает используемые в ячейках скрипты в директорию `src` и подготавливает R окружение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06859169",
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# def section_*(): + ipynb to py convert?\n",
        "try:\n",
        "    import google.colab\n",
        "except ImportError:\n",
        "    class StopExecution(Exception):\n",
        "        def _render_traceback_(self):\n",
        "            return ['Colab env not detected. Current cell is only for Colab.']\n",
        "    raise StopExecution()\n",
        "\n",
        "cur_dir = %pwd\n",
        "assert cur_dir == '/content'\n",
        "\n",
        "!mkdir -p src/repo1/\n",
        "%cd src/repo1/\n",
        "\n",
        "!git init\n",
        "!git sparse-checkout init\n",
        "!git sparse-checkout set \"src\"\n",
        "!git remote add origin https://github.com/PlaZMaD/climate.git\n",
        "!git pull --depth 1 origin v0.9.1\n",
        "\n",
        "%cd /content\n",
        "!cp -r src/repo1/src .\n",
        "\n",
        "# 1.3.2 vs 1.3.3 have different outputs. To test,\n",
        "# install.packages('https://cran.r-project.org/bin/windows/contrib/4.1/REddyProc_1.3.2.zip', repos = NULL, type = \"binary\")\n",
        "setup_colab_r_code = \"\"\"\n",
        "install_if_missing <- function(package, repos) {\n",
        "    # sink redirect is required path for ipynb output\n",
        "    sink(stdout(), type = \"message\")\n",
        "    if (!require(package, character.only = TRUE)) {\n",
        "        install.packages(package, dependencies = TRUE, repos=repos)\n",
        "        library(package, character.only = TRUE)\n",
        "    }\n",
        "    sink()\n",
        "}\n",
        "install_if_missing(\"REddyProc\", repos='http://cran.rstudio.com/')\n",
        "\"\"\"\n",
        "from rpy2 import robjects\n",
        "robjects.r(setup_colab_r_code)\n",
        "\n",
        "from src.ipynb_helpers import enable_word_wrap\n",
        "enable_word_wrap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034b04a5",
        "lines_to_next_cell": 0
      },
      "source": [
        "## Фильтрация и заполнение пропусков\n",
        "\n",
        "Далее `ig.eddyproc_options` - настройки, которые соответствуют опциям [онлайн утилиты](https://www.bgc-jena.mpg.de/REddyProc/ui/REddyProc.php).\n",
        "\n",
        "**Необходимо проверить:**  \n",
        "\n",
        "Включение детектирования условий недостаточной турбулентности  \n",
        "`is_to_apply_u_star_filtering=False`  \n",
        "Недостаточная турбулентность определяет границу применимости метода Eddy Covariance. При скорости потоков (uStar) ниже определенного порога поток CO2 может быть занижен. Данные, соответствующие этим условиям, заменяются на пропуски.\n",
        "\n",
        "В EddyProc доступен только метод подвижной точки `RTw`  \n",
        "`u_star_method=\"RTw\"`  \n",
        "Итеративная оценка точности вычисленного порога u* (bootstrap)  \n",
        "`is_bootstrap_u_star=False`\n",
        "\n",
        "Выбор метода разметки сезонов, для каждого из которых отдельно определяется порог насыщения. `Continuous` - начало сезонов в Марте, Июне, Сентябре, и Декабре, Декабрь при этом включается в *следующий* год. `WithinYear` - каждый год отдельно. `User` - по столбцу *season*.  \n",
        "`u_star_seasoning=\"Continuous\"`\n",
        "\n",
        "По сравнению с исходным инструментом REddyProc в этой тетради добавлена дополнительная возможность подстановки пользовательского значения порога в случае, когда порог невозможно рассчитать (например, если недостаточно данных). Для травянистых экосистем можно использовать `0.01`, для лесных - `0.1`, для отключения подстановки - `None`.  \n",
        "`ustar_fallback_value=0.01`  \n",
        "\n",
        "В случае других ошибок будет повторная попытка запуска с отключением фильтрации и предупреждением в логе ячейки.  \n",
        "<br>\n",
        "\n",
        "Включение и выбор методов разделения потока C02 на валовую первичную продукцию (GPP) и дыхание экосистемы (Reco).\n",
        "Метод `Reichstein05` для ночного времени и/или `Lasslop10` для дневного  \n",
        "`is_to_apply_gap_filling=True`  \n",
        "`is_to_apply_partitioning=True`  \n",
        "`partitioning_methods=[\"Reichstein05\", \"Lasslop10\"]`  \n",
        "\n",
        "Широта, долгота, временная зона  \n",
        "`latitude = 56.5`  \n",
        "`longitude = 32.6`  \n",
        "`timezone = +3`  \n",
        "\n",
        "По температуре воздуха или почвы будет происходить заполнение  \n",
        "`temperature_data_variable=\"Tair\"`\n",
        "\n",
        "**Дополнительные опции (согласованы с предыдущими секциями):**  \n",
        "\n",
        "Название местности, которое будет продублировано в названиях выходных файлов:    \n",
        "`site_id=ias_output_prefix`  \n",
        "Файл, из которого загружаются временные ряды:  \n",
        "`input_file=\"REddyProc.txt\"`  \n",
        "Директория, в которую утилита пишет контрольные изображения, базовую статистику по пропускам, заполненные ряды:  \n",
        "`output_dir=\"output/reddyproc\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "278caec5",
        "jupyter": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "from src.ipynb_globals import *\n",
        "from types import SimpleNamespace\n",
        "from src.reddyproc.reddyproc_bridge import reddyproc_and_postprocess\n",
        "import src.ipynb_globals as ig\n",
        "from src.helpers.io_helpers import ensure_empty_dir\n",
        "\n",
        "ig.eddyproc = SimpleNamespace()\n",
        "ig.eddyproc.options = SimpleNamespace(\n",
        "    site_id=ias_output_prefix,\n",
        "\n",
        "    is_to_apply_u_star_filtering=True,\n",
        "    ustar_fallback_value=0.01,\n",
        "\n",
        "    # uStarSeasoning = \"WithinYear\", \"Continuous\" , \"User\"\n",
        "    u_star_seasoning=\"Continuous\",\n",
        "    u_star_method=\"RTw\",\n",
        "    is_bootstrap_u_star=False,\n",
        "\n",
        "    is_to_apply_gap_filling=True,\n",
        "    is_to_apply_partitioning=True,\n",
        "\n",
        "    # \"Reichstein05\", \"Lasslop10\", ...\n",
        "    partitioning_methods=[\"Reichstein05\", \"Lasslop10\"],\n",
        "    latitude=56.5,\n",
        "    longitude=32.6,\n",
        "    timezone=+3.0,\n",
        "\n",
        "    # \"Tsoil\"\n",
        "    temperature_data_variable=\"Tair\",\n",
        "\n",
        "    input_file=f\"output/{reddyproc_filename}\",\n",
        "    output_dir=\"output/reddyproc\",\n",
        "    log_fname_end='_log.txt'\n",
        ")\n",
        "\n",
        "ensure_empty_dir(ig.eddyproc.options.output_dir)\n",
        "ig.eddyproc.out_info = reddyproc_and_postprocess(ig.eddyproc.options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bed439c"
      },
      "source": [
        "## Контрольные графики\n",
        "Отображение отдельных графиков из онлайн утилиты в удобной для проверки форме.  \n",
        "Заполненные данные, графики и проверочную статистику можно скачать одним архивом по кнопке **Download eddyproc outputs**.\n",
        "\n",
        "**Дополнительные опции:**  \n",
        "  \n",
        "Порядок и набор графиков формируется автоматически в переменной `output_sequence`, которую также можно поменять или переобъявить с помощью тэгов.  \n",
        "Тэги именно для этого варианта тетради будут видны после запуска ячейки по вызову `display_tag_info`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e66a94ab"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import src.ipynb_globals as ig\n",
        "from src.helpers.io_helpers import create_archive\n",
        "from src.reddyproc.postprocess_graphs import EProcOutputHandler, EProcImgTagHandler, EProcOutputGen\n",
        "from src.colab_routines import colab_add_download_button, colab_no_scroll\n",
        "\n",
        "tag_handler = EProcImgTagHandler(main_path='output/reddyproc', eproc_options=ig.eddyproc, img_ext='.png')\n",
        "eog = EProcOutputGen(tag_handler)\n",
        "\n",
        "output_sequence: Tuple[Union[List[str], str], ...] = (\n",
        "    \"## Тепловые карты\",\n",
        "    eog.hmap_compare_row('NEE_*'),\n",
        "    eog.hmap_compare_row('LE_f'),\n",
        "    eog.hmap_compare_row('H_f'),\n",
        "    \"## Суточный ход\",\n",
        "    eog.diurnal_cycle_row('NEE_*'),\n",
        "    eog.diurnal_cycle_row('LE_f'),\n",
        "    eog.diurnal_cycle_row('H_f'),\n",
        "    \"## 30-минутные потоки\",\n",
        "    eog.flux_compare_row('NEE_*'),\n",
        "    eog.flux_compare_row('LE_f'),\n",
        "    eog.flux_compare_row('H_f')\n",
        ")\n",
        "\n",
        "eio = EProcOutputHandler(output_sequence=output_sequence, tag_handler=tag_handler, out_info=ig.eddyproc.out_info)\n",
        "eio.prepare_images_safe()\n",
        "ig.arc_exclude_files = eio.img_proc.raw_img_duplicates\n",
        "\n",
        "eproc_arc_path = Path('output/reddyproc') / Path(ig.eddyproc.out_info.fnames_prefix + '.zip')\n",
        "create_archive(arc_path=eproc_arc_path, folders='output/reddyproc', top_folder='output/reddyproc',\n",
        "               include_fmasks=['*.png', '*.csv', '*.txt'], exclude_files=eio.img_proc.raw_img_duplicates)\n",
        "\n",
        "colab_add_download_button(eproc_arc_path, 'Download eddyproc outputs')\n",
        "\n",
        "colab_no_scroll()\n",
        "eio.display_images_safe()\n",
        "\n",
        "tag_handler.display_tag_info(eio.extended_tags())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEead6faY22W"
      },
      "source": [
        "## Выгрузка результатов\n",
        "\n",
        "Результаты работы всех сегментов тетради можно скачать одним архивом по кнопке **Download outputs**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4rv4ucOX8Yz"
      },
      "outputs": [],
      "source": [
        "from src.helpers.io_helpers import create_archive\n",
        "from pathlib import Path\n",
        "import src.ipynb_globals as ig\n",
        "\n",
        "arc_path=Path('output') / 'FluxFilter_output.zip'\n",
        "create_archive(arc_path=arc_path, folders=['output', 'output/reddyproc'], top_folder='output',\n",
        "               include_fmasks=['*.png', '*.csv', '*.txt', '*.log'], exclude_files=ig.arc_exclude_files)\n",
        "colab_add_download_button(arc_path, 'Download outputs')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}