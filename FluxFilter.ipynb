{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlaZMaD/climate/blob/main/FluxFilter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FluxFilter**"
      ],
      "metadata": {
        "id": "pqQVYpfkwA8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Введение**\n",
        "Этот скрипт разработан для визуализации и фильтрации 30-минутных данных с вышек, работающих по методу турбулентных пульсаций (eddy covariance). Скрипт работает с данными об экосистемных потоках тепла и парниковых газов, а также 30-минутными метеорологическими данными. Иначе, это инструмент перевода данных из уровня 1 в уровни 2 и 3.\n",
        "* Под уровнем 1 подразумеваются потоки, как они рассчитаны EddyPro (LI-COR Inc., США) и \"сырая\" 30-минутная метеорология в том виде, как она записывается на регистраторы на вышках.\n",
        "* Под уровнем 2 подразумеваются незаполненные 30-минутные данные, ответственный за станцию исключает периоды заведомо плохой работы приборов (т.е. данные за эти периоды заполнены кодом пропуска -9999). Такие данные собираются для Информационно-Аналитической системы (ИАС, разработчик ИКИ РАН).\n",
        "* Под уровнем 3 подразумеваются данные уровня 2, также незаполненные, но прошедшие тщательную физическую, статистическую и (в случае надобности) визуальную фильтрацию.\n",
        "\n",
        "## **Входные файлы**\n",
        "Форматы входных файлов: выходной файл EddyPro - full output (см. [мануал EddyPro](https://licor.app.boxenterprise.net/s/1ium2zmwm6hl36yz9bu4)) для потоков, а также biomet-файл EddyPro (см. [его же](https://licor.app.boxenterprise.net/s/1ium2zmwm6hl36yz9bu4)) для метеорологии. Турбулентные потоки и u* берутся из файла full output, а все метеорологические переменные (температура, относительная влажность и т.д.) берутся из файла biomet. Основные требования ко входным файлам:\n",
        "*   Файлы должны быть в формате .csv (текстовый файл, разделенный запятыми).\n",
        "*   Заголовки столбцов должны быть строго по руководству EddyPro, в скрипте переменные идентифицируются по названиям колонок (co2_flux для потока CO2 в full output, Ta_1_1_1 для температуры воздуха в biomet и т.д.).\n",
        "*   Код пропуска во входных файлах должен быть -9999\n",
        "*   Единицы для переменных файла biomet должны быть как основные единицы для файла biomet по руководству EddyPro. Исключение: температура воздуха/почвы должна быть в градусах Цельсия\n",
        "*   Файл-пример full output можно скачать [здесь](https://drive.google.com/file/d/1TyuHYZ0uh5teRiRFAga0XIqfU4vYW4-N/view?usp=sharing)\n",
        "*   Файл-пример biomet можно скачать [здесь](https://drive.google.com/file/d/1FjiBcSspDBlYlcg9Vzy71Sm49gOFZGBF/view?usp=sharing)\n",
        "*   В файле full output должны быть 3 строки заголовка и названия переменных должны быть записаны во 2-й строке\n",
        "*   В файле biomet должны быть 2 строки заголовка и названия переменных должны быть записаны во 2-й строке. По умолчанию без проблем читаются файлы, у которых дата и время записаны в колонке TIMESTAMP_1 в формате yyyy-mm-dd HHMM\n",
        "\n",
        "## **Выходные файлы**\n",
        "Форматы выходных файлов:\n",
        "1.   Файл базы данных ИАС уровня 2;\n",
        "2.   Входной файл для инструмента фильтрации по u*, заполнения пропусков и разделения потоков [REddyProcWeb](https://www.bgc-jena.mpg.de/5624918/Input-Format) (Институт Макса Планка, Германия)\n",
        "3. Входной файл для инструмента заполнения пропусков [Flux Analysis Tool](https://atmenv.envi.osakafu-u.ac.jp/staff/ueyama/softwares/) (M. Ueyama, Япония)\n",
        "4. Basic - какие фильтры были применены, что получилось\n",
        "5. Лог - записи в ходе работы скрипта, введенные для фильтрации параметры в данном пробеге\n",
        "\n",
        "## **Загрузка входных файлов**\n",
        "*   загрузить на google-диск файлы full output и biomet,\n",
        "*   открыть к ним доступ\n",
        "*   в конфиге загрузки данных заменить названия входных файлов на импортируемые\n",
        "*   в конфиге загрузки данных проверить формат входных даты и времени\n",
        "*   скопировать часть публичной ссылки в раздел Загружаем данные в команду !gdown\n",
        "\n",
        "## **Перед фильтрацией**\n",
        "*   Можно загружать несколько файлов full output и biomet, они будут автоматически расположены по возрастанию дат-времени и слиты в одну таблицу\n",
        "*   Осуществляется проверка меток времени для каждого входного файла (регуляризация)\n",
        "*   Рассчитываются VPD <-> RH, SWIN <-> RG <-> PAR в случае отсутствия\n",
        "*   Можно работать с потоком CO2 либо проверить данные о накоплении, прибавить их к потоку CO2 и работать с NEE\n",
        "\n",
        "## **Как происходит фильтрация**\n",
        "Скрипт позволяет выявить и удалить некачественные и выбивающиеся значения с помощью  1) физической, 2) статистической  фильтрации, проходящей под визуальным контролем - с просмотром графиков до фильтраций и после.\n",
        "1. Физическая фильтрация включает удаление плохих значений потоков с флагом EddyPro больше порогового, при уровне сигнала газоанализатора (CO2SS) ниже порогового значения, в дождь и после дождей, при высокой влажности, по ночным и дневным допустимым диапазонам.\n",
        "2. Статистическая фильтрация включает удаление выбивающихся значений (outliers/spikes/выбросы/пики/спайки) с помощью фильтров по минимальным и максимальным допустимым значениям, по перцентилям, по отклонениям от среднего суточного хода в окне несколько дней, отклонениям от средних в скользящем окне на несколько точек MAD (Sachs, 2006) и HAMPEL (Pearson et al., 2016).\n",
        "3. Опцию визуальной фильтрации данных (ручное удаление точек при просмотре графика) Google Colab не позволяет реализовать, но в версии для запуска в среде программирования визуальная фильтрация планируется.\n",
        "4. Можно исключить данные по списку интервалов (исключить с ... - по ...), например, калибровки по журналу технических работ.\n",
        "\n",
        "(с)Евгений Курбатов, Вадим Мамкин, Ольга Куричева\n"
      ],
      "metadata": {
        "id": "oE87fcFbwlIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Технический блок\n",
        "Импорт библиотек и определение функций"
      ],
      "metadata": {
        "id": "sj6Z0gnhVM-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "key = userdata.get('registry_key')"
      ],
      "metadata": {
        "id": "lZliIHxRJiqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install plotly-resampler dateparser\n",
        "%pip install --index-url https://public:{key}@gitlab.com/api/v4/projects/55331319/packages/pypi/simple --no-deps bglabutils==0.0.12"
      ],
      "metadata": {
        "id": "E-a6ANTGBsqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywv5kp0rzanK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pylab as plt\n",
        "import os\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "import dateutil\n",
        "from copy import deepcopy as copy\n",
        "\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "import plotly_resampler\n",
        "import dateparser\n",
        "\n",
        "import bglabutils.basic as bg\n",
        "import bglabutils.filters as bf\n",
        "\n",
        "import logging\n",
        "# import bglabutils.boosting as bb\n",
        "# import textwrap\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, filename=\"/content/log.log\", filemode=\"w\", force=True)\n",
        "logging.info(\"START\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции для отрисовки"
      ],
      "metadata": {
        "id": "c_5uwjkzfk45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colapse_filters(data, filters_db_in):\n",
        "  out_filter = {}\n",
        "  for feature, filters in filters_db_in.items():\n",
        "    if len(filters)>0:\n",
        "      out_filter[feature] = data[filters[0]].astype(int) if len(filters)==1 else np.logical_and.reduce((data[filters].astype(int)), axis=1).astype(int)\n",
        "  return out_filter\n",
        "\n",
        "def get_column_filter(data, filters_db_in, column_name):\n",
        "  if len(filters_db_in[column_name]) > 0:\n",
        "    return colapse_filters(data, filters_db_in)[column_name]\n",
        "  else:\n",
        "    return np.array([1]*len(data.index))\n",
        "\n",
        "def basic_plot( data, col2plot, filters_db=None,  min_days=8, window_days = 10, steps_per_day=2*24, use_resample=False):\n",
        "\n",
        "  multiplot = isinstance(col2plot, list)\n",
        "\n",
        "  window_days = window_days   # дней в окне\n",
        "  min_days = window_days//2 - 1\n",
        "  pl_data = data.copy()\n",
        "\n",
        "  layout = go.Layout(\n",
        "      paper_bgcolor='rgba(0,0,0,0)',\n",
        "      plot_bgcolor='rgba(0,0,0,0)'\n",
        "  )\n",
        "  color_data = 'darkorange'\n",
        "  color_line = 'darkslateblue'\n",
        "\n",
        "  add_color_data = copy(px.colors.qualitative.Pastel1)\n",
        "  add_color_line = copy(px.colors.qualitative.Prism)\n",
        "\n",
        "  add_color_data.insert(0, color_data)\n",
        "  add_color_line.insert(0, color_line)\n",
        "\n",
        "  fig = go.Figure(layout=layout)\n",
        "  if multiplot:\n",
        "    fig = make_subplots(rows=len(col2plot), cols=1, shared_xaxes=True, figure=fig, subplot_titles=[i.upper() for i in col2plot])\n",
        "  else:\n",
        "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, figure=fig, row_heights=[.8, .2], subplot_titles=[col2plot.upper(), 'Residuals'])\n",
        "\n",
        "  fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey', minor_ticks='inside', minor_tickcolor='Grey')\n",
        "  fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey')\n",
        "  # fig.update_layout(\n",
        "  #     title = col2plot,\n",
        "  #     xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
        "  # )\n",
        "  if not multiplot:\n",
        "    cols = [col2plot]\n",
        "  else:\n",
        "    cols = col2plot\n",
        "\n",
        "  fig.update_layout(\n",
        "    # title = \" \".join(cols),\n",
        "    xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
        "  )\n",
        "  for row, col2plot in enumerate(cols):\n",
        "    if filters_db is not None:\n",
        "      filters =  get_column_filter(pl_data, filters_db, col2plot)\n",
        "      pl_data.loc[~filters.astype(bool), col2plot] = np.nan\n",
        "\n",
        "    if steps_per_day % 2 == 0:\n",
        "      closed='left'\n",
        "    else:\n",
        "      closed='both'\n",
        "    rolling_mean = bg.calc_rolling(pl_data[col2plot], step=steps_per_day, rolling_window=window_days, min_periods=min_days)\n",
        "\n",
        "    fig.add_trace(go.Scattergl(x=pl_data.index, y=pl_data[col2plot], mode='markers', name=col2plot, marker_color=add_color_data[row]), row=row+1, col=1)\n",
        "    fig.add_trace(go.Scattergl(x=rolling_mean.index, y=rolling_mean, mode='lines', name=f'{col2plot} mean {window_days} days', opacity=.7, line_color=add_color_line[row]), row=row+1, col=1)\n",
        "    if not multiplot:\n",
        "      fig.add_trace(go.Scattergl(x=rolling_mean.index, y=rolling_mean - pl_data[col2plot], mode='lines', name=f'residuals'), row=2, col=1)\n",
        "\n",
        "  if use_resample:\n",
        "    fig = plotly_resampler.FigureResampler(fig, default_n_shown_samples=5000)\n",
        "\n",
        "  fig_config = {'toImageButtonOptions': {'filename': '_'.join(cols),}}\n",
        "  fig.show(config=fig_config)\n",
        "\n",
        "\n",
        "\n",
        "def plot_nice_year_hist_plotly(df, to_plot, time_col, filters_db):\n",
        "    pl_data = df.copy()#[to_plot]\n",
        "    if filters_db is not None:\n",
        "      print()\n",
        "      filters =  get_column_filter(df, filters_db, to_plot)\n",
        "      pl_data['filter'] = filters\n",
        "      pl_data.loc[~filters.astype(bool), to_plot] = np.nan\n",
        "    # print(pl_data.loc[pd.to_datetime('26 June 2016 1:30'), ['nee', 'nee_nightFilter', 'swin_1_1_1', 'filter']].to_string())\n",
        "    fig = go.Figure()\n",
        "    fig.update_layout(title = f'{to_plot}')\n",
        "    fig.add_trace(go.Heatmap(x=pl_data[time_col].dt.date, y=pl_data[time_col].dt.hour + 0.5*(pl_data[time_col].dt.minute//30), z=pl_data[to_plot]))\n",
        "    fig_config = {'toImageButtonOptions': {'filename': f'{to_plot}',}}\n",
        "\n",
        "    fig.show(config=fig_config)\n",
        "\n",
        "\n",
        "def make_filtered_plot(data_pl, col, filters_db):\n",
        "  data = data_pl.copy()\n",
        "  layout = go.Layout(\n",
        "      paper_bgcolor='rgba(0,0,0,0)',\n",
        "      plot_bgcolor='rgba(0,0,0,0)'\n",
        "  )\n",
        "  add_color_dot = copy(px.colors.qualitative.Dark24)\n",
        "  fig = go.Figure(layout=layout)\n",
        "  fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey', minor_ticks='inside', minor_tickcolor='Grey')\n",
        "  fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='Grey')\n",
        "\n",
        "  data['full_filter'] =  get_column_filter (data, filters_db, col)\n",
        "  data['full_filter'] = data['full_filter'].astype(int)\n",
        "  pl_data = data.query(f\"full_filter==0\")\n",
        "  color_ind = 0\n",
        "  fig.add_trace(go.Scattergl(x=data.query(\"full_filter==1\").index, y=data.query(\"full_filter==1\")[col], mode='markers', name=\"Good data\", marker_color=add_color_dot[color_ind] ))\n",
        "  color_ind += 1\n",
        "\n",
        "  if len(filters_db[col]) > 0:\n",
        "    for filter_name in filters_db[col]:\n",
        "      fig.add_trace(go.Scattergl(x=pl_data.query(f\"{filter_name}==0\").index, y=pl_data.query(f\"{filter_name}==0\")[col], mode='markers',   name=filter_name, marker_color=add_color_dot[color_ind]))\n",
        "      color_ind += 1\n",
        "      pl_data = pl_data.query(f\"{filter_name}==1\")\n",
        "\n",
        "  fig.update_layout(\n",
        "      title = f'{col2plot}',\n",
        "      xaxis_tickformat = '%H:%M %d %B <br>%Y'\n",
        "  )\n",
        "  fig_config = {'toImageButtonOptions': {'filename': f'{col}',}}\n",
        "  fig.show(config=fig_config)\n",
        "\n",
        "\n",
        "def plot_albedo (plot_data, filters_db):\n",
        "  pl_data = plot_data.copy()\n",
        "\n",
        "  layout = go.Layout(\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    plot_bgcolor='rgba(0,0,0,0)'\n",
        "    )\n",
        "\n",
        "\n",
        "  if ('swin_1_1_1' not in pl_data.columns) or ('swout_1_1_1' not in pl_data.columns):\n",
        "    print(\"No swin_1_1_1/sout_1_1_1\")\n",
        "    return 0\n",
        "  pl_data['albedo'] = pl_data['swout_1_1_1'].div(pl_data['swin_1_1_1'])\n",
        "  pl_data.loc[pl_data['swin_1_1_1']<=0, 'albedo'] = np.nan\n",
        "  pl_data.loc[pl_data['swout_1_1_1']<=0, 'albedo'] = np.nan\n",
        "\n",
        "  pl_ind  = pl_data[pl_data['albedo']<pl_data['albedo'].quantile(0.95)].index\n",
        "  fig = go.Figure(layout=layout)\n",
        "  fig.add_trace(go.Scattergl(x=pl_data.loc[pl_ind].index, y=pl_data.loc[pl_ind, 'albedo'], name=\"Albedo\"))\n",
        "  fig.update_layout(title = 'Albedo')\n",
        "  fig_config = {'toImageButtonOptions': {'filename': 'albedo',}}\n",
        "  fig.show(config=fig_config)\n"
      ],
      "metadata": {
        "id": "5AXOLjh5VeMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции для фильтрации"
      ],
      "metadata": {
        "id": "PKznP_r1foao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_filter(data_in, filters_db_in, config):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    for col, limits in config.items():\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "\n",
        "      data[f\"{col}_minmaxfilter\"] = filter\n",
        "\n",
        "      if col not in ['rh_1_1_1', 'sw_in_1_1_1', 'ppfd_1_1_1']:\n",
        "        data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
        "      else:\n",
        "        if col == 'rh_1_1_1':\n",
        "          data[col] = data[col].clip(upper=limits[1])\n",
        "          data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
        "        else:\n",
        "          data[col] = data[col].clip(lower=limits[0])\n",
        "          data.loc[data.query(f\"{col}<{limits[0]}|{col}>{limits[1]}\").index, f\"{col}_minmaxfilter\"] = 0\n",
        "\n",
        "      if f\"{col}_minmaxfilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_minmaxfilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"min_max_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def qc_filter(data_in, filters_db_in, config):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col, limits in config.items():\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      data[f\"{col}_qcfilter\"] = filter\n",
        "      if f\"qc_{col}\" not in data.columns and col != 'nee':\n",
        "        print(f\"No qc_{col} in data\")\n",
        "        continue\n",
        "      if col != 'nee':\n",
        "        data.loc[data[f\"qc_{col}\"] > config[col], f\"{col}_qcfilter\"] = 0\n",
        "      else:\n",
        "        data.loc[data[f\"qc_co2_flux\"] > config['co2_flux'], f\"nee_qcfilter\"] = 0\n",
        "\n",
        "    if f\"{col}_qcfilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_qcfilter\")\n",
        "    else:\n",
        "      print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"qc_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def std_window_filter(data_in, filters_db_in, config):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    for col, lconfig in config.items():\n",
        "      sigmas = lconfig['sigmas']\n",
        "      window_size = lconfig['window']\n",
        "      min_periods = lconfig['min_periods']#(window_size//2-1)\n",
        "      points_per_day = lconfig['points_per_day']\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      data[f\"{col}_stdwindowfilter\"] = filter\n",
        "      data['tmp_col'] = data[col]\n",
        "      data.loc[~filter.astype(bool), 'tmp_col'] = np.nan\n",
        "      rolling_mean = bg.calc_rolling(data['tmp_col'], rolling_window=window_size, step=points_per_day, min_periods= min_periods)\n",
        "      residuals = data['tmp_col'] - rolling_mean\n",
        "      rolling_sigma = residuals.rolling(window=window_size * points_per_day, center=True, closed='both',  min_periods=window_size * points_per_day//2).std()\n",
        "      data = data.drop(columns='tmp_col')\n",
        "      # print(rolling_sigma, rolling_mean)\n",
        "      upper_bound = rolling_mean + rolling_sigma * sigmas\n",
        "      lower_bound = rolling_mean - rolling_sigma * sigmas\n",
        "      upper_inds = upper_bound[upper_bound < data[col]].index\n",
        "      lower_inds = lower_bound[lower_bound > data[col]].index\n",
        "      data.loc[upper_inds , f\"{col}_stdwindowfilter\"] = 0\n",
        "      data.loc[lower_inds , f\"{col}_stdwindowfilter\"] = 0\n",
        "      # # print(len(lower_inds), len(upper_inds))\n",
        "      # plt.plot(rolling_mean)\n",
        "      # plt.title(col)\n",
        "      # plt.show()\n",
        "\n",
        "      if f\"{col}_stdwindowfilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_stdwindowfilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"std_window_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def meteorological_filter(data_in, filters_db_in, config, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "\n",
        "    for col in [\"co2_flux\", 'h', 'le', 'ch4_flux']:\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_physFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_physFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_physFilter\"] = filter\n",
        "\n",
        "    if 'co2_signal_strength' in data.columns and 'CO2SS_min' in config.keys():\n",
        "      data.loc[data['co2_signal_strength'] < config['CO2SS_min'], 'co2_flux_physFilter'] = 0\n",
        "    else:\n",
        "      print(\"No co2_signal_strength found\")\n",
        "\n",
        "    if 'ch4_signal_strength' in data.columns and 'CH4SS_min' in config.keys():\n",
        "      data.loc[data['ch4_signal_strength'] < config['CH4SS_min'], 'ch4_flux_physFilter'] = 0\n",
        "    else:\n",
        "      print(\"No ch4_signal_strength found\")\n",
        "\n",
        "    if 'p_rain_limit' in config.keys():\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'co2_flux_physFilter'] = 0\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'h_physFilter'] = 0\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'le_physFilter'] = 0\n",
        "      if 'rain_forward_flag' in config:\n",
        "        rain_forward_flag = config['rain_forward_flag']\n",
        "        for i in range(rain_forward_flag):\n",
        "          ind = data.loc[data['p_rain_1_1_1'] >  config['p_rain_limit']].index.shift(i, freq=file_freq)\n",
        "          data.loc[ind, 'co2_flux_physFilter'] = 0\n",
        "          data.loc[ind, 'h_physFilter'] = 0\n",
        "          data.loc[ind, 'le_physFilter'] = 0\n",
        "\n",
        "    if 'RH_max' in config.keys():\n",
        "      RH_max = config['RH_max']\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'co2_flux_physFilter'] = 0\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'le_physFilter'] = 0\n",
        "    logging.info(f\"meteorological_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_rh_filter(data_in, filters_db_in, config, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"co2_flux\", 'le', 'nee']:\n",
        "\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col}\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_rhFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_rhFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_rhFilter\"] = filter\n",
        "\n",
        "    if 'RH_max' in config.keys():\n",
        "      RH_max = config['RH_max']\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'co2_flux_rhFilter'] = 0\n",
        "      if 'nee' in data.columns:\n",
        "        data.loc[data['rh_1_1_1']>RH_max, 'nee_rhFilter'] = 0\n",
        "      data.loc[data['rh_1_1_1']>RH_max, 'le_rhFilter'] = 0\n",
        "    logging.info(f\"meteorological_rh_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def meteorological_night_filter(data_in, filters_db_in, config, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    col_of_interest = [\"h\", 'le', 'nee', 'co2_flux']\n",
        "\n",
        "    for col in col_of_interest:\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_nightFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_nightFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_nightFilter\"] = filter\n",
        "\n",
        "    if \"nee\" in data.columns:\n",
        "      data_night_index = data.query(\"swin_1_1_1<10&nee<0\").index\n",
        "      data.loc[data_night_index, f\"nee_nightFilter\"] = 0\n",
        "\n",
        "    if \"co2_flux\" in data.columns:\n",
        "      data_night_index = data.query(\"swin_1_1_1<10&co2_flux<0\").index\n",
        "      data.loc[data_night_index, f\"co2_flux_nightFilter\"] = 0\n",
        "\n",
        "    data_night_index = data.query(f\"(h<{config['night_h_limits'][0]}|h>{config['night_h_limits'][1]})&swin_1_1_1<10\").index\n",
        "    data.loc[data_night_index, f\"h_nightFilter\"] = 0\n",
        "\n",
        "    data_night_index = data.query(f\"(h<{config['night_le_limits'][0]}|h>{config['night_le_limits'][1]})&swin_1_1_1<10\").index\n",
        "    data.loc[data_night_index, f\"le_nightFilter\"] = 0\n",
        "\n",
        "    # if 'nee' in data.columns:\n",
        "    #   data_night_index = data.query(f'nee>{config[\"day_nee_limit\"]}&swin_1_1_1>=10').index\n",
        "    #   data.loc[data_night_index, f\"nee_nightFilter\"] = 0\n",
        "    logging.info(f\"meteorological_night_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def meteorological_day_filter(data_in, filters_db_in, config, file_freq='30T'):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    col_of_interest = ['nee']\n",
        "\n",
        "    for col in col_of_interest:\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_dayFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_dayFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_dayFilter\"] = filter\n",
        "\n",
        "    if 'nee' in data.columns:\n",
        "      data_day_index = data.query(f'nee>{config[\"day_nee_limit\"]}&swin_1_1_1>=10').index\n",
        "      data.loc[data_day_index, f\"nee_dayFilter\"] = 0\n",
        "    logging.info(f\"meteorological_day_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_co2ss_filter(data_in, filters_db_in, config, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    if 'CO2SS_min' not in config.keys():\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"co2_flux\", 'nee']:\n",
        "\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_co2ssFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_co2ssFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_co2ssFilter\"] = filter\n",
        "\n",
        "    if 'co2_signal_strength' in data.columns:\n",
        "      data.loc[data['co2_signal_strength'] < config['CO2SS_min'], 'co2_flux_co2ssFilter'] = 0\n",
        "    else:\n",
        "      print(\"No co2_signal_strength found\")\n",
        "    logging.info(f\"meteorological_co2ss_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_ch4ss_filter(data_in, filters_db_in, config, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    if 'CH4SS_min' not in config.keys():\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"ch4_flux\"]:\n",
        "\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col} column\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_ch4ssFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_ch4ssFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_ch4ssFilter\"] = filter\n",
        "\n",
        "    if 'ch4_signal_strength' in data.columns:\n",
        "      data.loc[data['ch4_signal_strength'] < config['CH4SS_min'], 'ch4_flux_ch4ssFilter'] = 0\n",
        "    else:\n",
        "      print(\"No ch4_signal_strength found\")\n",
        "    logging.info(f\"meteorological_coh4ss_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def meteorological_rain_filter(data_in, filters_db_in, config, file_freq='30T'):#,rain_forward_flag=3, p_rain_limit=.1,  filter_css=True):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col in [\"co2_flux\", 'h', 'le', 'nee', \"co2_flux\"]:\n",
        "      if col not in data.columns:\n",
        "        print(f\"no {col}\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_rainFilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_rainFilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_rainFilter\"] = filter\n",
        "\n",
        "    if 'p_rain_limit' in config.keys():\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'co2_flux_rainFilter'] = 0\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'h_rainFilter'] = 0\n",
        "      data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'le_rainFilter'] = 0\n",
        "      if 'nee' in data.columns:\n",
        "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'nee_rainFilter'] = 0\n",
        "      if 'ch4_flux' in data.columns:\n",
        "        data.loc[data['p_rain_1_1_1'] > config['p_rain_limit'], 'ch4_flux_rainFilter'] = 0\n",
        "\n",
        "      if 'rain_forward_flag' in config:\n",
        "        rain_forward_flag = config['rain_forward_flag']\n",
        "        for i in range(rain_forward_flag):\n",
        "          ind = data.loc[data['p_rain_1_1_1'] >  config['p_rain_limit']].index.shift(i, freq=file_freq)\n",
        "          if 'nee' in data.columns:\n",
        "            data.loc[ind, 'nee_rainFilter'] = 0\n",
        "          if 'ch4_flux' in data.columns:\n",
        "            data.loc[ind, 'ch4_flux_rainFilter'] = 0\n",
        "          data.loc[ind, 'co2_flux_rainFilter'] = 0\n",
        "          data.loc[ind, 'h_rainFilter'] = 0\n",
        "          data.loc[ind, 'le_rainFilter'] = 0\n",
        "    logging.info(f\"meteorological_rain_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def quantile_filter(data_in, filters_db_in, config):\n",
        "    if len(config) == 0:\n",
        "      return data_in, filters_db_in\n",
        "\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col, limits in config.items():\n",
        "      limit_down, limit_up = limits\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_quantilefilter\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_quantilefilter\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_quantilefilter\"] = filter\n",
        "      up_limit = data.loc[data[f'{col}_quantilefilter'] == 1, col].quantile(limit_up)\n",
        "      down_limit = data.loc[data[f'{col}_quantilefilter'] == 1, col].quantile(limit_down)\n",
        "      f_inds = data.query(f\"{col}_quantilefilter==1\").index\n",
        "      print(\"Quantile filter cut values: \", down_limit, up_limit)\n",
        "      # data.loc[f_inds, f'{col}_quantilefilter'] = ((data.loc[f_inds, col] <= up_limit) & (data.loc[f_inds, col] >= down_limit)).astype(int)\n",
        "      data.loc[data[col] > up_limit, f'{col}_quantilefilter'] = 0\n",
        "      data.loc[data[col] < down_limit, f'{col}_quantilefilter'] = 0\n",
        "\n",
        "      # print(col, (data.loc[f_inds, col] < down_limit).sum(), (data.loc[f_inds, col] > up_limit).sum(), len(data.loc[f_inds, col].index), ((data.loc[f_inds, col] < up_limit) & (data.loc[f_inds, col] > down_limit)).astype(int).sum())\n",
        "      # print(filter.sum(), data[f'{col}_quantilefilter'].sum(), filter.sum() - data[f'{col}_quantilefilter'].sum())\n",
        "    logging.info(f\"quantile_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def mad_hampel_filter(data_in, filters_db_in, config):\n",
        "    if len(config) == 0:\n",
        "      return data_in, filters_db_in\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "\n",
        "    for col, lconfig in config.items():\n",
        "      if col not in data.columns:\n",
        "        print(f\"No column with name {col}, skipping...\")\n",
        "        continue\n",
        "\n",
        "      hampel_window = lconfig['hampel_window']\n",
        "      z = lconfig['z']\n",
        "      filter = get_column_filter(data, filters_db, col)\n",
        "      if len(filter) == 0:\n",
        "        filter = [1]*len(data.index)\n",
        "\n",
        "      if f\"{col}_madhampel\" not in filters_db[col]:\n",
        "        filters_db[col].append(f\"{col}_madhampel\")\n",
        "      else:\n",
        "        print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "      data[f\"{col}_madhampel\"] = filter\n",
        "\n",
        "      # hampel_window = 20\n",
        "      print(f\"Processing {col}\")\n",
        "      outdata = bf.apply_hampel_after_mad(data.loc[data[f'{col}_madhampel']==1, :], [col], z=z, window_size=hampel_window)\n",
        "      data.loc[data[f'{col}_madhampel']==1, f'{col}_madhampel'] = outdata[f'{col}_filtered'].astype(int)\n",
        "      data[f\"{col}_madhampel\"] = data[f\"{col}_madhampel\"].astype(int)\n",
        "\n",
        "    logging.info(f\"mad_hampel_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "def manual_filter(data_in, filters_db_in, col_name, range, value ):\n",
        "    data = data_in.copy()\n",
        "    filters_db = filters_db_in.copy()\n",
        "    filter = get_column_filter(data, filters_db, col_name)\n",
        "    if len(filter) == 0:\n",
        "      filter = [1]*len(data.index)\n",
        "    data[f\"{col_name}_manualFilter\"] = filter\n",
        "    # if range not in data.index:\n",
        "    #   print('WARNING date range is not in index! Nothing is changed!')\n",
        "    #   return data, filters_db\n",
        "    try:\n",
        "      data.loc[range, f\"{col_name}_manualFilter\"] = value\n",
        "    except KeyError:\n",
        "      print(\"ERROR! Check the date range!\")\n",
        "      return data, filters_db\n",
        "\n",
        "    if f\"{col_name}_manualFilter\" not in filters_db[col_name]:\n",
        "        filters_db[col_name].append(f\"{col_name}_manualFilter\")\n",
        "    else:\n",
        "      print(\"filter already exist but will be overwritten\")\n",
        "    logging.info(f\"manual_filter applied with the next config: \\n {config}  \\n\")\n",
        "    return data, filters_db\n",
        "\n",
        "\n",
        "def winter_filter(data_in, filters_db_in, config, date_ranges):\n",
        "  data = data_in.copy()\n",
        "  filters_db = filters_db_in.copy()\n",
        "  if ('winter_nee_limits' not in config.keys()) and ('winter_ch4_flux_limits' not in config.keys()):\n",
        "    return data, filters_db\n",
        "\n",
        "  if 'winter_nee_limits' in config.keys():\n",
        "      for col in ['nee', 'co2_flux']:\n",
        "        if col not in data.columns:\n",
        "          print(f\"No column with name {col}, skipping...\")\n",
        "          continue\n",
        "\n",
        "        filter = get_column_filter(data, filters_db, col)\n",
        "        if len(filter) == 0:\n",
        "          filter = [1]*len(data.index)\n",
        "        data[f\"{col}_winterFilter\"] = filter\n",
        "        try:\n",
        "          for start, stop  in date_ranges:\n",
        "            range=pd.date_range(pd.to_datetime(start, dayfirst=True), pd.to_datetime(stop, dayfirst=True), freq='30min')\n",
        "            inds_down = data.loc[range].query(f\"{col}<{config['winter_nee_limits'][0]}\").index\n",
        "            inds_up = data.loc[range].query(f\"{col}>{config['winter_nee_limits'][1]}\").index\n",
        "            data.loc[inds_up, f\"{col}_winterFilter\"] = 0\n",
        "            data.loc[inds_down, f\"{col}_winterFilter\"] = 0\n",
        "        except KeyError:\n",
        "          print(\"ERROR! Check the date range!\")\n",
        "          return data, filters_db\n",
        "\n",
        "        if f\"{col}_winterFilter\" not in filters_db[col]:\n",
        "          filters_db[col].append(f\"{col}_winterFilter\")\n",
        "        else:\n",
        "          print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "  if 'winter_ch4_flux_limits' in config.keys():\n",
        "      for col in ['ch4_flux']:\n",
        "        if col not in data.columns:\n",
        "          print(f\"No column with name {col}, skipping...\")\n",
        "          continue\n",
        "\n",
        "        filter = get_column_filter(data, filters_db, col)\n",
        "        if len(filter) == 0:\n",
        "          filter = [1]*len(data.index)\n",
        "        data[f\"{col}_winterFilter\"] = filter\n",
        "        try:\n",
        "          for start, stop  in date_ranges:\n",
        "            range=pd.date_range(pd.to_datetime(start, dayfirst=True), pd.to_datetime(stop, dayfirst=True), freq='30min')\n",
        "            inds_down = data.loc[range].query(f\"{col}<{config['winter_ch4_flux_limits'][0]}\").index\n",
        "            inds_up = data.loc[range].query(f\"{col}>{config['winter_ch4_flux_limits'][1]}\").index\n",
        "            data.loc[inds_up, f\"{col}_winterFilter\"] = 0\n",
        "            data.loc[inds_down, f\"{col}_winterFilter\"] = 0\n",
        "        except KeyError:\n",
        "          print(\"ERROR! Check the date range!\")\n",
        "          return data, filters_db\n",
        "\n",
        "        if f\"{col}_winterFilter\" not in filters_db[col]:\n",
        "          filters_db[col].append(f\"{col}_winterFilter\")\n",
        "        else:\n",
        "          print(\"filter already exist but will be overwritten\")\n",
        "\n",
        "  logging.info(f\"winter_filter applied with the next config: \\n {config}  \\n\")\n",
        "  return data, filters_db"
      ],
      "metadata": {
        "id": "EuUwWEPRaVT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Формируем конфиг для загрузки и обработки данных\n"
      ],
      "metadata": {
        "id": "WfWRVITABzrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Конфиг загрузки данных\n",
        "**Необходимо поменять:**\n",
        "\n",
        "`###Запишите название Ваших файлов и путь к ним.`\n",
        "\n",
        "В `config['path']` должен быть либо путь до файла (`= ['1.csv']`) при имени файла 1.csv, либо лист путей в случае загрузки нескольких файлов (`= ['1.csv', '2.csv']`). При импорте через !gdown файла с google-диска достаточно указать в одинарной кавычке *имя файла.расширение*. Не забывайте расширение .csv!\n",
        "\n",
        "**Необходимо проверить:**\n",
        "\n",
        "`  format = \"%Y-%m-%d %H:%M\"  #\"%d/%m/%Y %H:%M\"  #\"%Y-%m-%d %H:%M:%S\"`\n",
        "\n",
        "Проверьте порядок записи даты (год, месяц, день) и разделители даты-времени во входных файлах, открыв их в текстовом редакторе. Возможные варианты:\n",
        "1.  Дата записана как 29.05.2024 и время как 12:00. Тогда они кодируются как\n",
        "\"%d.%m.%Y %H:%M\" – этот формат записан ниже по умолчанию, менять ничего не надо;\n",
        "2.  Дата записана как 29/05/2024 и время как 12:00. Измените в строке ниже формат на \"%d/%m/%Y %H:%M\"\n",
        "3.  Дата записана как 2024-05-29 и время как 1200. Измените в строке ниже формат на \"%Y-%m-%d %H%M\"\n",
        "4.  В остальных случаях действуйте по аналогии. Если в графе time есть секунды, то формат кодируется как \"%Y-%m-%d %H:%M:%S\".\n",
        "\n",
        "**Дополнительные опции (без уровня PRO лучше не менять)**:\n",
        "\n",
        "В `config['time']['converter']` должна находиться функция, которая примет на входа DataFrame и на выходе вернет корректную колонку формата DateTime, которая будет использоваться как временная метка.\n",
        "\n",
        "`config['-9999_to_nan']` будучи установленным `True` заменит -9999 на np.nan для адекватной работы алгоритмов.\n",
        "\n",
        "`config['repair_time']` будучи установленным `True` - проверит колонку с датой-временем на пропуски и монотонность, проведет регенерацию по первой-последней точке с учетом предполагаемой длины шага (вычисляется по паре первых значений ряда)."
      ],
      "metadata": {
        "id": "ox0UplWMe7wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Конфиг загрузки файла full output"
      ],
      "metadata": {
        "id": "CXIuHMoSHMts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {}\n",
        "config['debug'] = False  # True загрузит небольшой кусок файла, а не целый\n",
        "config['-9999_to_nan'] = True #заменяем -9999  на np.nan\n",
        "config['repair_time'] = True #генерируем новые временные метки в случае ошибок\n",
        "\n",
        "#####################\n",
        "#на случай сложных колонок времени\n",
        "config['time'] = {}\n",
        "config['time']['column_name'] = 'datetime'\n",
        "def my_datetime_converter(x):\n",
        "    date = x['date'].astype(str) #x['date'].dt.strftime('%d.%m.%Y') if is_datetime(x['date'].dtype) else x['date'].astype(str)\n",
        "    time = x['time'].astype(str) #x['time'].dt.strftime('%H:%M') if is_datetime(x['time'].dtype) else x['time'].astype(str)\n",
        "\n",
        "    x['tmp_datetime'] = date + \" \" + time\n",
        "    #Проверить формат даты-времени в FullOutput\n",
        "    format = \"%d.%m.%Y %H:%M\"#\"%d/%m/%Y %H:%M\"# \"%Y-%m-%d %H:%M\"  #\"%d/%m/%Y %H:%M\"  #\"%Y-%m-%d %H:%M:%S\"\n",
        "    return pd.to_datetime(x['tmp_datetime'], format=format)#dayfirst=True)#, format=format)\n",
        "config['time']['converter'] = my_datetime_converter\n",
        "#####################\n",
        "\n",
        "###Запишите название Ваших файлов и путь к ним. Если файлы будут импортированы с google-диска\n",
        "###через команду !gdown, то достаточно заменить название файла\n",
        "config['path'] = ['eddy_pro result_SSB 2023.csv']#['eddypro_GHG_biomet_CO2SS_Express_full_output_2023-03-29T020107_exp.csv']#['eddypro_noHMP_full_output_2014_1-5.csv', 'eddypro_noHMP_full_output_2014_5-12.csv']#['/content/eddypro_NCT_GHG_22-23dry_full_output.xlsx', '/content/eddypro_NCT_GHG_22wet_full_output.xlsx', '/content/eddypro_NCT_GHG_23wet_full output.xlsx']#'/content/new.csv'\n",
        "# config['path'] = '/content/DT_Full output.xlsx'"
      ],
      "metadata": {
        "id": "tVJ_DRBrlpYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Конфиг загрузки файла biomet\n",
        "**Необходимо поменять:**\n",
        "\n",
        "`###Запишите название Ваших файлов и путь к ним.`\n",
        "\n",
        "В config['path'] должен быть либо путь до файла (= ['1.csv']) при имени файла 1.csv, либо лист путей в случае загрузки нескольких файлов (= ['1.csv', '2.csv']). При импорте через !gdown файла с google-диска достаточно указать в одинарной кавычке имя файла.расширение. Не забывайте расширение .csv!\n",
        "\n",
        "**Необходимо проверить:**\n",
        "\n",
        "`  format = \"%Y-%m-%d %H%M\"  #\"%d.%m.%Y %H:%M\"  #yyyy-mm-dd HHMM`\n",
        "\n",
        "Проверьте порядок записи даты (год, месяц, день) и разделители даты-времени во входных файлах, открыв их в текстовом редакторе. В биомет-файле по умолчанию дата записана как 2011-11-12 и время как 1200. Кодируется как \"%Y-%m-%d %H%M\". В других случаях поменяйте код формата дат-времени согласно инструкции для блока Конфиг загрузки файла full output"
      ],
      "metadata": {
        "id": "S2Qc-fltJLaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_meteo = {}\n",
        "config_meteo ['use_biomet'] = True\n",
        "config_meteo['debug'] = False  # True загрузит небольшой кусок файла, а не целый\n",
        "config_meteo['-9999_to_nan'] = True #заменяем -9999  на np.nan\n",
        "config_meteo['repair_time'] = True #генерируем новые временные метки в случае ошибок\n",
        "\n",
        "#####################\n",
        "#на случай сложных колонок времени\n",
        "config_meteo['time'] = {}\n",
        "config_meteo['time']['column_name'] = 'datetime'\n",
        "def my_datetime_converter(x):\n",
        "    format = \"%Y-%m-%d %H%M\"  #\"%d.%m.%Y %H:%M\"  #yyyy-mm-dd HHMM\n",
        "    return pd.to_datetime(x[\"TIMESTAMP_1\"], format=format)\n",
        "config_meteo['time']['converter'] = my_datetime_converter\n",
        "#####################\n",
        "\n",
        "###Запишите название Ваших файлов и путь к ним. Если файлы будут импортированы с google-диска\n",
        "###через команду !gdown, то достаточно заменить название файла\n",
        "config_meteo['path'] = 'BiometFy4_2023.csv'#'BiometFy4_2016.csv'#'BiometNCT_2011-22.csv'"
      ],
      "metadata": {
        "id": "H7E5LGx1DVsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выбор колонок для графиков и фильтраций"
      ],
      "metadata": {
        "id": "DtxFTNnEfENz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Соберем обзорную информацию о нужных величинах:\n",
        "cols_to_investigate = []\n",
        "cols_to_investigate.append(\"co2_flux\")\n",
        "cols_to_investigate.append(\"ch4_flux\")\n",
        "cols_to_investigate.append(\"LE\")\n",
        "cols_to_investigate.append(\"H\")\n",
        "cols_to_investigate.append(\"co2_strg\")\n",
        "cols_to_investigate.append(\"Ta_1_1_1\")\n",
        "cols_to_investigate.append(\"RH_1_1_1\")\n",
        "cols_to_investigate.append(\"VPD_1_1_1\")\n",
        "cols_to_investigate.append(\"P_1_1_1\")\n",
        "cols_to_investigate.append(\"SWIN_1_1_1\")\n",
        "cols_to_investigate.append(\"PPFD_1_1_1\")\n",
        "# cols_to_investigate.append(\"co2_signal_strength\")\n",
        "# cols_to_investigate.append(\"ch4_signal_strength\")\n",
        "\n",
        "cols_to_investigate =  [k.lower()for k in cols_to_investigate]"
      ],
      "metadata": {
        "id": "nLnivFTtg9cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Настройка параметров анализа данных\n",
        "\n",
        "Все настройки для co2_flux будут применены для nee, в случае его расчета"
      ],
      "metadata": {
        "id": "wVpYvr9_fKBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Фильтрация физическая"
      ],
      "metadata": {
        "id": "FH2uRGi4p5Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points_per_day = 2*24\n",
        "window_size = 10\n",
        "\n",
        "file_freq = '30T'\n",
        "calc_nee = True\n",
        "\n",
        "ias_output_prefix = 'tv_fy4'\n",
        "ias_output_version = 'v01'"
      ],
      "metadata": {
        "id": "pPemVdWVbq2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конфиг фильтрации по флагам качества, данные с флагами в интервале (-inf, val] будут помечены как валидные, а данные с значением флага больше порога будут исключены."
      ],
      "metadata": {
        "id": "5MK90gyzQryZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qc_config = {}\n",
        "qc_config['h'] = 1  #Если система флагов была 1-9, поправить\n",
        "qc_config['le'] = 1  #Если система флагов была 1-9, поправить\n",
        "qc_config['co2_flux'] = 1  #Если система флагов была 1-9, поправить\n",
        "qc_config['ch4_flux'] = 1  #Если система флагов была 1-9, поправить"
      ],
      "metadata": {
        "id": "ukl734CBblay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конфиг фильтрации по метеопараметрам, возможные опции:\n",
        "\n",
        "*   `CO2SS_min` - уберет co2_signal_strength ниже указанного значения\n",
        "*   `p_rain_limit` - уберет H, LE и CO2_FLUX для P_rain_1_1_1 выше указанного лимита\n",
        "*   `rain_forward_flag` - уберет  значения на указанное число записей вперед от каждого отфильтрованного на прошлом шаге значения\n",
        "*   `RH_max` - уберет значения LE и CO2_FLUX для которых RH_1_1_1 больше указанного порога\n",
        "* `day_nee_limit` порог для nee в дневное время (исключение положительных NEE днем)\n",
        "* `night_h_limits` `night_le_limits` допустимые ночные диапазоны LE и H\n",
        "\n",
        "Ночные (Swin<10)  NEE<0 исключаются\n",
        "\n",
        "При отсутствии в конфиге какого-либо из параметров фильтрация не применяется.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPIFpLN_-8Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meteo_filter_config = {}\n",
        "meteo_filter_config['CO2SS_min'] = 80.\n",
        "meteo_filter_config['p_rain_limit'] = .1\n",
        "meteo_filter_config['rain_forward_flag'] = 2\n",
        "# meteo_filter_config['RH_max'] = 98 #нужно применять только тогда, когда нет CO2SS (образование конденсата) и диагностики анемометра\n",
        "#и данные не были отфильтрованы по этим показателям на этапе расчета в EddyPro\n",
        "meteo_filter_config['day_nee_limit'] = 5\n",
        "meteo_filter_config['night_h_limits'] = [-50, 20]\n",
        "meteo_filter_config['night_le_limits'] = [-50, 20]\n",
        "\n",
        "meteo_filter_config['winter_nee_limits'] = [0, 5]\n",
        "meteo_filter_config['winter_ch4_flux_limits'] = [-1, 1]\n",
        "\n",
        "meteo_filter_config['CH4SS_min'] = 20."
      ],
      "metadata": {
        "id": "vxpiAbWk2yYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Фильтрация статистическая"
      ],
      "metadata": {
        "id": "utUX7SA4qA_I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6wFri3jhZeu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конфиг фильтрации по абсолютным значениям.\n",
        "Для `rh_1_1_1` значения выше границы не отбрасываются, а заменяются на пограничные. Для `ppfd_1_1_1`, `swin_1_1_1` аналогично обрабатываются минимальные значения."
      ],
      "metadata": {
        "id": "wWISuF-xQCwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_config  = {}\n",
        "min_max_config['co2_flux'] = [-40, 40]\n",
        "min_max_config['co2_strg'] = [-20, 20]\n",
        "min_max_config['h'] = [-100, 800]\n",
        "min_max_config['le'] = [-100, 1000]\n",
        "min_max_config['u_star'] = [0, 10]\n",
        "min_max_config['ta_1_1_1'] = [-50, 50]\n",
        "min_max_config['p_1_1_1'] = [0, 100]\n",
        "min_max_config['vpd_1_1_1'] = [0, 50]\n",
        "min_max_config['rh_1_1_1'] = [0, 100]#max\n",
        "min_max_config['swin_1_1_1'] = [0, 1200]#min\n",
        "min_max_config['ppfd_1_1_1'] = [0, 2400]#min\n",
        "min_max_config['ch4_flux'] = [-10, 10]"
      ],
      "metadata": {
        "id": "HQfIYFOd9uzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конфиг фильтрации по отклонению от среднего хода.\n",
        "* `window` - размер окна в днях для подсчета среднего хода\n",
        "* `points_per_day` - число измерений в сутках\n",
        "* `sigmas` - допустимый интервал отклонения от среднего хода; вне интервала значения помечаются как отфильтрованные"
      ],
      "metadata": {
        "id": "vmyTKbV1RdjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_filter_config = {}\n",
        "window_filter_config['co2_flux'] = {'sigmas': 2, 'window': 10, 'points_per_day': points_per_day}\n",
        "window_filter_config['ch4_flux'] = {'sigmas': 2, 'window': 10, 'points_per_day': points_per_day}\n",
        "window_filter_config['ta_1_1_1'] = {'sigmas': 2, 'window': 10, 'points_per_day': points_per_day}\n",
        "window_filter_config['u_star'] = {'sigmas': 2, 'window': 10, 'points_per_day': points_per_day}\n",
        "for col in ['h', 'le', 'rh_1_1_1', 'vpd_1_1_1']:\n",
        "  window_filter_config[col] = {'sigmas': 4, 'window': 10, 'points_per_day': points_per_day, 'min_periods': 4}\n",
        "for col in ['swin_1_1_1', 'ppfd_1_1_1']:\n",
        "  window_filter_config[col] = {'sigmas': 4, 'window': 10, 'points_per_day': points_per_day, 'min_periods': 4}"
      ],
      "metadata": {
        "id": "xfRVNYbFYzG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конфиг фильтрации выше-ниже порога по перцентилям (выпадающие строки отфильтровываются)"
      ],
      "metadata": {
        "id": "KF_MGD7pSGre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantile_filter_config = {}\n",
        "quantile_filter_config['co2_flux'] = [0.01, 0.99]\n",
        "quantile_filter_config['ch4_flux'] = [0.01, 0.99]\n",
        "quantile_filter_config['co2_strg'] = [0.01, 0.99]"
      ],
      "metadata": {
        "id": "asO_t2tZmiD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конфиг для статистической фильтрации."
      ],
      "metadata": {
        "id": "cPiTN288UaP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# madhampel_filter_config = {i:{'z': 5.5, 'hampel_window': 10} for i in cols_to_investigate if 'p_1_1_1' not in i}\n",
        "madhampel_filter_config = {}\n",
        "madhampel_filter_config['co2_flux'] = {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['ch4_flux'] = {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['le'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['h'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config['co2_strg'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'ta_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'rh_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'vpd_1_1_1'] =  {'z': 5.5, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'swin_1_1_1'] =  {'z': 7.0, 'hampel_window': 10}\n",
        "madhampel_filter_config[ 'ppfd_1_1_1'] =  {'z': 7.0, 'hampel_window': 10}"
      ],
      "metadata": {
        "id": "2b3eBVFUq3AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загружаем данные"
      ],
      "metadata": {
        "id": "wVF1vDm4EauW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Необходимо поменять:**\n",
        "\n",
        "После !gdown вставьте символы после d/ и до следующего / из публичной ссылки на файл, лежащий на google-диске. К примеру, если ссылка\n",
        "https://drive.google.com/file/d/1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB/view?usp=sharing,\n",
        "то команда будет записана как\n",
        "`!gdown 1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB`\n",
        "\n",
        "`#Загрузка файла full output`\n",
        "Здесь нужно прописать символы из ссылки на файл full output\n",
        "\n",
        "`#Загрузка файла biomet`\n",
        "Здесь нужно прописать символы из ссылки на файл biomet"
      ],
      "metadata": {
        "id": "LV9FvvtnVqdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # !gdown https://docs.google.com/spreadsheets/d/1Xb3hKpL0vZEfa8ZfSlQE78Jn41gEekfvJtFnKAcotMA/export?format=csv -O new.csv\n",
        "# # !gdown 1-e8Bx9UXj_yZxlPamb6s80YriIEWcL_i\n",
        "\n",
        "#Загрузка файла full output\n",
        "#https://drive.google.com/file/d/1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB/view?usp=sharing\n",
        "# !gdown 1fGhmvra0evNzM0xkM2nu5T-N_rSPoXUB\n",
        "!gdown 1CGJmXyFu_pmzTLitG5aU8fwY8gW3CI1n\n",
        "\n",
        "#Загрузка файла biomet\n",
        "#https://drive.google.com/file/d/1FjiBcSspDBlYlcg9Vzy71Sm49gOFZGBF/view?usp=sharing\n",
        "# !gdown 1FjiBcSspDBlYlcg9Vzy71Sm49gOFZGBF\n",
        "!gdown 19XsOw5rRJMVMyG1ntRpibfkUpRAP2H4k"
      ],
      "metadata": {
        "id": "KMu4IqY45HG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, time = bg.load_df(config)\n",
        "data = data[next(iter(data))]  #т.к. изначально у нас словарь"
      ],
      "metadata": {
        "id": "Xw5TapK10EhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяем корректность временных меток. Убираем повторы, дополняем пропуски. На случай загрузки нескольких файлов. При загрузке одного делается автоматически."
      ],
      "metadata": {
        "id": "6j7ombDYqyC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_number_of_time_entries = pd.date_range(data[time].iloc[0], data[time].iloc[-1],\n",
        "                                                freq=data[time].iloc[1] - data[time].iloc[0])\n",
        "if not correct_number_of_time_entries.size == len(data.index):\n",
        "    if correct_number_of_time_entries.size < len(data.index):\n",
        "      print('Removing duplicates')\n",
        "      data = data[~data.index.duplicated(keep='first')]\n",
        "    else:\n",
        "      print(\"Adding missed timestamps\")\n",
        "      dates = pd.date_range(data.index.min(), data.index.max())\n",
        "      data = data.reindex(dates, fill_value=np.nan)\n",
        "data.index = pd.DatetimeIndex(data.index, freq=\"30T\")\n",
        "print(\"Диапазон времени full_output: \", data.index[[0, -1]])\n",
        "logging.info(f\"Data loaded from {config['path']}\")\n",
        "logging.info(\"Time range for full_output: \"+ \" - \".join(data.index[[0,-1]].strftime('%Y-%m-%d %H:%M')))"
      ],
      "metadata": {
        "id": "FWCJ-EVrjnH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  data_meteo, time_meteo  = bg.load_df(config_meteo)\n",
        "  data_meteo = data_meteo[next(iter(data_meteo))]  #т.к. изначально у нас словарь\n",
        "  print(\"Диапазон времени метео: \", data_meteo.index[[0, -1]])\n",
        "  logging.info(f\"MeteoData loaded from {config_meteo['path']}\")\n",
        "  logging.info(\"Time range for meteo: \"+ \" - \".join(data_meteo.index[[0,-1]].strftime('%Y-%m-%d %H:%M')))"
      ],
      "metadata": {
        "id": "65DLIIucNOPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Колонки в FullOutput \\n\", data.columns.to_list())\n",
        "if config_meteo ['use_biomet']:\n",
        "  print(\"Колонки в метео \\n\", data_meteo.columns.to_list())"
      ],
      "metadata": {
        "id": "rZbqd6adhHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сливаем в один DataFrame."
      ],
      "metadata": {
        "id": "FF78Wlq9rD_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  data = data.join(data_meteo, how='left', rsuffix='_meteo')\n",
        "  if data[data_meteo.columns[-1]].isna().sum() == len(data.index):\n",
        "    print(\"Bad meteo data range, skipping! Setting config_meteo ['use_biomet']=False\")\n",
        "    config_meteo ['use_biomet'] = False"
      ],
      "metadata": {
        "id": "9v0rxHehhZEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns= data.columns.str.lower()"
      ],
      "metadata": {
        "id": "C8lLDYOWzH2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Предобработка"
      ],
      "metadata": {
        "id": "QDHkyl_PruXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переименовываем колонки для единого формата, рассчитываем VPD <-> RH, SWIN <-> RG и PAR <-> SWIN в случае отсутствия.\n"
      ],
      "metadata": {
        "id": "Nh5MosYXS6aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "have_rh_flag = False\n",
        "have_vpd_flag = False\n",
        "have_par_flag = False\n",
        "have_swin_flag = False\n",
        "have_rg_flag = False\n",
        "have_p_flag = False\n",
        "have_pr_flag = False\n",
        "have_ppfd_flag = False\n",
        "\n",
        "for col_name in data.columns:\n",
        "  if 'u*' in col_name:\n",
        "    print(f\"renaming {col_name} to u_star\")\n",
        "    data = data.rename(columns={col_name: 'u_star'})\n",
        "  if 'ppfd_in_1_1_1' in col_name:\n",
        "    print(f\"renaming {col_name} to ppfd_1_1_1\")\n",
        "    data = data.rename(columns={col_name: 'ppfd_1_1_1'})\n",
        "  if 'sw_in_1_1_1' in col_name:\n",
        "    print(f\"renaming {col_name} to swin_1_1_1\")\n",
        "    data = data.rename(columns={col_name: 'swin_1_1_1'})\n",
        "  if 'co2_signal_strength' in col_name:\n",
        "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
        "  if \"rh_1_1_1\" in col_name:\n",
        "    have_rh_flag =True\n",
        "  if \"vpd_1_1_1\" in col_name:\n",
        "    have_vpd_flag = True\n",
        "  if 'swin' in col_name or 'sw_in' in col_name:\n",
        "    have_swin_flag = True\n",
        "  if 'par' in col_name:\n",
        "    have_par_flag = True\n",
        "  if 'rg_1_1_1' in col_name:\n",
        "    have_rg_flag = True\n",
        "  if 'p_1_1_1' in col_name:\n",
        "    have_p_flag = True\n",
        "  if 'p_rain_1_1_1' in col_name:\n",
        "    have_pr_flag = True\n",
        "  if 'ppfd_1_1_1' in col_name:\n",
        "    have_ppfd_flag = True\n",
        "  if col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()] or 'co2_signal_strength' in col_name:\n",
        "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
        "  if col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()] or 'ch4_signal_strength' in col_name:\n",
        "    print(f\"renaming {col_name} to ch4_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'ch4_signal_strength'})\n",
        "\n",
        "if not (have_rg_flag or have_swin_flag):\n",
        "  print(\"NO RG AND SWIN\")\n",
        "else:\n",
        "  print(\"Checking RG-SWIN pair\")\n",
        "  if not have_rg_flag:\n",
        "    data['rg_1_1_1'] = data['swin_1_1_1']\n",
        "  if not have_swin_flag:\n",
        "    data['swin_1_1_1'] = data['rg_1_1_1']\n",
        "    have_swin_flag = True\n",
        "\n",
        "\n",
        "if not (have_p_flag or have_pr_flag):\n",
        "  print(\"NO P and P_RAIN\")\n",
        "else:\n",
        "  print(\"Checking P <-> P_rain pair\")\n",
        "  if not have_p_flag:\n",
        "    data['p_1_1_1'] = data['p_rain_1_1_1']\n",
        "  if not have_pr_flag:\n",
        "    data['p_rain_1_1_1'] = data['p_1_1_1']\n",
        "\n",
        "\n",
        "if not (have_vpd_flag or have_rh_flag):\n",
        "  print(\"NO RH AND VPD\")\n",
        "else:\n",
        "    temp_k = (data['ta_1_1_1'] + 273.15)\n",
        "    logE = 23.5518-(2937.4/temp_k)-4.9283*np.log10(temp_k)\n",
        "    ehpa = np.power(10, logE)\n",
        "    if not have_vpd_flag:\n",
        "      print(\"calculating vpd_1_1_1 from rh\")\n",
        "      data['vpd_1_1_1'] = ehpa - (ehpa*data['rh_1_1_1']/100)\n",
        "    if not have_rh_flag:\n",
        "      print(\"calculating rh_1_1_1 from vpd\")\n",
        "      data['rh_1_1_1'] = ehpa\n",
        "\n",
        "\n",
        "if not (have_par_flag or have_swin_flag):\n",
        "  print(\"NO PAR and SWin\")\n",
        "else:\n",
        "    if not have_par_flag:\n",
        "      data['par'] = data['swin_1_1_1'] / 0.47#SWin=PAR*0.47\n",
        "    if not have_swin_flag:\n",
        "      data['swin_1_1_1'] = 0.47 * data['par']\n",
        "\n",
        "if not (have_ppfd_flag or have_swin_flag):\n",
        "  print(\"NO PPFD and SWin\")\n",
        "else:\n",
        "    if not have_ppfd_flag:\n",
        "      data['ppfd_1_1_1'] = data['swin_1_1_1'] / 0.47\n",
        "    if not have_swin_flag:\n",
        "      data['swin_1_1_1'] = 0.47 * data['ppfd_1_1_1']\n",
        "\n",
        "for col_name in ['co2_signal_strength_7500_mean', 'CO2SS'.lower()]:\n",
        "  # print(data.columns.to_list())\n",
        "  if col_name in data.columns.to_list():\n",
        "    print(f\"renaming {col_name} to co2_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'co2_signal_strength'})\n",
        "\n",
        "for col_name in ['ch4_signal_strength_7700_mean', 'CH4SS'.lower()]:\n",
        "  # print(data.columns.to_list())\n",
        "  if col_name in data.columns.to_list():\n",
        "    print(f\"renaming {col_name} to ch4_signal_strength\")\n",
        "    data = data.rename(columns={col_name: 'ch4_signal_strength'})\n"
      ],
      "metadata": {
        "id": "mAdYXJFdSRbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Получение NEE из потока CO2 и накопления"
      ],
      "metadata": {
        "id": "soyyX-MCbaXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка накопления. Рассчитанное по одному уровню в EddyPro не всегда корректно. Корректность проверяется суточным ходом: должен быть рост запаса в течение ночи, резкое уменьшение утром."
      ],
      "metadata": {
        "id": "lqWwGSMObro4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация co2_strg с удалением значений выше и ниже пороговых перцентилей. Заполнение пропусков co2_strg  длиной 3 точки и менее – линейной интерполяцией. Полученные отфильтрованные и заполненные значения co2_strg показываются на графике. Принятие решения, суммировать ли co2_flux и co2_strg для получения NEE или работать дальше с co2_flux."
      ],
      "metadata": {
        "id": "2yqwO7Uhcjmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Пробелы длиной 3 и меньше заполняются линейно\n",
        "if calc_nee and 'co2_strg' in data.columns:\n",
        "  tmp_data = data.copy()\n",
        "  tmp_data['co2_strg_tmp'] = tmp_data['co2_strg'].copy()\n",
        "  tmp_filter_db =  {'co2_strg_tmp': []}\n",
        "  if 'co2_strg' in  quantile_filter_config.keys():\n",
        "    tmp_q_config = {'co2_strg_tmp':quantile_filter_config['co2_strg']}\n",
        "  else:\n",
        "    tmp_q_config = {}\n",
        "  tmp_filter_db = {'co2_strg_tmp':[]}\n",
        "  tmp_data, tmp_filter_db = quantile_filter(tmp_data, tmp_filter_db, tmp_q_config)\n",
        "  tmp_data.loc[~get_column_filter(tmp_data, tmp_filter_db, 'co2_strg_tmp').astype(bool), 'co2_strg_tmp'] = np.nan\n",
        "  # tmp_data['co2_strg_tmp'] = tmp_data['co2_strg_tmp'].interpolate(limit=3)\n",
        "  # tmp_data['co2_strg_tmp'].fillna(bg.calc_rolling(tmp_data['co2_strg_tmp'], rolling_window=10 , step=24 * 2, min_periods=4))\n",
        "  basic_plot(tmp_data, ['co2_strg_tmp'], tmp_filter_db)\n",
        "  print(tmp_q_config, tmp_filter_db, tmp_data['co2_strg_tmp_quantilefilter'].value_counts())\n"
      ],
      "metadata": {
        "id": "cjt05XXtbr69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Решаем, суммировать ли исходный co2_flux и co2_strg_filtered_filled для получения NEE\n",
        "calc_with_strg = True   #В случае, если дальше работаем с NEE, оставить True.\n",
        "logging.info(f\"calc_with_strg is set to {calc_with_strg}\")\n",
        "#Для того, чтобы работать дальше с co2_flux, игнорируя co2_strg, поставить False"
      ],
      "metadata": {
        "id": "2IQ7W6pslYF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if calc_nee and 'co2_strg' in data.columns:\n",
        "  if calc_with_strg:\n",
        "    data['nee'] = (tmp_data['co2_flux'] + tmp_data['co2_strg_tmp']).copy()\n",
        "  else:\n",
        "    data['nee'] = data['co2_flux'].copy()\n",
        "  del tmp_data\n",
        "  if 'nee' not in cols_to_investigate:\n",
        "    cols_to_investigate.append('nee')\n",
        "  for filter_config in [qc_config, meteo_filter_config, min_max_config, window_filter_config, quantile_filter_config, madhampel_filter_config]:\n",
        "    if 'co2_flux' in filter_config:\n",
        "      filter_config['nee'] = filter_config['co2_flux']"
      ],
      "metadata": {
        "id": "ueuvsNxYdtgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Обзор статистики по интересующим колонкам"
      ],
      "metadata": {
        "id": "mUgwuaFYribB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_investigate = [p for p in cols_to_investigate if p in data.columns]"
      ],
      "metadata": {
        "id": "dhcplCMbXtkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[:, cols_to_investigate].describe()\n",
        "\n",
        "fig, axs = plt.subplots(ncols=min(3, len(cols_to_investigate)), nrows=int(np.ceil(len(cols_to_investigate)/3)), squeeze=False, figsize=(13, 8))\n",
        "\n",
        "for ind, ax in enumerate(axs.reshape(-1)):\n",
        "  if ind >= len(cols_to_investigate):\n",
        "    break\n",
        "  feature = cols_to_investigate[ind]\n",
        "  ax.boxplot(data[feature].to_numpy()[~np.isnan(data[feature].to_numpy())])\n",
        "  ax.set_title(f\"Boxplot for {feature}\")\n",
        "plt.tight_layout()\n",
        "fig.show()\n",
        "\n",
        "data[cols_to_investigate].describe()"
      ],
      "metadata": {
        "id": "YfusqiotOi3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Фильтрация данных физическая"
      ],
      "metadata": {
        "id": "0oJLXYGbr93S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data = data.copy()\n",
        "filters_db = {col: [] for col in plot_data.columns.to_list()}\n",
        "print(plot_data.columns.to_list())"
      ],
      "metadata": {
        "id": "apGNk8eBxgBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по флагам качества"
      ],
      "metadata": {
        "id": "BL_6XxGGsCBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = qc_filter(plot_data, filters_db, qc_config)"
      ],
      "metadata": {
        "id": "GGwe7_uU1C8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по порогу CO2SS и CH4SS"
      ],
      "metadata": {
        "id": "M_gKSTNYyzjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = meteorological_co2ss_filter(plot_data, filters_db, meteo_filter_config)"
      ],
      "metadata": {
        "id": "viq7BZue9Ett"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = meteorological_ch4ss_filter(plot_data, filters_db, meteo_filter_config)"
      ],
      "metadata": {
        "id": "5RrPfxfiJGhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по допустимым значениям RH"
      ],
      "metadata": {
        "id": "qwqVDeH6y73_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = meteorological_rh_filter(plot_data, filters_db, meteo_filter_config)"
      ],
      "metadata": {
        "id": "11isfvNZ9FGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по наличию дождя"
      ],
      "metadata": {
        "id": "oSX2h9QzzFkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  plot_data, filters_db = meteorological_rain_filter(plot_data, filters_db, meteo_filter_config)"
      ],
      "metadata": {
        "id": "jz696mc09FlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по ночным и дневным допустимым диапазонам"
      ],
      "metadata": {
        "id": "Xy2y00P1zJtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  plot_data, filters_db = meteorological_night_filter(plot_data, filters_db, meteo_filter_config)"
      ],
      "metadata": {
        "id": "ED_Qh6TS0Qkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  plot_data, filters_db = meteorological_day_filter(plot_data, filters_db, meteo_filter_config)"
      ],
      "metadata": {
        "id": "X3Vguu8MK635"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## фильтрация зимних периодов, уточните даты!"
      ],
      "metadata": {
        "id": "fzfTJdNe68Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if ('winter_nee_limits' in meteo_filter_config.keys()) or ('winter_ch4_flux_limits' in meteo_filter_config.keys()):\n",
        "  plot_albedo(plot_data, filters_db)"
      ],
      "metadata": {
        "id": "wJ87D57S7A91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  unroll_filters_db = filters_db.copy()\n",
        "  date_ranges = [\n",
        "      ['01.01.2023 00:00', '26.03.2023 00:00'],\n",
        "      ['13.11.2023 00:00', '31.12.2023 00:00'],\n",
        "  ]\n",
        "  #date_ranges = []\n",
        "  # date_ranges.append(['25.8.2014 00:00', '26.8.2014 00:00'])\n",
        "  plot_data, filters_db = winter_filter(plot_data, filters_db, meteo_filter_config, date_ranges)\n"
      ],
      "metadata": {
        "id": "Z_RAYINf67PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация по футпринту\n",
        "будет в следующей версии скрипта\n",
        "\n",
        "`fetch = 1 #или 0. 1 – остаются, 0 – убираются `"
      ],
      "metadata": {
        "id": "iipFLxf6fu5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Фильтрация данных статистическая"
      ],
      "metadata": {
        "id": "UAdRtCPGq6_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по минимальным и максимальным допустимым значениям"
      ],
      "metadata": {
        "id": "LcwZplknsHJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if config_meteo ['use_biomet']:\n",
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = min_max_filter(plot_data, filters_db, min_max_config)"
      ],
      "metadata": {
        "id": "FyJaM1zC1DDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по перцентилям"
      ],
      "metadata": {
        "id": "j62U1dw8sTEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if config_meteo ['use_biomet']:\n",
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = quantile_filter(plot_data, filters_db, quantile_filter_config)"
      ],
      "metadata": {
        "id": "aNQ4XDK01DME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## по отклонению от среднего хода"
      ],
      "metadata": {
        "id": "7Sg76Bwasnb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, filters_db = std_window_filter(plot_data, filters_db, window_filter_config)"
      ],
      "metadata": {
        "id": "uoDvHhoQ2MMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Фильтрация выбросов MAD & Hampel"
      ],
      "metadata": {
        "id": "iXl5RdINss9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unroll_filters_db = filters_db.copy()\n",
        "plot_data, tmp_filter = mad_hampel_filter(plot_data, filters_db, madhampel_filter_config)"
      ],
      "metadata": {
        "id": "gl9cImVr2MO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ручная фильтрация\n",
        "\n",
        "1- оставить,\n",
        "0 -удалить\n"
      ],
      "metadata": {
        "id": "iu8MLKyh1AFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "man_ranges = [\n",
        "    ['1.5.2023 00:00', '1.6.2023 00:00'],\n",
        "    # ['25.8.2015 00:00', '26.8.2015 00:00'],\n",
        "]\n",
        "for start, stop in man_ranges:\n",
        "  plot_data, tmp_filter = manual_filter(plot_data, filters_db, col_name=\"nee\", range=pd.date_range(pd.to_datetime(start, dayfirst=True), pd.to_datetime(stop, dayfirst=True), freq='30min'), value=0)"
      ],
      "metadata": {
        "id": "ADy534At0_fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## На случай необходимости откатить последний фильтр\n",
        "Не работает с повторно-запущенными несколько раз"
      ],
      "metadata": {
        "id": "APyqyqSEHx3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filters_db = unroll_filters_db.copy()"
      ],
      "metadata": {
        "id": "IYmSC2gpH4zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сводная таблица результатов фильтрации"
      ],
      "metadata": {
        "id": "quGbtDaJ_gID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_filters = {}\n",
        "for key, filters in filters_db.items():\n",
        "   if len(filters) > 0:\n",
        "    pl_data = plot_data.copy()\n",
        "    for filter_name in filters:\n",
        "      all_filters[filter_name] = []\n",
        "      all_filters[filter_name].append(len(pl_data.index))\n",
        "      filtered_amount = len(pl_data.query(f\"{filter_name}==0\").index)\n",
        "      all_filters[filter_name].append(filtered_amount)\n",
        "      # old_val =  len(pl_data.index)\n",
        "      pl_data = pl_data.query(f\"{filter_name}==1\")\n",
        "      # print(filter_name, filtered_amount, len(pl_data.index) - old_val)\n",
        "fdf_df = pd.DataFrame(all_filters)\n",
        "print(\"Какая часть данных от общего количества (в %) была отфильтрована:\")\n",
        "print(fdf_df.iloc[1]/len(plot_data)*100)\n",
        "logging.info(\"Какая часть данных от общего количества (в %) была отфильтрована:\")\n",
        "logging.info(fdf_df.iloc[1]/len(plot_data)*100)"
      ],
      "metadata": {
        "id": "Pg78qGJ9_miW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Отрисовка рядов"
      ],
      "metadata": {
        "id": "gA_IPavss0bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Отрисовка результатов фильтрации данных"
      ],
      "metadata": {
        "id": "ijPM6mnJtMv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_terator = iter(cols_to_investigate)"
      ],
      "metadata": {
        "id": "50Xhczc-BRc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для экономии памяти и адекватной работы колаба графики будут выводиться поочередно при повторном запуске ячейки."
      ],
      "metadata": {
        "id": "uat4oESzU4__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col2plot = next(plot_terator, False)\n",
        "col2plot = 'nee' #Подставить нужное: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1, co2_signal_strength\n",
        "#Или просто запускать повторно для переключения к следующему параметру\n",
        "if col2plot:\n",
        "  make_filtered_plot(plot_data, col2plot, filters_db)\n",
        "else:\n",
        "  print(\"No more data, start from the begining!\")\n",
        "  plot_terator = iter(cols_to_investigate)"
      ],
      "metadata": {
        "id": "NhNoFAd7DqNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #линейное заполнение пропусков, limit - сколько может быть пропущенных подряд\n",
        "# for col in cols_to_investigate:\n",
        "#   plot_data[col] = plot_data[col].interpolate(limit=5)"
      ],
      "metadata": {
        "id": "ZG_wF2qW-Qwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Заполнение ходом\n",
        "# for col in cols_to_investigate:\n",
        "#   plot_data[col].fillna(bg.calc_rolling(plot_data[col], rolling_window=10 * 2 * 24, step=24 * 2, min_periods=7*2*24))"
      ],
      "metadata": {
        "id": "VtJ8wyx2-XCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Отрисовка среднего хода для отфильтрованных рядов"
      ],
      "metadata": {
        "id": "MwuXRVTMtBz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_terator = iter(cols_to_investigate)"
      ],
      "metadata": {
        "id": "pWDTiucTgRlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Пример вычисления среднего хода\n",
        "\n",
        "col2plot = next(plot_terator, False)\n",
        "#Можно задать вручную\n",
        "# col2plot = 'h'#\"co2_flux\"\n",
        "col2plot = ['nee', 'le'] #Подставить нужное: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1\n",
        "#Или просто запускать повторно для переключения к следующему параметру\n",
        "if col2plot:\n",
        "  basic_plot(plot_data, col2plot, filters_db)\n",
        "else:\n",
        "  print(\"No more data, start from the begining!\")\n",
        "  plot_terator = iter(cols_to_investigate)"
      ],
      "metadata": {
        "id": "COKiwe7020D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тепловые карты потоков для отфильтрованных данных"
      ],
      "metadata": {
        "id": "RKEg6YBstXMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['nee', 'le', 'h']: #Подставить нужное: co2_flux, le, h, co2_strg, ta_1_1_1, rh_1_1_1, vpd_1_1_1, p_1_1_1, swin_1_1_1, ppfd_1_1_1\n",
        "#Или просто запускать повторно для переключения к следующему параметру\n",
        "  plot_nice_year_hist_plotly(plot_data, col ,time, filters_db)\n"
      ],
      "metadata": {
        "id": "mCUJYURKEL-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сохранение данных в формате REddyProc"
      ],
      "metadata": {
        "id": "dokSxicNtdva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим шаблон шапки для файла REddyProc и сохраним требуемые переменные, не забыв учесть фильтрацию."
      ],
      "metadata": {
        "id": "tDqsi61kSeak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddyproc_filename = \"REddyProc.txt\"\n",
        "output_template = {'Year': ['-'],\t'DoY': ['-'],\t'Hour': ['-'],\t'NEE': ['umolm-2s-1'],\t'LE': ['Wm-2'],\t'H': ['Wm-2'],\t'Rg': ['Wm-2'],\t'Tair': ['degC'], \t'Tsoil': ['degC'],\t'rH': ['%'], \t'VPD': ['hPa'], \t'Ustar': ['ms-1']}"
      ],
      "metadata": {
        "id": "YVu2UrCzLqb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  eddy_df = plot_data.copy()\n",
        "\n",
        "  for column, filter in filters_db.items():\n",
        "    filter = get_column_filter(eddy_df, filters_db, column)\n",
        "    eddy_df.loc[~filter.astype(bool), column] = np.nan\n",
        "\n",
        "\n",
        "  eddy_df['Year'] = eddy_df[time].dt.year\n",
        "  eddy_df['DoY'] = eddy_df[time].dt.dayofyear\n",
        "  eddy_df['Hour'] = eddy_df[time].dt.hour + eddy_df[time].dt.minute/60\n",
        "\n",
        "  eddy_df['NEE'] = eddy_df['nee'].fillna(-9999)\n",
        "  eddy_df['LE'] = eddy_df['le'].fillna(-9999)\n",
        "  eddy_df['H'] = eddy_df['h'].fillna(-9999)\n",
        "  eddy_df['Rg'] = eddy_df['rg_1_1_1'].fillna(-9999)\n",
        "  eddy_df['Tair'] = eddy_df['ta_1_1_1'].fillna(-9999)\n",
        "  if 'ts_1_1_1' in eddy_df.columns:\n",
        "    eddy_df['Tsoil'] = eddy_df['ts_1_1_1'].fillna(-9999)\n",
        "  eddy_df['rH'] = eddy_df['rh_1_1_1'].fillna(-9999)\n",
        "  eddy_df['VPD'] = eddy_df['vpd_1_1_1'].fillna(-9999)\n",
        "  eddy_df['Ustar'] = eddy_df['u_star'].fillna(-9999)\n",
        "\n",
        "  i=0\n",
        "  while eddy_df.iloc[i]['Hour'] != 0.5:\n",
        "    i += 1\n",
        "  eddy_df = eddy_df.iloc[i:]\n",
        "\n",
        "  if len(eddy_df.index) < 90 * points_per_day:\n",
        "    print(\"WARNING!  < 90 days in reddyproc file!\")\n",
        "  pd.DataFrame({key: item for key, item in output_template.items() if key in eddy_df.columns}).to_csv(reddyproc_filename, index=False, sep=' ')\n",
        "  eddy_df.to_csv(reddyproc_filename,  index=False, header=False, columns = [i for i in output_template.keys()  if i in eddy_df.columns], mode='a', sep=' ')\n",
        "  del eddy_df\n",
        "  logging.info(f\"REddyProc file saved to {reddyproc_filename}\")"
      ],
      "metadata": {
        "id": "GFulh7FtNWtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сохранение данных ИАС"
      ],
      "metadata": {
        "id": "62o5-p8ZzR5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "\tias_df = plot_data.copy()\n",
        "\tfor column, filter in filters_db.items():\n",
        "\t\tfilter = get_column_filter(ias_df, filters_db, column)\n",
        "\t\tias_df.loc[~filter.astype(bool), column] = np.nan\n",
        "\tias_df = ias_df.fillna(-9999)\n",
        "\n",
        "\tcol_match =  {\"co2_flux\" : \"FC_1_1_1\", \"qc_co2_flux\" : \"FC_SSITC_TEST_1_1_1\", \"LE\" : \"LE_1_1_1\",\n",
        "\t\t\"qc_LE\" : \"LE_SSITC_TEST_1_1_1\", \"H\" : \"H_1_1_1\", \"qc_H\" : \"H_SSITC_TEST_1_1_1\", \"Tau\" : \"TAU_1_1_1\",\n",
        "\t\t\"qc_Tau\" : \"TAU_SSITC_TEST_1_1_1\", \"co2_strg\" : \"SC_1_1_1\", \"co2_mole_fraction\" : \"CO2_1_1_1\",\n",
        "\t\t\"h2o_mole_fraction\" : \"H2O_1_1_1\", \"sonic_temperature\" : \"T_SONIC_1_1_1\", \"u*\" : \"USTAR_1_1_1\",\n",
        "\t\t\"Ta_1_1_1\" : \"TA_1_1_1\", \"Pa_1_1_1\" : \"PA_1_1_1\", \"Swin_1_1_1\" : \"SW_IN_1_1_1\", \"Swout_1_1_1\" : \"SW_OUT_1_1_1\",\n",
        "\t\t\"Lwin_1_1_1\" : \"LW_IN_1_1_1\", \"Lwout_1_1_1\" : \"LW_OUT_1_1_1\", \"PPFD_1_1_1\" : \"PPFD_IN_1_1_1\",\n",
        "\t\t\"Rn_1_1_1\" : \"NETRAD_1_1_1\", \"MWS_1_1_1\" : \"WS_1_1_1\", \"Ts_1_1_1\" : \"TS_1_1_1\", \"Ts_2_1_1\" : \"TS_2_1_1\",\n",
        "\t\t\"Ts_3_1_1\" : \"TS_3_1_1\", \"Pswc_1_1_1\" : \"SWC_1_1_1\", \"Pswc_2_1_1\" : \"SWC_2_1_1\", \"Pswc_3_1_1\" : \"SWC_3_1_1\",\n",
        "\t\t\"SHF_1_1_1\" : \"G_1_1_1\", \"SHF_2_1_1\" : \"G_2_1_1\", \"SHF_3_1_1\" : \"G_3_1_1\", \"L\" : \"MO_LENGTH_1_1_1\",\n",
        "\t\t\"(z-d)/L\" : \"ZL_1_1_1\", \"x_peak\" : \"FETCH_MAX_1_1_1\", \"x_70%\" : \"FETCH_70_1_1_1\", \"x_90%\" : \"FETCH_90_1_1_1\", \"ch4_flux\" : \"FCH4_1_1_1\", \"qc_ch4_flux\" : \"FCH4_SSITC_TEST_1_1_1\", \"ch4_mole_fraction\" : \"CH4_1_1_1\", \"ch4_strg\" : \"SCH4_1_1_1\",  \"ch4_signal_strength\" : \"CH4_RSSI_1_1_1\", \"co2_signal_strength\" : \"CO2_STR_1_1_1\"}\n",
        "\tcol_match = {key.lower(): item for key, item in col_match.items()}\n",
        "\n",
        "\tias_df = ias_df.rename(columns=col_match)\n",
        "\ttime_cols = ['TIMESTAMP_START', 'TIMESTAMP_END', 'DTime']\n",
        "\tvar_cols = [col_match[col] for col in col_match.keys() if col_match[col] in ias_df.columns]\n",
        "\n",
        "\tias_df['TIMESTAMP_START'] = ias_df[time].dt.strftime('%Y%m%d%H%M')\n",
        "\tias_df['TIMESTAMP_END'] = (ias_df[time] + pd.Timedelta(0.5, \"H\")).dt.strftime('%Y%m%d%H%M')\n",
        "\tias_df['DTime'] = np.round(ias_df[time].dt.dayofyear + 1./48*2*ias_df[time].dt.hour + 1./48*(ias_df[time].dt.minute//30), decimals=3)\n",
        "\n",
        "\tif 'h_strg' in ias_df.columns:\n",
        "\t\tias_df['SH'] = ias_df['h_strg']\n",
        "\t\tvar_cols.append('SH')\n",
        "\tif 'le_strg' in ias_df.columns:\n",
        "\t\tias_df['SLE'] = ias_df['le_strg']\n",
        "\t\tvar_cols.append('SLE')\n",
        "\n",
        "\tias_year = ias_df[time].dt.year.min()\n",
        "\tvar_cols.sort()\n",
        "\tcol_list_ias = time_cols + var_cols\n",
        "\tprint(col_list_ias)\n",
        "\tias_df = ias_df[col_list_ias]\n",
        "\tias_df.to_csv(f\"{ias_output_prefix}_{ias_year}_{ias_output_version}.csv\", index=False)\n",
        "\tlogging.info(f\"IAS file saved to {ias_output_prefix}_{ias_year}_{ias_output_version}.csv\")"
      ],
      "metadata": {
        "id": "yaLoIQmtzaYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сохранение данных FAT"
      ],
      "metadata": {
        "id": "Pm8hiMrb_wRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "0ll51nOal6Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config_meteo ['use_biomet']:\n",
        "  fat_output_template = {'DoY': ['--'], r'u*': ['m s-1'],\t'H': ['W m-2'], 'lE': ['-'],\t'NEE': ['umol m-2 s-1'],\t'PPFD': ['umol m-2 s-1'], 'Ta':['oC'], 'VPD':['kPa'], 'PPFD_gapfilling': ['umol m-2 s-1'], 'Ta_gapfilling': ['oC'], 'VPD_gapfilling': ['kPa'], 'period': ['--']}\n",
        "\n",
        "  fat_df = plot_data.copy()\n",
        "\n",
        "\n",
        "  for column, filter in filters_db.items():\n",
        "    filter = get_column_filter(fat_df, filters_db, column)\n",
        "    fat_df.loc[~filter.astype(bool), column] = np.nan\n",
        "\n",
        "\n",
        "\n",
        "  fat_df['DoY'] = np.round(fat_df[time].dt.dayofyear + 0.02*fat_df[time].dt.hour + 0.01*(fat_df[time].dt.minute//30), decimals=2)\n",
        "  fat_df[r'u*'] = fat_df['u_star'].fillna(-99999)\n",
        "  fat_df['H'] = fat_df['h'].fillna(-99999)\n",
        "  fat_df['lE'] = fat_df['le'].fillna(-99999)\n",
        "  fat_df['NEE'] = fat_df['nee'].fillna(-99999)\n",
        "  fat_df['PPFD'] = fat_df['ppfd_1_1_1'].fillna(-99999)\n",
        "  fat_df['Ta'] = fat_df['ta_1_1_1'].fillna(-99999)\n",
        "  fat_df['VPD'] = fat_df['vpd_1_1_1'].fillna(-99999)\n",
        "\n",
        "  fat_df['period'] = fat_df.index.month%12//3 + 1\n",
        "\n",
        "  fat_df['PPFD_gapfilling'] = fat_df['ppfd_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['ppfd_1_1_1'], rolling_window=10 , step=24 * 2, min_periods=4)).fillna(-99999)\n",
        "  fat_df['Ta_gapfilling'] = fat_df['ta_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['ta_1_1_1'], rolling_window=10 , step=24 * 2, min_periods=4)).fillna(-99999)\n",
        "  fat_df['VPD_gapfilling'] = fat_df['vpd_1_1_1'].interpolate(limit=3).fillna(bg.calc_rolling(fat_df['vpd_1_1_1'], rolling_window=10 , step=24 * 2, min_periods=4)).fillna(-99999)\n",
        "\n",
        "  for year in fat_df.index.year.unique():\n",
        "    fat_filename = f\"FAT_{year}.csv\"\n",
        "    pd.DataFrame(fat_output_template).to_csv(fat_filename, index=False)\n",
        "    save_data = fat_df.loc[fat_df[time].dt.year==year]\n",
        "    fat_df.to_csv(fat_filename,  index=False, header=False, columns = [i for i in fat_output_template.keys()], mode='a')#, sep=' ')\n",
        "  del fat_df\n",
        "  logging.info(f\"FAT file saved to {fat_filename}\")"
      ],
      "metadata": {
        "id": "w9hkPLkB_zd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Общий файл"
      ],
      "metadata": {
        "id": "GQ1bpermu8eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data.to_csv('basic.csv', index=None)\n",
        "logging.info(f\"Basic file saved to 'basic.csv'\")"
      ],
      "metadata": {
        "id": "pk1lGANovC5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модифицированный общий файл"
      ],
      "metadata": {
        "id": "-MSrgUD0-19l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_save = ['Date', 'Time', 'DoY', 'ta', 'rh', 'vpd', 'swin', 'ppfd', 'p', 'h', 'le', 'co2_flux', 'co2_strg']\n",
        "\n",
        "basic_df = plot_data.copy()\n",
        "\n",
        "basic_df['Date'] = basic_df[time].dt.date\n",
        "basic_df['Time'] = basic_df[time].dt.time\n",
        "basic_df['DoY'] = np.round(fat_df[time].dt.dayofyear + 0.02*fat_df[time].dt.hour + 0.01*(fat_df[time].dt.minute//30), decimals=2)\n",
        "\n",
        "for col in ['ta', 'rh', 'vpd', 'swin', 'ppfd', 'p']:\n",
        "  col_pos = [bool(re.fullmatch(f\"{col}(_[1-9]){1,4})\", col_in)) for col_in in  basic_df.columns]\n",
        "  if not any(col_pos):\n",
        "    continue\n",
        "  else:\n",
        "    real_col_name = basic_df.columns[np.argmax(col_pos)]\n",
        "    basic_df[col] = basic_df[real_col_name]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "basic_df = basic_df[[col for col in columns_to_save if col in basic_df.columns]]\n",
        "basic_df.fillna(-99999)\n",
        "basic_df.to_csv('basic_new.csv', index=None)\n",
        "logging.info(f\"New basic file saved to 'basic.csv'\")"
      ],
      "metadata": {
        "id": "22dPWc2u-6IG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}